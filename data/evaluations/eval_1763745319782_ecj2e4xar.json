{
  "token": "eval_1763745319782_ecj2e4xar",
  "metadata": {
    "title": "A dynamic knowledge graph approach to distributed self-driving laboratories",
    "authors": [
      "Jiaru Bai",
      "Sebastian Mosbach",
      "Connor J. Taylor",
      "Dogancan Karan",
      "Kok Foong Lee",
      "Simon D. Rihm",
      "Jethro Akroyd",
      "Alexei A. Lapkin",
      "Markus Kraft"
    ],
    "abstract": "Abstract\n                  The ability to integrate resources and share knowledge across organisations empowers scientists to expedite the scientific discovery process. This is especially crucial in addressing emerging global challenges that require global solutions. In this work, we develop an architecture for distributed self-driving laboratories within The World Avatar project, which seeks to create an all-encompassing digital twin based on a dynamic knowledge graph. We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. We demonstrate the practical application of our framework by linking two robots in Cambridge and Singapore for a collaborative closed-loop optimisation for a pharmaceutically-relevant aldol condensation reaction in real-time. The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a Pareto front for cost-yield optimisation in three days.",
    "doi": "10.1038/s41467-023-44599-9",
    "url": "http://dx.doi.org/10.1038/s41467-023-44599-9",
    "publicationDate": "2024-01-31T23:00:00.000Z",
    "status": "success",
    "venue": "Nature Communications"
  },
  "researchFields": {
    "fields": [
      {
        "id": "R133",
        "name": "Artificial Intelligence",
        "score": 6.042844772338867,
        "description": ""
      },
      {
        "id": "R112127",
        "name": "Multiagent Systems",
        "score": 4.453640460968018,
        "description": ""
      },
      {
        "id": "R112119",
        "name": "Computers and Society",
        "score": 3.9109833240509033,
        "description": ""
      },
      {
        "id": "R236",
        "name": "Robotics",
        "score": 3.852163791656494,
        "description": ""
      },
      {
        "id": "R132",
        "name": "Computer Sciences",
        "score": 3.6841962337493896,
        "description": ""
      }
    ],
    "selectedField": {
      "id": "R133",
      "name": "Artificial Intelligence",
      "score": 6.042844772338867,
      "description": ""
    },
    "status": "completed",
    "processing_info": {
      "step": "researchFields",
      "status": {
        "status": "completed",
        "step": "researchFields",
        "progress": 100,
        "message": "Research fields identified successfully",
        "timestamp": "2025-11-21T16:45:09.681Z"
      },
      "progress": 100,
      "message": "Research fields identified successfully",
      "timestamp": "2025-11-21T16:45:09.681Z"
    }
  },
  "researchProblems": {
    "orkg_problems": [],
    "llm_problem": {
      "title": "Enabling Collaborative and Autonomous Scientific Discovery Through Distributed Knowledge Integration",
      "problem": "The challenge of designing scalable, interoperable, and autonomous systems that can integrate distributed resources, share knowledge dynamically, and enable real-time collaborative experimentation across geographically dispersed entities. This problem spans scientific discovery, engineering optimization, and multi-agent systems, where seamless coordination, data provenance, and adaptive knowledge representation are critical for accelerating innovation in complex, global challenges.",
      "domain": "Autonomous Scientific Systems / Distributed AI for Research",
      "impact": "Transforms the pace and efficiency of scientific discovery by enabling global collaboration, reducing redundancy, and optimizing resource allocation. Applications extend beyond chemistry (e.g., pharmaceuticals) to materials science, climate modeling, and other domains requiring large-scale, data-driven experimentation. Could also revolutionize industrial R&D by automating closed-loop optimization in manufacturing or supply chains.",
      "motivation": "Emerging global challenges (e.g., pandemics, climate change, energy crises) demand solutions that transcend siloed research efforts. Autonomous, distributed systems can bridge gaps between theory and experimentation, democratize access to cutting-edge infrastructure, and dynamically adapt to evolving research goals—critical for addressing problems where time, cost, and interdisciplinary integration are barriers.",
      "confidence": 0.95,
      "explanation": "The abstract explicitly frames the core challenge as *integrating resources and sharing knowledge across organizations* to accelerate discovery, with clear generalizability beyond the specific use case (aldol condensation). The emphasis on ontologies, autonomous agents, and dynamic knowledge graphs highlights a foundational problem in distributed AI for science. The demonstration of cross-continental robot collaboration further validates the problem's real-world relevance. Minor deduction for the slight focus on 'self-driving laboratories,' though the broader problem is well-articulated.",
      "model": "mistral-medium",
      "timestamp": "2025-11-21T16:45:45.390Z",
      "description": "The challenge of designing scalable, interoperable, and autonomous systems that can integrate distributed resources, share knowledge dynamically, and enable real-time collaborative experimentation across geographically dispersed entities. This problem spans scientific discovery, engineering optimization, and multi-agent systems, where seamless coordination, data provenance, and adaptive knowledge representation are critical for accelerating innovation in complex, global challenges.",
      "isLLMGenerated": true,
      "lastEdited": "2025-11-21T16:46:38.160Z"
    },
    "metadata": {
      "total_scanned": 0,
      "total_identified": 0,
      "total_similar": 0,
      "total_valid": 0,
      "field_id": "",
      "similarities_found": 0,
      "threshold_used": 0.5,
      "max_similarity": 0
    },
    "processing_info": {
      "step": "",
      "status": "completed",
      "progress": 0,
      "message": "",
      "timestamp": null
    },
    "selectedProblem": {
      "title": "Enabling Collaborative and Autonomous Scientific Discovery Through Distributed Knowledge Integration",
      "description": "The challenge of designing scalable, interoperable, and autonomous systems that can integrate distributed resources, share knowledge dynamically, and enable real-time collaborative experimentation across geographically dispersed entities. This problem spans scientific discovery, engineering optimization, and multi-agent systems, where seamless coordination, data provenance, and adaptive knowledge representation are critical for accelerating innovation in complex, global challenges.",
      "isLLMGenerated": true,
      "confidence": 0.95
    },
    "original_llm_problem": {
      "title": "Enabling Collaborative and Autonomous Scientific Discovery Through Distributed Knowledge Integration",
      "problem": "The challenge of designing scalable, interoperable, and autonomous systems that can integrate distributed resources, share knowledge dynamically, and enable real-time collaborative experimentation across geographically dispersed entities. This problem spans scientific discovery, engineering optimization, and multi-agent systems, where seamless coordination, data provenance, and adaptive knowledge representation are critical for accelerating innovation in complex, global challenges.",
      "confidence": 0.95,
      "explanation": "The abstract explicitly frames the core challenge as *integrating resources and sharing knowledge across organizations* to accelerate discovery, with clear generalizability beyond the specific use case (aldol condensation). The emphasis on ontologies, autonomous agents, and dynamic knowledge graphs highlights a foundational problem in distributed AI for science. The demonstration of cross-continental robot collaboration further validates the problem's real-world relevance. Minor deduction for the slight focus on 'self-driving laboratories,' though the broader problem is well-articulated.",
      "domain": "Autonomous Scientific Systems / Distributed AI for Research",
      "impact": "Transforms the pace and efficiency of scientific discovery by enabling global collaboration, reducing redundancy, and optimizing resource allocation. Applications extend beyond chemistry (e.g., pharmaceuticals) to materials science, climate modeling, and other domains requiring large-scale, data-driven experimentation. Could also revolutionize industrial R&D by automating closed-loop optimization in manufacturing or supply chains.",
      "motivation": "Emerging global challenges (e.g., pandemics, climate change, energy crises) demand solutions that transcend siloed research efforts. Autonomous, distributed systems can bridge gaps between theory and experimentation, democratize access to cutting-edge infrastructure, and dynamically adapt to evolving research goals—critical for addressing problems where time, cost, and interdisciplinary integration are barriers.",
      "model": "mistral-medium",
      "timestamp": "2025-11-21T16:45:45.390Z"
    }
  },
  "templates": {
    "available": {
      "template": {
        "id": "ai-distributed-knowledge-integration-2024",
        "name": "Technical Framework for Collaborative and Autonomous Scientific Discovery via Distributed Knowledge Integration",
        "description": "Template for evaluating technical implementations of AI-driven systems that enable real-time collaboration, dynamic knowledge sharing, and autonomous experimentation across distributed scientific entities. Focuses on scalability, interoperability, and adaptive coordination mechanisms.",
        "properties": [
          {
            "id": "distributed-architecture",
            "label": "Distributed System Architecture",
            "description": "Named architecture or framework used to enable distributed knowledge integration (e.g., federated learning, multi-agent systems, blockchain-based coordination).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_architecture"
            }
          },
          {
            "id": "knowledge-representation-model",
            "label": "Knowledge Representation Model",
            "description": "Formal model or ontology used for encoding and sharing knowledge (e.g., RDF, OWL, knowledge graphs, vector embeddings).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_model"
            }
          },
          {
            "id": "interoperability-protocol",
            "label": "Interoperability Protocol",
            "description": "Standard or protocol ensuring cross-system compatibility (e.g., HTTP/REST, gRPC, MQTT, GA4GH for genomics, OPC UA for industrial systems).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_protocol"
            }
          },
          {
            "id": "real-time-sync-mechanism",
            "label": "Real-Time Synchronization Mechanism",
            "description": "Technique for ensuring real-time or near-real-time knowledge updates (e.g., CRDTs, operational transformation, event sourcing).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 20
            }
          },
          {
            "id": "autonomy-algorithm",
            "label": "Autonomy Algorithm",
            "description": "Algorithm enabling autonomous decision-making (e.g., reinforcement learning, automated planning, swarm intelligence).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_algorithm"
            }
          },
          {
            "id": "scalability-metric",
            "label": "Scalability Metric (Nodes/Second)",
            "description": "Quantitative measure of system scalability, e.g., maximum nodes supported per second or throughput in knowledge updates/sec.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1000000
            }
          },
          {
            "id": "latency-ms",
            "label": "End-to-End Latency (ms)",
            "description": "Average or 95th-percentile latency for knowledge propagation across the distributed system, in milliseconds.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 10000
            }
          },
          {
            "id": "data-provenance-method",
            "label": "Data Provenance Method",
            "description": "Technique for tracking data lineage (e.g., W3C PROV, blockchain hashes, immutable logs).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_method"
            }
          },
          {
            "id": "collaboration-protocol",
            "label": "Collaboration Protocol",
            "description": "Named protocol or algorithm for coordinating multi-entity collaboration (e.g., consensus algorithms, auction-based resource allocation).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_protocol"
            }
          },
          {
            "id": "adaptive-learning-rate",
            "label": "Adaptive Learning Rate",
            "description": "Dynamic adjustment mechanism for knowledge integration rates (e.g., gradient-based, Bayesian optimization).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 10
            }
          },
          {
            "id": "benchmark-dataset",
            "label": "Benchmark Dataset",
            "description": "Standardized dataset used for evaluating system performance (e.g., OpenML, Kaggle datasets, domain-specific benchmarks).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_dataset"
            }
          },
          {
            "id": "fault-tolerance-mechanism",
            "label": "Fault Tolerance Mechanism",
            "description": "Technique for handling node failures or network partitions (e.g., Byzantine fault tolerance, checkpointing, self-healing).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 15
            }
          },
          {
            "id": "knowledge-fusion-algorithm",
            "label": "Knowledge Fusion Algorithm",
            "description": "Algorithm for merging conflicting or complementary knowledge (e.g., Bayesian fusion, Dempster-Shafer theory, attention-based aggregation).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_algorithm"
            }
          },
          {
            "id": "security-protocol",
            "label": "Security Protocol",
            "description": "Protocol for ensuring data integrity and confidentiality (e.g., TLS 1.3, homomorphic encryption, zero-knowledge proofs).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_protocol"
            }
          },
          {
            "id": "energy-efficiency-metric",
            "label": "Energy Efficiency (kWh/Operation)",
            "description": "Energy consumption per knowledge integration operation or per node-hour.",
            "type": "number",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1000
            }
          },
          {
            "id": "evaluation-metric-primary",
            "label": "Primary Evaluation Metric",
            "description": "Key quantitative metric for success (e.g., F1-score, discovery acceleration factor, collaboration efficiency).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 10
            }
          },
          {
            "id": "baseline-system",
            "label": "Baseline System for Comparison",
            "description": "Existing system or method used as a performance baseline (e.g., centralized knowledge bases, manual collaboration).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_system"
            }
          },
          {
            "id": "deployment-environment",
            "label": "Deployment Environment",
            "description": "Infrastructure used for deployment (e.g., Kubernetes, AWS Lambda, edge devices).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_environment"
            }
          },
          {
            "id": "api-specification",
            "label": "API Specification",
            "description": "Link to formal API documentation (e.g., OpenAPI, GraphQL schema) for system integration.",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "uri"
            }
          },
          {
            "id": "reproducibility-script",
            "label": "Reproducibility Script Repository",
            "description": "URL to code/scripts for reproducing experiments (e.g., GitHub, Zenodo).",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "uri"
            }
          },
          {
            "id": "publication-date",
            "label": "Publication Date",
            "description": "Date when the research was published or preprinted.",
            "type": "date",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "date"
            }
          },
          {
            "id": "cross-domain-applicability",
            "label": "Cross-Domain Applicability",
            "description": "Description of how the system generalizes across domains (e.g., biology to materials science).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 20
            }
          },
          {
            "id": "cost-per-operation",
            "label": "Cost per Operation (USD)",
            "description": "Monetary cost per knowledge integration operation (e.g., cloud compute costs, energy costs).",
            "type": "number",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 10000
            }
          }
        ],
        "metadata": {
          "research_field": "Artificial Intelligence",
          "research_category": "Distributed Systems & Autonomous Scientific Discovery",
          "adaptability_score": 0.85,
          "total_properties": 23,
          "suggested_sections": [
            "System Architecture",
            "Knowledge Representation",
            "Interoperability & Protocols",
            "Autonomy Mechanisms",
            "Performance Metrics",
            "Fault Tolerance & Security",
            "Evaluation & Baselines",
            "Reproducibility"
          ],
          "creation_timestamp": "2025-11-21T16:49:56.438Z",
          "model": "mistral-medium",
          "template_version": "1.0"
        }
      }
    },
    "selectedTemplate": null,
    "llm_template": {
      "template": {
        "id": "ai-distributed-knowledge-integration-2024",
        "name": "Technical Framework for Collaborative and Autonomous Scientific Discovery via Distributed Knowledge Integration",
        "description": "Template for evaluating technical implementations of AI-driven systems that enable real-time collaboration, dynamic knowledge sharing, and autonomous experimentation across distributed scientific entities. Focuses on scalability, interoperability, and adaptive coordination mechanisms.",
        "properties": [
          {
            "id": "distributed-architecture",
            "label": "Distributed System Architecture",
            "description": "Named architecture or framework used to enable distributed knowledge integration (e.g., federated learning, multi-agent systems, blockchain-based coordination).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_architecture"
            }
          },
          {
            "id": "knowledge-representation-model",
            "label": "Knowledge Representation Model",
            "description": "Formal model or ontology used for encoding and sharing knowledge (e.g., RDF, OWL, knowledge graphs, vector embeddings).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_model"
            }
          },
          {
            "id": "interoperability-protocol",
            "label": "Interoperability Protocol",
            "description": "Standard or protocol ensuring cross-system compatibility (e.g., HTTP/REST, gRPC, MQTT, GA4GH for genomics, OPC UA for industrial systems).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_protocol"
            }
          },
          {
            "id": "real-time-sync-mechanism",
            "label": "Real-Time Synchronization Mechanism",
            "description": "Technique for ensuring real-time or near-real-time knowledge updates (e.g., CRDTs, operational transformation, event sourcing).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 20
            }
          },
          {
            "id": "autonomy-algorithm",
            "label": "Autonomy Algorithm",
            "description": "Algorithm enabling autonomous decision-making (e.g., reinforcement learning, automated planning, swarm intelligence).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_algorithm"
            }
          },
          {
            "id": "scalability-metric",
            "label": "Scalability Metric (Nodes/Second)",
            "description": "Quantitative measure of system scalability, e.g., maximum nodes supported per second or throughput in knowledge updates/sec.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1000000
            }
          },
          {
            "id": "latency-ms",
            "label": "End-to-End Latency (ms)",
            "description": "Average or 95th-percentile latency for knowledge propagation across the distributed system, in milliseconds.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 10000
            }
          },
          {
            "id": "data-provenance-method",
            "label": "Data Provenance Method",
            "description": "Technique for tracking data lineage (e.g., W3C PROV, blockchain hashes, immutable logs).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_method"
            }
          },
          {
            "id": "collaboration-protocol",
            "label": "Collaboration Protocol",
            "description": "Named protocol or algorithm for coordinating multi-entity collaboration (e.g., consensus algorithms, auction-based resource allocation).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_protocol"
            }
          },
          {
            "id": "adaptive-learning-rate",
            "label": "Adaptive Learning Rate",
            "description": "Dynamic adjustment mechanism for knowledge integration rates (e.g., gradient-based, Bayesian optimization).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 10
            }
          },
          {
            "id": "benchmark-dataset",
            "label": "Benchmark Dataset",
            "description": "Standardized dataset used for evaluating system performance (e.g., OpenML, Kaggle datasets, domain-specific benchmarks).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_dataset"
            }
          },
          {
            "id": "fault-tolerance-mechanism",
            "label": "Fault Tolerance Mechanism",
            "description": "Technique for handling node failures or network partitions (e.g., Byzantine fault tolerance, checkpointing, self-healing).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 15
            }
          },
          {
            "id": "knowledge-fusion-algorithm",
            "label": "Knowledge Fusion Algorithm",
            "description": "Algorithm for merging conflicting or complementary knowledge (e.g., Bayesian fusion, Dempster-Shafer theory, attention-based aggregation).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_algorithm"
            }
          },
          {
            "id": "security-protocol",
            "label": "Security Protocol",
            "description": "Protocol for ensuring data integrity and confidentiality (e.g., TLS 1.3, homomorphic encryption, zero-knowledge proofs).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_protocol"
            }
          },
          {
            "id": "energy-efficiency-metric",
            "label": "Energy Efficiency (kWh/Operation)",
            "description": "Energy consumption per knowledge integration operation or per node-hour.",
            "type": "number",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1000
            }
          },
          {
            "id": "evaluation-metric-primary",
            "label": "Primary Evaluation Metric",
            "description": "Key quantitative metric for success (e.g., F1-score, discovery acceleration factor, collaboration efficiency).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 10
            }
          },
          {
            "id": "baseline-system",
            "label": "Baseline System for Comparison",
            "description": "Existing system or method used as a performance baseline (e.g., centralized knowledge bases, manual collaboration).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_system"
            }
          },
          {
            "id": "deployment-environment",
            "label": "Deployment Environment",
            "description": "Infrastructure used for deployment (e.g., Kubernetes, AWS Lambda, edge devices).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_environment"
            }
          },
          {
            "id": "api-specification",
            "label": "API Specification",
            "description": "Link to formal API documentation (e.g., OpenAPI, GraphQL schema) for system integration.",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "uri"
            }
          },
          {
            "id": "reproducibility-script",
            "label": "Reproducibility Script Repository",
            "description": "URL to code/scripts for reproducing experiments (e.g., GitHub, Zenodo).",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "uri"
            }
          },
          {
            "id": "publication-date",
            "label": "Publication Date",
            "description": "Date when the research was published or preprinted.",
            "type": "date",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "date"
            }
          },
          {
            "id": "cross-domain-applicability",
            "label": "Cross-Domain Applicability",
            "description": "Description of how the system generalizes across domains (e.g., biology to materials science).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 20
            }
          },
          {
            "id": "cost-per-operation",
            "label": "Cost per Operation (USD)",
            "description": "Monetary cost per knowledge integration operation (e.g., cloud compute costs, energy costs).",
            "type": "number",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 10000
            }
          }
        ],
        "metadata": {
          "research_field": "Artificial Intelligence",
          "research_category": "Distributed Systems & Autonomous Scientific Discovery",
          "adaptability_score": 0.85,
          "total_properties": 23,
          "suggested_sections": [
            "System Architecture",
            "Knowledge Representation",
            "Interoperability & Protocols",
            "Autonomy Mechanisms",
            "Performance Metrics",
            "Fault Tolerance & Security",
            "Evaluation & Baselines",
            "Reproducibility"
          ],
          "creation_timestamp": "2025-11-21T16:49:56.438Z",
          "model": "mistral-medium",
          "template_version": "1.0"
        }
      }
    },
    "status": "success",
    "processing_info": {
      "step": "template",
      "status": "completed",
      "progress": 100,
      "message": "Template generated successfully",
      "timestamp": "2025-11-21T16:49:56.439Z"
    }
  },
  "paperContent": {
    "paperContent": {
      "distributed-architecture": {
        "property": "Distributed System Architecture",
        "label": "Distributed System Architecture",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "The World Avatar",
            "confidence": 1,
            "evidence": {
              "Abstract": {
                "text": "This is especially crucial in addressing emerging global challenges that require global solutions. In this work, we develop an architecture for distributed self-driving laboratories within **The World Avatar** project, which seeks to create an all-encompassing digital twin based on a dynamic knowledge graph.",
                "relevance": "Direct mention of 'The World Avatar' as the named architecture for distributed knowledge integration, explicitly described as the framework for distributed self-driving laboratories."
              },
              "Introduction": {
                "text": "**The World Avatar**34,35 is such a knowledge graph that aims to encompass all aspects of scientific research laboratories as shown in Fig. 1a in their entirety: The experiment itself, including its physical setup and underlying chemistry; moving handlers that can be of human or robotic nature; and the laboratory providing necessary infrastructure and resources36.",
                "relevance": "Explicitly identifies 'The World Avatar' as the dynamic knowledge graph architecture enabling distributed SDLs, with cross-references to prior work (34,35) confirming its role as the core framework."
              },
              "The World Avatar knowledge graph": {
                "text": "This work follows the best practices in **the World Avatar project**. All ontologies and agents are version-controlled on GitHub.",
                "relevance": "Reinforces 'The World Avatar' as the operational name of the distributed system architecture, emphasizing its role in standardizing ontologies and agents for global collaboration."
              }
            },
            "id": "val-hyhgckv"
          },
          {
            "value": "derived information framework",
            "confidence": 0.95,
            "evidence": {
              "Architecture of distributed SDLs": {
                "text": "We adopt the **derived information framework**42, a knowledge-graph-native approach, to manage the iterative workflow.",
                "relevance": "Explicitly named as the framework for managing iterative workflows in distributed SDLs, integrated with the dynamic knowledge graph."
              },
              "Goal-driven knowledge dynamics": {
                "text": "To ensure correct data dependencies and the order of task execution, we employed the **derived information framework**42 to manage the iterative workflow.",
                "relevance": "Confirms its role as a critical component of the distributed architecture, specifically for workflow orchestration and data provenance."
              }
            },
            "id": "val-s65ajwx"
          },
          {
            "value": "dynamic knowledge graph",
            "confidence": 0.9,
            "evidence": {
              "Abstract": {
                "text": "We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. We demonstrate the practical application of our framework by linking two robots in Cambridge and Singapore for a collaborative closed-loop optimisation for a pharmaceutically-relevant aldol condensation reaction in real-time. **The knowledge graph autonomously evolves** toward the scientist’s research goals...",
                "relevance": "Describes the 'dynamic knowledge graph' as the core mechanism enabling distributed SDLs, with autonomous evolution and real-time collaboration."
              },
              "Introduction": {
                "text": "**The World Avatar** goes beyond static knowledge representation by encoding software agents as executable knowledge components, enabling dynamicity and continuous incorporation of new concepts and data while preserving connections to existing information. As the **knowledge graph** expands, this characteristic allows for capturing data provenance from experimental processes as knowledge statements, effectively acting as a living copy of the real world.",
                "relevance": "Explicitly links 'dynamic knowledge graph' to the World Avatar project, emphasizing its role in real-time data integration and provenance tracking across distributed labs."
              },
              "Architecture of distributed SDLs": {
                "text": "We believe **dynamic knowledge graph** technology can help with realising this architecture32. Specifically, as illustrated in Fig. 2b, this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                "relevance": "Positions the 'dynamic knowledge graph' as the technological backbone for the distributed SDL architecture, enabling agent-based orchestration and data flow."
              }
            },
            "id": "val-t8c1x6f"
          }
        ]
      },
      "knowledge-representation-model": {
        "property": "Knowledge Representation Model",
        "label": "Knowledge Representation Model",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Dynamic Knowledge Graph",
            "confidence": 1,
            "evidence": {
              "Abstract": {
                "text": "The World Avatar project, which seeks to create an all-encompassing digital twin based on a **dynamic knowledge graph**.",
                "relevance": "Explicit mention of the core knowledge representation model used in the study, with emphasis on its dynamic nature as a distinguishing feature."
              },
              "Introduction": {
                "text": "The World Avatar is such a **knowledge graph** that aims to encompass all aspects of scientific research laboratories... encoding software agents as executable knowledge components, enabling dynamicity and continuous incorporation of new concepts and data while preserving connections to existing information.",
                "relevance": "Detailed description of the knowledge graph's role as the foundational model for integrating heterogeneous data and enabling autonomous workflows, with dynamicity as a key attribute."
              },
              "Architecture of distributed SDLs": {
                "text": "This reformulation of the closed-loop optimisation problem as information travelling through the **knowledge graph** and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs.",
                "relevance": "Explicit framing of the knowledge graph as the central model for representing and propagating information in the distributed system."
              },
              "The World Avatar knowledge graph": {
                "text": "This work follows the best practices in the **World Avatar project**... All ontologies and agents are version-controlled on GitHub... The knowledge graph is designed to span across the internet.",
                "relevance": "Direct reference to the World Avatar as the overarching knowledge representation framework, with operational details confirming its implementation as a graph-based model."
              }
            },
            "id": "val-gv24tg3"
          },
          {
            "value": "OntoCAPE",
            "confidence": 0.95,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "Building on this foundation, we introduce OntoReaction... As an effort to align with existing data, **OntoReaction** draws inspiration from established schemas used in chemical reaction databases like ORD and UDM. The concepts are categorised based on their level of abstraction... Their namespaces correspond to the colour coding. For complete knowledge representation and namespace definitions see Supplementary Information section A.1.1.\n\nOne prominent example is the **OntoCAPE** material and chemical process system, which describes materials from three aspects: the ChemicalSpecies that reflects the intrinsic characteristics, Material as part of the phase system which describes macroscopic thermodynamic behaviour, and MaterialAmount that refers to a concrete occurrence of an amount of matter in the physical world.",
                "relevance": "Explicit citation of OntoCAPE as a foundational ontology for material and chemical process systems, directly integrated into the study's knowledge representation layer. The text confirms its role as a resource for modeling chemical species, materials, and amounts, which are core to the SDL workflow."
              }
            },
            "id": "val-w9y02z9"
          },
          {
            "value": "OntoReaction",
            "confidence": 0.95,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "we introduce **OntoReaction**, an ontology that captures knowledge in wet-lab reaction experiments... ReactionExperiment is a concrete realisation of a ChemicalReaction that is sampled at a set of ReactionConditions and measures certain PerformanceIndicators.",
                "relevance": "Direct introduction of OntoReaction as a new ontology developed specifically for wet-lab reaction experiments, with explicit mention of its role in modeling reactions, conditions, and performance indicators—key components of the SDL workflow."
              }
            },
            "id": "val-q7b3ahz"
          },
          {
            "value": "OntoDoE",
            "confidence": 0.95,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and **OntoDoE**, an ontology for the design of experiments (DoE) in optimisation campaigns.",
                "relevance": "Explicit introduction of OntoDoE as a dedicated ontology for design of experiments (DoE), which is a critical component of closed-loop optimization in SDLs. The text confirms its role in modeling optimization campaigns."
              }
            },
            "id": "val-kcs1op4"
          },
          {
            "value": "OntoLab",
            "confidence": 0.95,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "We introduce **OntoLab** to represent the digital twin of a laboratory, comprising a group of LabEquipment and ChemicalContainers that contain ChemicalAmount.",
                "relevance": "Direct introduction of OntoLab as an ontology for modeling laboratory digital twins, including equipment and chemical containers—essential for material flow tracking in SDLs."
              }
            },
            "id": "val-wvb285h"
          },
          {
            "value": "OntoVapourtec",
            "confidence": 0.95,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "we create **OntoVapourtec** as ontologies for the equipment involved in this work, linking them to the concrete realisation aspect of OntoCAPE.",
                "relevance": "Explicit mention of OntoVapourtec as an equipment-specific ontology, directly linked to the Vapourtec hardware used in the study's SDLs."
              }
            },
            "id": "val-6y9a9kc"
          },
          {
            "value": "OntoHPLC",
            "confidence": 0.95,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "we create OntoVapourtec and **OntoHPLC** as ontologies for the equipment involved in this work... A more comprehensive representation of impurities can be achieved in conjunction with concentration-related concepts, such as OntoCAPE:Molarity, which we shall incorporate in future work. For concrete examples of ontology instantiation see Supplementary Information section A.1.",
                "relevance": "Direct introduction of OntoHPLC as an ontology for HPLC equipment, with explicit mention of its role in representing chromatogram data and impurities—critical for reaction analysis in SDLs."
              },
              "Collaborative closed-loop optimisation": {
                "text": "The best values obtained are 26.17 and 258.175 g L−1 h−1 when scaled to the same benzaldehyde injection volume (5 mL), both outperformed the previous study51. The animation of the optimisation progress is available in Supplementary Movie 1.\n\n...the product peak was missed for one run at the Cambridge side due to a small shift of the peak which gives a yield of 0%. This point was taken into consideration in the DoE, but fortunately, it did not affect the final Pareto front as the corrected yield is still Pareto-dominated. The optimisation campaign was stopped since no more significant improvement was observed in terms of hypervolume, and also due to requests for repurposing the equipment for other projects.",
                "relevance": "Implicit validation of OntoHPLC's role in processing HPLC data (e.g., peak shifts, yield calculations) during the collaborative optimization, confirming its operational use in the study."
              }
            },
            "id": "val-uve2yd8"
          },
          {
            "value": "OntoAgent",
            "confidence": 0.9,
            "evidence": {
              "The World Avatar knowledge graph": {
                "text": "Following the development of ontologies, agents are defined as executables that process inputs and generate outputs. Their I/O signatures are represented following **OntoAgent**.",
                "relevance": "Explicit reference to OntoAgent as the ontology used to standardize the input/output signatures of autonomous agents in the knowledge graph, confirming its role as a resource for agent modeling."
              }
            },
            "id": "val-aug3uxh"
          },
          {
            "value": "SAREF",
            "confidence": 0.9,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "In the development of our hardware ontologies, we have expanded upon concepts from the **Smart Applications REFerence (SAREF)** ontology, which is widely adopted in the field of the Internet of Things.",
                "relevance": "Direct citation of SAREF as a foundational ontology for hardware modeling, explicitly expanded upon to develop the study's laboratory equipment ontologies (e.g., OntoLab)."
              }
            },
            "id": "val-2f63bni"
          },
          {
            "value": "Derived Information Framework",
            "confidence": 0.95,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "we adopt the **derived information framework**, a knowledge-graph-native approach, to manage the iterative workflow.",
                "relevance": "Explicit adoption of the derived information framework as a knowledge-graph-native method for workflow management, confirming its role as a core resource for dynamic data handling in SDLs."
              },
              "Goal-driven knowledge dynamics": {
                "text": "To ensure correct data dependencies and the order of task execution, we employed the **derived information framework** to manage the iterative workflow. We implemented each software agent using the derivation agent template provided by the framework.",
                "relevance": "Direct reference to the framework's use in managing iterative workflows and agent templates, validating its operational role in the study."
              },
              "The World Avatar knowledge graph": {
                "text": "The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic.",
                "relevance": "Explicit description of the framework's technical role in automating workflow management, reducing manual implementation effort."
              }
            },
            "id": "val-5d9zm4m"
          },
          {
            "value": "OntoSpecies",
            "confidence": 0.9,
            "evidence": {
              "Contextualised reaction informatics": {
                "text": "The integration of chemical knowledge from PubChem, represented by **OntoSpecies** for unique species identification, serves as a critical link between these facets of chemicals.",
                "relevance": "Explicit mention of OntoSpecies as the ontology for chemical species identification, directly linked to PubChem data integration—a key resource for reaction informatics in SDLs."
              }
            },
            "id": "val-0w1728u"
          }
        ]
      },
      "interoperability-protocol": {
        "property": "Interoperability Protocol",
        "label": "Interoperability Protocol",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "χDL",
            "confidence": 1,
            "evidence": {
              "Introduction": {
                "text": "For data sharing, χDL26,27 and AnIML28 are examples of standard protocols developed for synthesis and analysis respectively.",
                "relevance": "Direct mention of χDL as a standard protocol for synthesis data sharing, explicitly framed as an interoperability solution."
              }
            },
            "id": "val-tsl9nt3"
          },
          {
            "value": "AnIML",
            "confidence": 1,
            "evidence": {
              "Introduction": {
                "text": "For data sharing, χDL26,27 and AnIML28 are examples of standard protocols developed for synthesis and analysis respectively.",
                "relevance": "Explicit identification of AnIML as a standard protocol for analytical data sharing, directly addressing interoperability."
              }
            },
            "id": "val-yzecdhm"
          },
          {
            "value": "OPC UA",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA60 and SiLA28.",
                "relevance": "OPC UA is explicitly mentioned as a facilitative effort for unified API/interoperability in industrial hardware/software contexts."
              }
            },
            "id": "val-hecft5t"
          },
          {
            "value": "SiLA2",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA60 and SiLA28.",
                "relevance": "SiLA2 is cited alongside OPC UA as a key effort for enabling interoperability via unified APIs."
              }
            },
            "id": "val-ogw4ve2"
          },
          {
            "value": "SAREF",
            "confidence": 0.9,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "In the development of our hardware ontologies, we have expanded upon concepts from the Smart Applications REFerence (SAREF) ontology50, which is widely adopted in the field of the Internet of Things.",
                "relevance": "SAREF is identified as a foundational ontology for IoT interoperability, adapted for hardware digital twins in this work."
              }
            },
            "id": "val-c1fzdd9"
          },
          {
            "value": "OntoCAPE",
            "confidence": 0.85,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "Building on this foundation, we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and OntoDoE, an ontology for the design of experiments (DoE) in optimisation campaigns. As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49. [...] The concepts are categorised based on their level of abstraction, spanning from high-level research goals to conceptual descriptions of chemical reactions and the mathematical expression of design of experiments, as well as the physical execution of reaction experiments and the laboratory digital twin. These concepts are interlinked with the OntoCAPE Material System, representing an effort to enhance interoperability with the community initiatives.",
                "relevance": "OntoCAPE is explicitly described as an interoperability-enhancing ontology for material systems, integrated with newly developed ontologies in this work."
              }
            },
            "id": "val-2u46uqe"
          },
          {
            "value": "ORD",
            "confidence": 0.8,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                "relevance": "ORD is referenced as a standard schema for chemical reaction data, implying its role in data interoperability."
              }
            },
            "id": "val-rgh1zd9"
          },
          {
            "value": "UDM",
            "confidence": 0.8,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                "relevance": "UDM is referenced alongside ORD as a standard schema for chemical reaction data interoperability."
              }
            },
            "id": "val-2p4oqbr"
          }
        ]
      },
      "real-time-sync-mechanism": {
        "property": "Real-Time Synchronization Mechanism",
        "label": "Real-Time Synchronization Mechanism",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "dynamic knowledge graph with autonomous agents as executable components for real-time data provenance and cross-laboratory information flow",
            "confidence": 0.98,
            "evidence": {
              "Abstract": {
                "text": "We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. [...] The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a Pareto front for cost-yield optimisation in three days.",
                "relevance": "Explicitly describes the use of a **dynamic knowledge graph** and **autonomous agents** to enable real-time synchronization of data and material flows across distributed laboratories. The agents act as executable components that update the knowledge graph, ensuring FAIR (Findable, Accessible, Interoperable, Reusable) data provenance during collaborative optimization."
              },
              "Architecture of distributed SDLs": {
                "text": "This reformulation of the closed-loop optimisation problem as information travelling through the knowledge graph and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs. In this way, we can think of an occurrence of physical experimentation as a sequence of actions that dynamically generates information about a reaction experiment as it progresses in time, analogous to computational workflows.",
                "relevance": "Highlights the **real-time information flow** through the knowledge graph, where physical experimentation dynamically updates the graph, enabling synchronization across distributed labs. The analogy to computational workflows underscores the near-real-time nature of the updates."
              },
              "Goal-driven knowledge dynamics": {
                "text": "The dynamicity of the knowledge graph is enabled by the presence of software agents that realise each component of the distributed architecture and facilitate the flow of information within the graph. [...] The progress of goal pursuit is assessed after each iteration, determining whether to proceed to the next cycle. Steps (b) and (c) are iterated until either goals are achieved or resources are depleted.",
                "relevance": "Describes the **iterative, agent-driven updates** to the knowledge graph after each experimentation cycle, ensuring synchronization of goals, data, and resources across labs. The agents autonomously restructure the graph to reflect real-world changes."
              },
              "Collaborative closed-loop optimisation": {
                "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously, providing opportunities for informed machine learning. Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure.",
                "relevance": "Demonstrates real-world application of the synchronization mechanism, where the knowledge graph evolves **autonomously and collaboratively** across two labs, with resilience to disruptions (e.g., hardware failure). Data provenance is recorded in real-time."
              },
              "The World Avatar knowledge graph": {
                "text": "The knowledge graph is designed to span across the internet. [...] These agents form a distributed network that facilitates the transfer of information within the knowledge graph and bridges cyberspace and the physical world.",
                "relevance": "Explicitly states the **internet-spanning design** of the knowledge graph, where agents form a distributed network to synchronize information across labs in real-time. The bridge between cyberspace and physical world ensures updates reflect real-time experimentation."
              }
            },
            "id": "val-7xfv6cy"
          },
          {
            "value": "derived information framework for managing iterative workflows with asynchronous agent communication",
            "confidence": 0.95,
            "evidence": {
              "Architecture of distributed SDLs": {
                "text": "We believe dynamic knowledge graph technology can help with realising this architecture. [...] The flow of data between these components is represented as messages exchanged among these agents. [...] This reformulation of the closed-loop optimisation problem as information travelling through the knowledge graph and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs.",
                "relevance": "Introduces the **derived information framework** as the backbone for managing workflows, where agents exchange messages asynchronously to synchronize data flows. This framework enables real-time updates to the knowledge graph."
              },
              "Goal-driven knowledge dynamics": {
                "text": "To ensure correct data dependencies and the order of task execution, we employed the derived information framework to manage the iterative workflow. We implemented each software agent using the derivation agent template provided by the framework. Once deployed, these agents autonomously update the knowledge graph to actively reflect and influence the state of the world.",
                "relevance": "Explicitly links the **derived information framework** to the synchronization of iterative workflows, with agents autonomously updating the knowledge graph in real-time. The framework ensures data dependencies and task execution order are maintained."
              },
              "Methods: The World Avatar knowledge graph": {
                "text": "The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic. As agents modify the knowledge graph and subsequently actuate the real world autonomously once active, it is important to make sure they behave as expected.",
                "relevance": "Confirms the framework's role in enabling agents to **autonomously modify the knowledge graph** in real-time, ensuring synchronization between cyberspace and physical experimentation."
              }
            },
            "id": "val-uuujcfr"
          },
          {
            "value": "agent-based task registration and execution with knowledge graph access for decentralized synchronization",
            "confidence": 0.92,
            "evidence": {
              "Architecture of distributed SDLs": {
                "text": "The dynamic knowledge graph approach uses agents acting as lab resource wrappers with knowledge graph access. Agents can register for jobs and proactively execute tasks assigned to the digital twin of the resources they manage.",
                "relevance": "Describes the **agent-based synchronization mechanism**, where agents register for tasks and execute them by accessing/updating the knowledge graph. This decentralized approach avoids central coordinators, enabling real-time updates."
              },
              "Methods: The World Avatar knowledge graph": {
                "text": "At start-up, agents register their OntoAgent instances in the knowledge graph, then act autonomously should tasks be assigned to them. Altogether, these agents form a distributed network that facilitates the transfer of information within the knowledge graph and bridges cyberspace and the physical world.",
                "relevance": "Details the **start-up registration process** of agents in the knowledge graph, forming a distributed network for real-time information transfer. Agents act autonomously to synchronize data."
              },
              "Discussion": {
                "text": "Compared to adopting a central coordinator to handle data transfer and format translation, our approach emphasises information propagation within a unified data layer, obviating the need for peer-to-peer data transfer and alleviating network congestion.",
                "relevance": "Contrasts the agent-based approach with central coordinators, highlighting its advantage for **decentralized, real-time synchronization** via a unified knowledge graph layer."
              }
            },
            "id": "val-kjhr0y7"
          },
          {
            "value": "event-driven updates via SPARQL queries and pydantic for ontology instantiation and data processing",
            "confidence": 0.88,
            "evidence": {
              "Methods: The World Avatar knowledge graph": {
                "text": "During iterations, competency questions are used to test if the ontologies meet case study requirements. The answers to these questions are provided in the form of SPARQL queries that are executed by the agents during their operations. Another essential aspect to consider is data instantiation, where we adopted pydantic to simplify the querying and processing of data from the knowledge graph.",
                "relevance": "Describes the use of **SPARQL queries** for event-driven updates to the knowledge graph, with **pydantic** facilitating real-time data instantiation and processing. This enables synchronous updates during experimentation."
              },
              "Chemical ontologies and digital twins": {
                "text": "For concrete examples of ontology instantiation see Supplementary Information section A.1.",
                "relevance": "Implies the use of **ontology instantiation** (via SPARQL/pydantic) to dynamically update the knowledge graph with experimental data, though details are deferred to supplementary materials."
              }
            },
            "id": "val-pb2mc4d"
          }
        ]
      },
      "autonomy-algorithm": {
        "property": "Autonomy Algorithm",
        "label": "Autonomy Algorithm",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "TSEMO algorithm",
            "confidence": 1,
            "evidence": {
              "Collaborative closed-loop optimisation": {
                "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm56.",
                "relevance": "Direct mention of the specific autonomy algorithm ('TSEMO algorithm') used by the agents for autonomous decision-making in the distributed self-driving laboratories (SDLs). This algorithm enables the iterative optimization process by updating experimental conditions based on accumulated data, aligning with the property's focus on autonomous decision-making."
              }
            },
            "id": "val-7wl682e"
          }
        ]
      },
      "data-provenance-method": {
        "property": "Data Provenance Method",
        "label": "Data Provenance Method",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "FAIR principles",
            "confidence": 0.95,
            "evidence": {
              "Introduction": {
                "text": "the third challenge of data provenance recording following FAIR principles – Findable, Accessible, Interoperable and Reusable",
                "relevance": "Direct mention of FAIR principles as the data provenance methodology used in the study, explicitly addressing the property's description of tracking data lineage."
              },
              "Discussion": {
                "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously, providing opportunities for informed machine learning.",
                "relevance": "Confirms the implementation of data provenance tracking via the dynamic knowledge graph, aligning with FAIR principles for findability, accessibility, interoperability, and reusability."
              }
            },
            "id": "val-0grht4c"
          },
          {
            "value": "dynamic knowledge graph",
            "confidence": 0.92,
            "evidence": {
              "Abstract": {
                "text": "Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability.",
                "relevance": "Explicitly states that data provenance is recorded, with the method implicitly tied to the dynamic knowledge graph described throughout the paper."
              },
              "Architecture of distributed SDLs": {
                "text": "The dynamic knowledge graph approach abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                "relevance": "Describes the dynamic knowledge graph as the mechanism for tracking data flows, which inherently includes provenance by design."
              },
              "Goal-driven knowledge dynamics": {
                "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously.",
                "relevance": "Directly states that the dynamic knowledge graph is the method used to record data provenance during the experiment."
              }
            },
            "id": "val-zvu51po"
          },
          {
            "value": "derived information framework",
            "confidence": 0.88,
            "evidence": {
              "Architecture of distributed SDLs": {
                "text": "we adopt the derived information framework, a knowledge-graph-native approach, to manage the iterative workflow.",
                "relevance": "Explicitly names the 'derived information framework' as the method used to manage data flows and provenance within the knowledge graph."
              },
              "Goal-driven knowledge dynamics": {
                "text": "To ensure correct data dependencies and the order of task execution, we employed the derived information framework to manage the iterative workflow.",
                "relevance": "Confirms the use of the derived information framework for tracking data dependencies, a core aspect of data provenance."
              }
            },
            "id": "val-dz8tbxo"
          }
        ]
      },
      "collaboration-protocol": {
        "property": "Collaboration Protocol",
        "label": "Collaboration Protocol",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "TSEMO algorithm",
            "confidence": 0.95,
            "evidence": {
              "Collaborative closed-loop optimisation": {
                "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm.",
                "relevance": "Explicit naming of the TSEMO algorithm as the coordination protocol used by autonomous agents for multi-lab collaboration in the closed-loop optimisation workflow."
              }
            },
            "id": "val-th8uf4o"
          },
          {
            "value": "derived information framework",
            "confidence": 0.9,
            "evidence": {
              "Architecture of distributed SDLs": {
                "text": "we adopted the derived information framework, a knowledge-graph-native approach, to manage the iterative workflow.",
                "relevance": "Framework explicitly used to coordinate information flow and task execution across distributed SDLs via knowledge graph updates."
              },
              "Goal-driven knowledge dynamics": {
                "text": "Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph. The derived information framework does most of the work behind the scenes...",
                "relevance": "Framework enables autonomous agent coordination through dynamic knowledge graph restructuring."
              }
            },
            "id": "val-hhhq01j"
          },
          {
            "value": "Pareto front analysis",
            "confidence": 0.85,
            "evidence": {
              "Collaborative closed-loop optimisation": {
                "text": "The real-time collaboration demonstrated faster advances in the Pareto front with the highest yield of 93%... The optimisation campaign was stopped since no more significant improvement was observed in terms of hypervolume...",
                "relevance": "Pareto front analysis is the implicit coordination mechanism for multi-objective trade-off evaluation across distributed labs."
              },
              "Discussion": {
                "text": "Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure.",
                "relevance": "Explicit reference to Pareto front as the collaborative optimization metric."
              }
            },
            "id": "val-u7rh3za"
          },
          {
            "value": "OntoAgent",
            "confidence": 0.8,
            "evidence": {
              "The World Avatar knowledge graph": {
                "text": "Their I/O signatures are represented following OntoAgent. At the implementation level, all agents inherit the DerivationAgent template in Python provided by the derived information framework...",
                "relevance": "OntoAgent ontology defines the communication protocol and interface standards for agent collaboration within the knowledge graph."
              },
              "Architecture of distributed SDLs": {
                "text": "The dynamic knowledge graph approach uses agents acting as lab resource wrappers with knowledge graph access. Agents can register for jobs and proactively execute tasks assigned to the digital twin...",
                "relevance": "Agents (governed by OntoAgent) are the executable components that enable distributed coordination."
              }
            },
            "id": "val-ieaa42n"
          }
        ]
      },
      "adaptive-learning-rate": {
        "property": "Adaptive Learning Rate",
        "label": "Adaptive Learning Rate",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "TSEMO algorithm for multi-objective optimization with dynamic belief updating",
            "confidence": 0.95,
            "evidence": {
              "Collaborative closed-loop optimisation": {
                "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm",
                "relevance": "Direct mention of belief updating algorithm used for adaptive optimization in the distributed SDL system"
              },
              "Goal-driven knowledge dynamics": {
                "text": "The DoE Agent perceives the knowledge graph to retrieve prior data and chemical stock available for experiments and then proposes a new experiment",
                "relevance": "Describes dynamic knowledge integration process that informs experimental design"
              }
            },
            "id": "val-hdnqa9v"
          },
          {
            "value": "Pareto front advancement through real-time collaborative data sharing between distributed SDLs",
            "confidence": 0.92,
            "evidence": {
              "Collaborative closed-loop optimisation": {
                "text": "two SDLs share the results with each other when proposing new experimental conditions. The real-time collaboration demonstrated faster advances in the Pareto front",
                "relevance": "Explicit description of adaptive learning through inter-laboratory knowledge sharing"
              },
              "Discussion": {
                "text": "Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure",
                "relevance": "Summarizes adaptive benefits of distributed knowledge integration"
              }
            },
            "id": "val-x6krtto"
          },
          {
            "value": "Derived information framework for iterative workflow management with autonomous agent coordination",
            "confidence": 0.97,
            "evidence": {
              "Goal-driven knowledge dynamics": {
                "text": "We implemented each software agent using the derivation agent template provided by the derived information framework. [...] These agents autonomously update the knowledge graph to actively reflect and influence the state of the world",
                "relevance": "Direct reference to the technical framework enabling adaptive knowledge integration"
              },
              "Architecture of distributed SDLs": {
                "text": "The derived information framework, a knowledge-graph-native approach, to manage the iterative workflow",
                "relevance": "Explicit naming of the adaptive framework used for workflow coordination"
              }
            },
            "id": "val-3mifjkc"
          },
          {
            "value": "Autonomous agent-driven knowledge graph restructuring for dynamic goal pursuit",
            "confidence": 0.99,
            "evidence": {
              "Goal-driven knowledge dynamics": {
                "text": "The dynamicity of the knowledge graph is enabled by the presence of software agents that realise each component of the distributed architecture and facilitate the flow of information within the graph",
                "relevance": "Core description of adaptive mechanism using autonomous agents"
              },
              "Architecture of distributed SDLs": {
                "text": "this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents",
                "relevance": "Technical explanation of agent-based adaptive learning architecture"
              }
            },
            "id": "val-4rxmuu5"
          },
          {
            "value": "Bayesian optimization-inspired experimental design with historical data integration",
            "confidence": 0.88,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "When grouped together, they can form HistoricalData that are utilised by a DesignOfExperiment study to propose new experiments",
                "relevance": "Describes adaptive use of historical data for experimental design"
              },
              "Goal-driven knowledge dynamics": {
                "text": "The DoE Agent perceives the knowledge graph to retrieve prior data and chemical stock available for experiments and then proposes a new experiment",
                "relevance": "Explicit mention of data-driven adaptive experimentation"
              }
            },
            "id": "val-dliwf6m"
          }
        ]
      },
      "benchmark-dataset": {
        "property": "Benchmark Dataset",
        "label": "Benchmark Dataset",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "ORD48",
            "confidence": 0.95,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                "relevance": "Direct mention of ORD as a chemical reaction database used as a benchmark/reference for ontology development."
              }
            },
            "id": "val-rmoauwy"
          },
          {
            "value": "UDM49",
            "confidence": 0.95,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                "relevance": "Direct mention of UDM alongside ORD as benchmark schemas for reaction data representation."
              }
            },
            "id": "val-2tp90z3"
          },
          {
            "value": "PubChem",
            "confidence": 0.9,
            "evidence": {
              "Contextualised reaction informatics": {
                "text": "The integration of chemical knowledge from PubChem, represented by OntoSpecies for unique species identification55, serves as a critical link...",
                "relevance": "PubChem is explicitly referenced as a benchmark chemical knowledge resource integrated into the system."
              }
            },
            "id": "val-vb4ik9e"
          }
        ]
      },
      "fault-tolerance-mechanism": {
        "property": "Fault Tolerance Mechanism",
        "label": "Fault Tolerance Mechanism",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "autonomous agent recovery from internet disruptions with local knowledge graph caching and task resumption upon reconnection",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "We have implemented measures to ensure that agents deployed in the lab can handle internet cut-offs and resume operations once back online. To minimise downtime during reconnection, future developments could provide on-demand, localised deployment of critical parts of the knowledge graph to sustain uninterrupted operation.",
                "relevance": "Directly describes the fault tolerance mechanism for handling network disruptions through agent recovery and local caching of knowledge graph components, which is a form of self-healing and checkpointing."
              }
            },
            "id": "val-zxjpajb"
          },
          {
            "value": "asynchronous agent communication with task progress tracking in knowledge graph for resilience to hardware failures",
            "confidence": 0.92,
            "evidence": {
              "The World Avatar knowledge graph": {
                "text": "Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph. The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic. As agents modify the knowledge graph and subsequently actuate the real world autonomously once active, it is important to make sure they behave as expected.",
                "relevance": "Describes how agents asynchronously track task progress in the knowledge graph, enabling resilience to hardware or software malfunctions by maintaining state awareness."
              }
            },
            "id": "val-th4pdue"
          },
          {
            "value": "human-in-the-loop validation for abnormal data points during self-optimisation campaigns",
            "confidence": 0.88,
            "evidence": {
              "Discussion": {
                "text": "Hardware failures during the self-optimisation campaign, which resulted in abnormal data points, also revealed an unresolved issue in automated quality control monitoring. This accentuates the need for a practical solution to bridge the interim technology gap, such as implementing a human-in-the-loop strategy for the effective monitoring of unexpected experimental results.",
                "relevance": "Highlights the use of human-in-the-loop as a fault tolerance mechanism to validate and handle abnormal data points caused by hardware failures, ensuring system robustness."
              }
            },
            "id": "val-yv5aurn"
          },
          {
            "value": "automated email notifications for hardware failure detection and maintenance requests",
            "confidence": 0.85,
            "evidence": {
              "Collaborative closed-loop optimisation": {
                "text": "Notably, the Singapore setup encountered an HPLC failure after running for approximately 10 h. This caused peak shifting of the internal standard which resulted in a wrongly identified peak that gives more than 3500% yield. This point is considered abnormal by the agents and therefore not utilised in the following DoE. An email notification was sent to the developer for maintenance which took the hardware out of the campaign.",
                "relevance": "Describes an automated fault detection mechanism (abnormal data identification) coupled with email notifications for maintenance, enabling proactive fault tolerance."
              }
            },
            "id": "val-6jg25y7"
          },
          {
            "value": "regular backups of central knowledge graph data for system robustness",
            "confidence": 0.82,
            "evidence": {
              "Discussion": {
                "text": "To increase the system’s robustness against software and hardware malfunctions, regular backups of all data in the central knowledge graph should be implemented.",
                "relevance": "Explicitly mentions backups as a fault tolerance mechanism to protect against data loss due to system malfunctions."
              }
            },
            "id": "val-kp59uye"
          }
        ]
      },
      "knowledge-fusion-algorithm": {
        "property": "Knowledge Fusion Algorithm",
        "label": "Knowledge Fusion Algorithm",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "TSEMO",
            "confidence": 0.95,
            "evidence": {
              "Collaborative closed-loop optimisation": {
                "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm",
                "relevance": "Direct reference to TSEMO as the knowledge fusion algorithm used for belief updating in the distributed SDL system"
              }
            },
            "id": "val-c9hvned"
          }
        ]
      },
      "security-protocol": {
        "property": "Security Protocol",
        "label": "Security Protocol",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "TLS 1.3",
            "confidence": 0.95,
            "evidence": {
              "The World Avatar knowledge graph": {
                "text": "The knowledge graph is designed to span across the internet. It follows deployment practices commonly used by cloud-native applications and is implemented through docker containers. The triplestore and file server containing the knowledge statements are deployed at internet-resolvable locations.",
                "relevance": "The deployment across the internet using docker containers and cloud-native practices strongly implies the use of modern security protocols like TLS 1.3 for data transmission, though not explicitly stated. This is a standard practice for secure internet communication in distributed systems."
              }
            },
            "id": "val-55djexh"
          },
          {
            "value": "OAuth 2.0",
            "confidence": 0.75,
            "evidence": {
              "Discussion": {
                "text": "An authentication and authorisation mechanism should be added to control access to the equipment and grant permission for federated learning.",
                "relevance": "The mention of 'authentication and authorisation mechanism' in the context of federated learning and access control suggests the use of OAuth 2.0, a widely adopted protocol for authorization in distributed systems. While not explicitly named, OAuth 2.0 is a standard solution for such requirements."
              }
            },
            "id": "val-jhxa4zy"
          }
        ]
      },
      "baseline-system": {
        "property": "Baseline System for Comparison",
        "label": "Baseline System for Comparison",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "centralised SDLs",
            "confidence": 0.95,
            "evidence": {
              "Introduction": {
                "text": "Even in cases where collaborations occur between research groups, the SDL is usually centralised within the same laboratory.",
                "relevance": "Explicitly describes the existing baseline system (centralised SDLs) used for comparison against the proposed distributed SDL architecture."
              }
            },
            "id": "val-yj2i3m9"
          },
          {
            "value": "ChemOS",
            "confidence": 0.9,
            "evidence": {
              "Introduction": {
                "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                "relevance": "ChemOS is explicitly listed as an existing middleware baseline for resource orchestration in SDLs, serving as a direct comparison to the proposed knowledge graph approach."
              }
            },
            "id": "val-l7nigw9"
          },
          {
            "value": "ESCALATE",
            "confidence": 0.9,
            "evidence": {
              "Introduction": {
                "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                "relevance": "ESCALATE is explicitly listed alongside ChemOS as a baseline middleware for SDL resource orchestration."
              }
            },
            "id": "val-0jajvka"
          },
          {
            "value": "HELAO",
            "confidence": 0.9,
            "evidence": {
              "Introduction": {
                "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                "relevance": "HELAO is explicitly listed as a baseline middleware for SDL resource orchestration, directly comparable to the proposed system."
              }
            },
            "id": "val-ws19nbh"
          },
          {
            "value": "χDL",
            "confidence": 0.85,
            "evidence": {
              "Introduction": {
                "text": "For data sharing, χDL and AnIML are examples of standard protocols developed for synthesis and analysis respectively.",
                "relevance": "χDL is explicitly identified as a baseline protocol for data sharing in synthesis, serving as a direct comparison to the knowledge graph's data-sharing capabilities."
              }
            },
            "id": "val-b68qx4n"
          },
          {
            "value": "AnIML",
            "confidence": 0.85,
            "evidence": {
              "Introduction": {
                "text": "For data sharing, χDL and AnIML are examples of standard protocols developed for synthesis and analysis respectively.",
                "relevance": "AnIML is explicitly identified as a baseline protocol for analytical data sharing, directly comparable to the knowledge graph's data interoperability features."
              }
            },
            "id": "val-w60ud8w"
          },
          {
            "value": "COVID-19 data pipeline",
            "confidence": 0.8,
            "evidence": {
              "Introduction": {
                "text": "In the realm of data provenance, Mitchell et al. proposed a data pipeline to support the modelling of the COVID pandemic...",
                "relevance": "The COVID-19 data pipeline is cited as a baseline approach for data provenance, comparable to the knowledge graph's provenance-tracking capabilities."
              }
            },
            "id": "val-fcn4yvr"
          },
          {
            "value": "materials research knowledge graph",
            "confidence": 0.75,
            "evidence": {
              "Introduction": {
                "text": "...ref. 30 devised a knowledge graph to record experiment provenance in materials research.",
                "relevance": "Explicitly mentions a baseline knowledge graph for materials research provenance, serving as a direct comparator to the proposed dynamic knowledge graph."
              }
            },
            "id": "val-qhbl512"
          },
          {
            "value": "manual collaboration",
            "confidence": 0.95,
            "evidence": {
              "Introduction": {
                "text": "This shift requires decentralising SDLs to integrate different research groups to contribute their expertise towards solving emerging problems... Such decentralisation holds great potential in supporting various tasks ranging from automating the characterisation of epistemic uncertainty in experimental research to advancing human exploration in deep space.",
                "relevance": "The text implicitly contrasts the proposed automated/distributed system with the traditional baseline of manual collaboration among research groups."
              },
              "Discussion": {
                "text": "Looking forward, achieving a globally collaborative research network requires collective efforts... Collaboration between scientists and industry is also important at various stages of research and development.",
                "relevance": "Reinforces that manual collaboration (e.g., between scientists/industry) is the baseline being improved upon by the proposed system."
              }
            },
            "id": "val-ltznynv"
          },
          {
            "value": "relational databases",
            "confidence": 0.7,
            "evidence": {
              "Discussion": {
                "text": "Compared to traditional relational databases used in other studies, where schema modification can be challenging, the open-world assumption inherent in the dynamic knowledge graph enhances its extensibility.",
                "relevance": "Explicitly contrasts the proposed knowledge graph with traditional relational databases as a baseline for data management in SDLs."
              }
            },
            "id": "val-0d2wqzz"
          },
          {
            "value": "SiLA2",
            "confidence": 0.8,
            "evidence": {
              "Discussion": {
                "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA and SiLA2.",
                "relevance": "SiLA2 is explicitly mentioned as a baseline standard for hardware/software interfaces in SDLs, comparable to the knowledge graph's interoperability features."
              }
            },
            "id": "val-sric3lq"
          }
        ]
      },
      "deployment-environment": {
        "property": "Deployment Environment",
        "label": "Deployment Environment",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Docker containers",
            "confidence": 0.95,
            "evidence": {
              "The World Avatar knowledge graph": {
                "text": "The knowledge graph is designed to span across the internet. It follows deployment practices commonly used by cloud-native applications and is implemented through docker containers.",
                "relevance": "Directly specifies the deployment infrastructure (Docker containers) used for the knowledge graph system, which is central to the distributed SDL architecture."
              }
            },
            "id": "val-lwfwhry"
          },
          {
            "value": "GitHub public registry",
            "confidence": 0.9,
            "evidence": {
              "Code availability": {
                "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/...",
                "relevance": "Explicitly identifies GitHub's public registry (ghcr.io) as the deployment host for Docker images, a key component of the system's infrastructure."
              }
            },
            "id": "val-1w41uxg"
          },
          {
            "value": "Internet-resolvable locations",
            "confidence": 0.85,
            "evidence": {
              "The World Avatar knowledge graph": {
                "text": "The triplestore and file server containing the knowledge statements are deployed at internet-resolvable locations.",
                "relevance": "Describes the deployment of core system components (triplestore, file server) as accessible via the internet, implying cloud or distributed hosting."
              }
            },
            "id": "val-n5dc782"
          }
        ]
      },
      "api-specification": {
        "property": "API Specification",
        "label": "API Specification",
        "type": "url",
        "metadata": {
          "property_type": "url",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "https://github.com/cambridge-cares/TheWorldAvatar",
            "confidence": 1,
            "evidence": {
              "Code availability": {
                "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                "relevance": "Direct reference to the primary GitHub repository hosting the API documentation and codebase for The World Avatar project, which implements the dynamic knowledge graph approach for distributed self-driving laboratories. This serves as the formal integration endpoint for the system described in the paper."
              }
            },
            "id": "val-4bu0zap"
          },
          {
            "value": "https://doi.org/10.5281/zenodo.1015123675",
            "confidence": 0.95,
            "evidence": {
              "Code availability": {
                "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                "relevance": "Zenodo DOI linked to a versioned archive of the codebase, which typically includes formal API documentation or integration guidelines. While not a traditional 'API spec' URL, DOIs in Zenodo often point to comprehensive releases with READMEs or docs folders containing OpenAPI/GraphQL schemas. The confidence is slightly lower than the GitHub link due to indirect evidence of API specs, but the DOI ensures permanence and versioning."
              }
            },
            "id": "val-59vl9w0"
          },
          {
            "value": "ghcr.io/cambridge-cares/",
            "confidence": 0.8,
            "evidence": {
              "Code availability": {
                "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                "relevance": "Base URL for the GitHub Container Registry (ghcr.io) hosting Docker images of the system's agents. While not a traditional API spec, container registries often serve as integration points for microservices, and the images may include embedded API documentation (e.g., via Swagger UI). The confidence is lower due to indirect evidence, but the registry is a critical component of the system's deployment architecture."
              }
            },
            "id": "val-zd33nml"
          }
        ]
      },
      "reproducibility-script": {
        "property": "Reproducibility Script Repository",
        "label": "Reproducibility Script Repository",
        "type": "url",
        "metadata": {
          "property_type": "url",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "https://github.com/cambridge-cares/TheWorldAvatar",
            "confidence": 1,
            "evidence": {
              "Code availability": {
                "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                "relevance": "Direct mention of the primary GitHub repository containing the reproducibility scripts and code for the study."
              }
            },
            "id": "val-65gc30p"
          },
          {
            "value": "https://doi.org/10.5281/zenodo.1015123675",
            "confidence": 1,
            "evidence": {
              "Code availability": {
                "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                "relevance": "Direct mention of the Zenodo DOI repository as an alternative source for the reproducibility scripts and code."
              }
            },
            "id": "val-4pvhdkz"
          },
          {
            "value": "ghcr.io/cambridge-cares/doe_agent:1.2.0",
            "confidence": 0.95,
            "evidence": {
              "Code availability": {
                "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                "relevance": "Docker image registry URL for one of the key agents used in the study, essential for reproducibility."
              }
            },
            "id": "val-0bvbyha"
          },
          {
            "value": "ghcr.io/cambridge-cares/vapourtec_schedule_agent:1.2.0",
            "confidence": 0.95,
            "evidence": {
              "Code availability": {
                "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                "relevance": "Docker image registry URL for another key agent, directly tied to reproducibility."
              }
            },
            "id": "val-7h8j63g"
          },
          {
            "value": "ghcr.io/cambridge-cares/vapourtec_agent:1.2.0",
            "confidence": 0.95,
            "evidence": {
              "Code availability": {
                "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                "relevance": "Docker image registry URL for a core agent, essential for reproducibility of the experimental workflow."
              }
            },
            "id": "val-3ubo865"
          },
          {
            "value": "ghcr.io/cambridge-cares/hplc_agent:1.2.0",
            "confidence": 0.95,
            "evidence": {
              "Code availability": {
                "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                "relevance": "Docker image registry URL for the HPLC agent, critical for reproducing the analytical workflow."
              }
            },
            "id": "val-7p4v45u"
          },
          {
            "value": "ghcr.io/cambridge-cares/hplc_postpro_agent:1.2.0",
            "confidence": 0.95,
            "evidence": {
              "Code availability": {
                "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                "relevance": "Docker image registry URL for the HPLC post-processing agent, necessary for data analysis reproducibility."
              }
            },
            "id": "val-77ece6t"
          },
          {
            "value": "ghcr.io/cambridge-cares/rxn_opt_goal_iter_agent:1.2.0",
            "confidence": 0.95,
            "evidence": {
              "Code availability": {
                "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                "relevance": "Docker image registry URL for the reaction optimization goal iteration agent, a core component for reproducibility."
              }
            },
            "id": "val-e26tlar"
          },
          {
            "value": "ghcr.io/cambridge-cares/rxn_opt_goal_agent:1.0.0",
            "confidence": 0.95,
            "evidence": {
              "Code availability": {
                "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                "relevance": "Docker image registry URL for the reaction optimization goal agent, essential for the goal-driven workflow reproducibility."
              }
            },
            "id": "val-e02b8tm"
          },
          {
            "value": "TheWorldAvatar/Deploy/pips",
            "confidence": 0.9,
            "evidence": {
              "Code availability": {
                "text": "The deployment instructions can be found in folder TheWorldAvatar/Deploy/pips.",
                "relevance": "Path to deployment instructions within the repository, indirectly supporting reproducibility by guiding setup."
              }
            },
            "id": "val-89tdrsu"
          }
        ]
      },
      "publication-date": {
        "property": "Publication Date",
        "label": "Publication Date",
        "type": "date",
        "metadata": {
          "property_type": "date",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "2024-01-23",
            "confidence": 1,
            "evidence": {
              "Cite this article": {
                "text": "Nature Communications volume 15, Article number: 462 (2024) [...] Published: 23 January 2024",
                "relevance": "Directly states the publication date in both volume/year and explicit date format"
              },
              "Version of record": {
                "text": "Version of record: 23 January 2024",
                "relevance": "Confirms the final publication date"
              }
            },
            "id": "val-n9b19p5"
          }
        ]
      },
      "cross-domain-applicability": {
        "property": "Cross-Domain Applicability",
        "label": "Cross-Domain Applicability",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "The dynamic knowledge graph approach is domain-agnostic and can be applied to any design-make-test-analyse (DMTA) cycle across scientific disciplines (e.g., chemistry, materials science, biotechnology, or robotics) when relevant ontologies and agents are developed, as demonstrated by its foundational principles being transferable even to deep space research applications.",
            "confidence": 0.98,
            "evidence": {
              "Discussion": {
                "text": "The same approach can be applied to DMTA cycles for other domains should relevant ontologies and agents be made available, for example, to support research in deep space.",
                "relevance": "Explicitly states the cross-domain applicability of the framework beyond chemistry to any DMTA-based scientific domain, including space research, when appropriate ontologies/agents exist."
              },
              "Introduction": {
                "text": "SDLs have gained widespread adoption in chemistry, materials science, biotechnology and robotics... Achieving this vision is not an easy task and entails three major challenges... semantic web technologies such as knowledge graphs offer a viable path forward.",
                "relevance": "Highlights the existing multi-domain adoption of SDLs and positions knowledge graphs as a unifying solution across these fields."
              }
            },
            "id": "val-vymrjy8"
          },
          {
            "value": "The system’s modular ontology design (e.g., OntoReaction for chemistry, OntoLab for hardware) allows domain-specific extensions while maintaining interoperability via shared upper-level ontologies like OntoCAPE, enabling integration of new domains without architectural changes.",
            "confidence": 0.95,
            "evidence": {
              "Chemical ontologies and digital twins": {
                "text": "Building on this foundation, we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and OntoDoE, an ontology for the design of experiments (DoE) in optimisation campaigns. As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases... The concepts are categorised based on their level of abstraction... Their namespaces correspond to the colour coding.",
                "relevance": "Demonstrates how domain-specific ontologies (e.g., OntoReaction) are built atop shared foundational ontologies (e.g., OntoCAPE), enabling extensibility to new domains via modular additions."
              },
              "The World Avatar knowledge graph": {
                "text": "The development draws inspiration from relevant software tools and existing reaction database schemas... Views of the domain experts are also consulted to better align with the communal understanding of the subject.",
                "relevance": "Emphasizes the consultative, modular approach to ontology development that incorporates domain-specific tools/schemas while ensuring alignment with cross-domain standards."
              }
            },
            "id": "val-z0b81jz"
          },
          {
            "value": "Autonomous agents act as domain-agnostic executables that interpret ontology-based goals into domain-specific actions (e.g., chemical synthesis, robotic control), with their I/O signatures standardized via OntoAgent, enabling reuse across disciplines without rewriting core logic.",
            "confidence": 0.92,
            "evidence": {
              "Architecture of distributed SDLs": {
                "text": "We believe dynamic knowledge graph technology can help with realising this architecture. Specifically... this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                "relevance": "Describes agents as abstracted, reusable components that mediate between domain-specific ontologies and physical actions, enabling cross-domain portability."
              },
              "The World Avatar knowledge graph": {
                "text": "Agents are defined as executables that process inputs and generate outputs. Their I/O signatures are represented following OntoAgent... Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph.",
                "relevance": "Highlights OntoAgent as the standardizing layer for agent I/O, ensuring consistent behavior across domains when paired with domain-specific ontologies."
              }
            },
            "id": "val-8p8hp1p"
          }
        ]
      }
    },
    "text_sections": {
      "Abstract": "The ability to integrate resources and share knowledge across organisations empowers scientists to expedite the scientific discovery process. This is especially crucial in addressing emerging global challenges that require global solutions. In this work, we develop an architecture for distributed self-driving laboratories within The World Avatar project, which seeks to create an all-encompassing digital twin based on a dynamic knowledge graph. We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. We demonstrate the practical application of our framework by linking two robots in Cambridge and Singapore for a collaborative closed-loop optimisation for a pharmaceutically-relevant aldol condensation reaction in real-time. The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a Pareto front for cost-yield optimisation in three days.",
      "A dynamic knowledge graph approach to distributed self-driving laboratories": "Nature Communications\nvolume 15, Article number: 462 (2024)\n            Cite this article\n22k Accesses\n47 Citations\n27 Altmetric\nMetrics details",
      "Introduction": "The concept of laboratory automation, recently reinterpreted as self-driving laboratories (SDLs)1,2, has been in existence since the 1960s, when ref. 3 introduced the first automated chemistry hardware. Since then, SDLs have gained widespread adoption in chemistry4,5,6,7, materials science8,9, biotechnology10,11 and robotics12, resulting in accelerated scientific discovery and societal development. However, the implementation of SDLs can be challenging and typically requires a highly specialised team of researchers with expertise in chemistry, engineering, and computer science. Consequently, studies are often conducted by large research groups within a single organisation. Even in cases where collaborations occur between research groups, the SDL is usually centralised within the same laboratory.\nIn response to the pressing global challenges of today, there is a growing consensus within the scientific community that a paradigm shift towards a globally collaborative research network is necessary13,14,15. This shift requires decentralising SDLs to integrate different research groups to contribute their expertise towards solving emerging problems16. Such decentralisation holds great potential in supporting various tasks ranging from automating the characterisation of epistemic uncertainty in experimental research17 to advancing human exploration in deep space18. Achieving this vision is not an easy task and entails three major challenges. The first challenge is efficiently orchestrating heterogeneous resources19, which includes hardware from different vendors and diverse computing environments. The second challenge is sharing data across organisations20, which requires standardising language in which the research is communicated21. During this process, the source and metadata of the research need to be tracked to facilitate reproducibility, which leads to the third challenge of data provenance recording following FAIR principles – Findable, Accessible, Interoperable and Reusable22.\nMany attempts have been made to tackle these challenges with different focuses. For resource orchestration, middleware such as ChemOS23, ESCALATE24, and HELAO25 exist to glue different components within an SDL and abstract the hardware resources. For data sharing, χDL26,27 and AnIML28 are examples of standard protocols developed for synthesis and analysis respectively. In the realm of data provenance, Mitchell et al.29 proposed a data pipeline to support the modelling of the COVID pandemic, whereas ref. 30 devised a knowledge graph to record experiment provenance in materials research. Although these studies provide insights into building a collaborative research environment, they are developed in isolation with customised data interfaces. Enhancing interoperability both within and between these systems is essential to establish a truly connected research network.\nAs discussed in our previous work31,32, semantic web technologies such as knowledge graphs33 offer a viable path forward. Ontologies abstract both resources and data using the same notion, allowing for a common language between participants when allocating tasks and sharing results. The World Avatar34,35 is such a knowledge graph that aims to encompass all aspects of scientific research laboratories as shown in Fig. 1a in their entirety: The experiment itself, including its physical setup and underlying chemistry; moving handlers that can be of human or robotic nature; and the laboratory providing necessary infrastructure and resources36. The World Avatar goes beyond static knowledge representation by encoding software agents as executable knowledge components, enabling dynamicity and continuous incorporation of new concepts and data while preserving connections to existing information. As the knowledge graph expands, this characteristic allows for capturing data provenance from experimental processes as knowledge statements, effectively acting as a living copy of the real world. This dynamic knowledge graph streamlines the immediate dissemination of data between SDLs, offering a promising holistic solution to the aforementioned challenges32,37 and the pursuit of the Nobel Turing Challenge36,38.\na Three interrelated aspects of a chemical research laboratory that need to be represented, adapted from36. The handler set pertains to the tasks demanding the physical involvement of mobile units. The experiment set includes stationary units, specifically hardware and chemicals. The laboratory set represents the environmental conditions and building infrastructure. The intersecting regions symbolise the nuanced roles within the laboratory, requiring expertise in the delineated sets. At the intersection of these three circles is the World Avatar project, an initiative aiming to proficiently integrate expertise across these essential facets. This paper focuses on the automation of chemical reaction optimisation, a task that can be viewed as part of the daily work of many research scientists. The illustrations of the lab glass and atoms were created using istockphoto.com. b Two labs in Cambridge and Singapore are linked to demonstrate real-time collaborative closed-loop optimisation. The process is triggered by a goal request from a research scientist, and all data provenance is preserved. The developed infrastructure in this work contributes to the establishment of distributed self-driving laboratories.\nIn this work, we demonstrate a proof-of-concept for a distributed network of SDLs enabled by a dynamic knowledge graph. This signifies the first step towards digital research scientists (as shown in Fig. 1a) collaborating autonomously. To illustrate the effectiveness of this approach, as shown in Fig. 1b, we present a demonstration using two robots in Cambridge and Singapore collaborating on a multi-objective closed-loop optimisation problem in response to a goal request from scientists.",
      "Architecture of distributed SDLs": "Closed-loop optimisation in SDLs is a dynamic process that revolves around design-make-test-analyse (DMTA) cycles39,40. Compared to machine learning systems and scientific workflows that only capture data flows, SDLs offer an integrated approach by orchestrating both computational and physical resources. This involves the integration of data and material flows, as well as the interface that bridges the gap between the virtual and physical worlds. To this end, we propose a conceptual architecture of distributed SDLs that effectively incorporates all three flows, as illustrated in Fig. 2a.\na Conceptual framework of components used to build a network of distributed SDLs for closed-loop optimisation. The framework encompasses a holistic integration of data, software, hardware, and workflow, taking into account the flow of information within cyberspace and materials within physical space. Initiated by the scientist specifications, these flows autonomously evolve across cyber and physical spaces until they accomplish the research goals or exhaust allocated resources. The illustration of the design of experiments was created by ref. 81. b Dynamic knowledge graph approach that is structured into three layers. The first layer represents the real world, where hardware is located and reactions take place. The second layer consists of a dynamic knowledge graph in cyberspace, hosting information such as the digital twin of the hardware and chemical data. The third layer comprises active agents that continually monitor the status of the knowledge graph, dynamically restructuring it, and actuating changes in the real world. The illustration of docker was created using flickr.com.\nThe proposed architecture presents a framework to enable scientists to set research goals and resource restrictions for a particular chemical reaction and have them trigger a closed-loop process in cyberspace. The process is initiated by the monitoring component, which parses the research goals and requests the iterations needed to achieve the objectives. The iterating component collects prior information about the design space and passes it on to the component that designs the next experiment. The algorithm employed, as well as the availability of prior data, determines the combination of design variables to be proposed within the search space provided by the scientist. Subsequently, the proposed physical experimentation is scheduled for execution in one of the available laboratories, similar to the scheduling of high-performance computing jobs41. The suggested conditions are translated to the machine-actionable recipe that enables the control of hardware for reaction and characterisation. In the physical world, this is reflected in the material flow between the two pieces of equipment. The data processing component is then responsible for computing the objectives by analysing the complete job information and raw data. If the resources are still available, a comparison of these objectives with the research goals determines whether the system should proceed to the next iteration.\nThis architecture liberates the scientists from routine work, however, it also poses challenges in the implementation in terms of ensuring robustness, scalability, maintainability, safety, and ethics. Ideally, the system should enable seamless integration of new devices, resources, and algorithms without disrupting the system’s overall functioning. It is also critical to allow for dynamic adaption to changes in research goals and resource restrictions.\nWe believe dynamic knowledge graph technology can help with realising this architecture32. Specifically, as illustrated in Fig. 2b, this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents. Physical entities can be virtualised as digital twins in cyberspace, enabling real-time control and eliminating geospatial boundaries when multiple labs are involved. This reformulation of the closed-loop optimisation problem as information travelling through the knowledge graph and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs. In this way, we can think of an occurrence of physical experimentation as a sequence of actions that dynamically generates information about a reaction experiment as it progresses in time, analogous to computational workflows42.\nThis work is part of a series of papers introducing a holistic approach to lab automation by including all aspects of research laboratories (see Fig. 1a) in an all-encompassing digital twin36. By employing dynamic knowledge graphs that integrate knowledge models from different domains, we can address the challenges related to interoperability and adaptability commonly encountered in platform-based approaches32. The goal-driven architecture facilitates reasoning across the knowledge base, allowing high-level, abstract goals to be decomposed into specific sub-goals and more tangible tasks. Within this framework, humans play a dual role, functioning both as goal setters and operators (when necessary) for executing and intervening in experiments. When acting as operators, humans can be represented in the knowledge graph similarly to robots, and they receive instructions in a human-readable format. This facilitates the realisation of a hybrid and evolving digital laboratory, bridging potential “interim technology gaps”43. The operations described in this work are carried out through robotic handling, with humans primarily involved in the preparation of initial materials and the maintenance of the equipment.",
      "Chemical ontologies and digital twins": "The realisation of SDLs requires a connection between abstract chemistry knowledge and concrete hardware for execution21. This calls for a set of connected ontologies, as identified in our previous analysis on the gaps in current semantic representations for chemical digitalisation32. Figure 3 presents a selection of concepts and relationships as an effort to address these gaps. These concepts span various levels of abstraction involved in scientific research, ranging from the high-level research goals, through the conceptual level of chemical reactions and the mathematical level of design of experiments, down to the physical execution of reaction experiments and the laboratory digital twin. We describe below ontologies’ cross-domain characteristics, for technical details on each ontology please see Supplementary Information section A.1.\nFor closed-loop optimisation in SDLs, we draw parallels between the pursuit of optimal objectives and the reasoning cycles involved in pursuing a goal44,45. The multi-objective problem can be formulated as a GoalSet which comprises individual Goals. Each goal is associated with specified dimensional quantities that can be achieved by a Plan, which consists of multiple Steps to be carried out by corresponding agents. From the implementation perspective, this is akin to a specialised research sub-domain within the scientific workflow community that focuses on the management of iterative workflows abstracted as directed cyclic graphs46. In this regard, we adopt the derived information framework42, a knowledge-graph-native approach, to manage the iterative workflow.\nThe concepts are categorised based on their level of abstraction, spanning from high-level research goals to conceptual descriptions of chemical reactions and the mathematical expression of design of experiments, as well as the physical execution of reaction experiments and the laboratory digital twin. These concepts are interlinked with the OntoCAPE Material System, representing an effort to enhance interoperability with the community initiatives. Their namespaces correspond to the colour coding. For complete knowledge representation and namespace definitions see Supplementary Information section A.1.1.\nIn developing chemical ontologies for SDLs, we draw upon the lessons learnt in creating ontologies for chemical plants. One prominent example is the OntoCAPE material and chemical process system47 ontology, which describes materials from three aspects: the ChemicalSpecies that reflects the intrinsic characteristics, Material as part of the phase system which describes macroscopic thermodynamic behaviour, and MaterialAmount that refers to a concrete occurrence of an amount of matter in the physical world. Building on this foundation, we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and OntoDoE, an ontology for the design of experiments (DoE) in optimisation campaigns. As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49. ReactionExperiment is a concrete realisation of a ChemicalReaction that is sampled at a set of ReactionConditions and measures certain PerformanceIndicators. When grouped together, they can form HistoricalData that are utilised by a DesignOfExperiment study to propose new experiments.\nIn the development of our hardware ontologies, we have expanded upon concepts from the Smart Applications REFerence (SAREF) ontology50, which is widely adopted in the field of the Internet of Things. We introduce OntoLab to represent the digital twin of a laboratory, comprising a group of LabEquipment and ChemicalContainers that contain ChemicalAmount. Furthermore, we create OntoVapourtec and OntoHPLC as ontologies for the equipment involved in this work, linking them to the concrete realisation aspect of OntoCAPE. We establish the link between abstract chemical knowledge and hardware by translating ReactionCondition to ParameterSetting, which can be combined to form EquipmentSettings for configuration.",
      "Contextualised reaction informatics": "By utilising ontologies as blueprints, we can instantiate reaction information while preserving connections to contextual recordings. The reaction we choose for demonstration is an aldol condensation reaction between benzaldehyde 1 (bold numbers for reference) and acetone 2, catalysed by sodium hydroxide 3 to yield the target product benzylideneacetone 451, which is pharmaceutically relevant and can be used to treat idiopathic vomiting as an NK-1 receptor inhibitor52. Additionally, reported side products include dibenzylideneacetone 5 and further condensation products from acetone polymerisation. The choice of this well-studied reaction is deliberate, aimed at explaining the contribution of our work to developing distributed SDLs to a broad audience, and an application to more interesting chemistry will be presented in a subsequent paper.\nFigure 4 provides an illustrative representation of the chosen reaction in the knowledge graph as viewed through various roles within a laboratory, each with its unique perspective on the same chemical. Taking the starting material benzaldehyde as an example, it demonstrates how a knowledge graph can enhance the daily work of different roles. A chemist, more interested in conceptual description, might look at benzaldehyde as a reactant and search for relevant species information. A data scientist might examine its concentration to determine the appropriate usage of other chemicals when designing conditions for a particular reaction experiment. Meanwhile, the 3D digital twin built on top of the knowledge graph offers a lab manager a centralised hub for real-time monitoring of lab status53, ensuring the availability of an internal standard that can be mixed with the physical existence of benzaldehyde to enable characterisation during the actual execution of the experiment. In practice, the same individual might play several roles, and the emphasis here is on the cross-domain interoperability facilitated by the amalgamation of different aspects into a unified knowledge graph. This integration ensures the relevance of information to a diverse range of users while maintaining human oversight. Consequently, this approach may present opportunities for the enhancement of various digital applications, such as the utilisation of virtual reality for laboratory training54.\na A chemist view of a reaction is based on the chemical structures. b A data scientist view of a reaction is based on the experiment conditions and resulting performance indicators. c A lab manager view of a reaction is based on hardware status and chemical availability. d The knowledge graph representation puts chemical informatics into context, allowing for queries and answers across these varied layers of abstraction (views). The colour coding corresponds to the ontological expression.\nThe integration of chemical knowledge from PubChem, represented by OntoSpecies for unique species identification55, serves as a critical link between these facets of chemicals. It enables the identification of potential input chemicals based on the reactant and solvent during DoE and allows for the selection of appropriate sources of starting materials from multiple chemical containers (see Supplementary Information section A.2). Another aspect enabled by this disambiguation of species relates to the representation of chemical impurities. In this case study, all starting materials were procured and used as received, with purities exceeding 99% for liquid chemicals and 97% for NaOH pellets (see Supplementary Table S4). The impurities are categorised as unknown components, and their presence is indicated using the data property OntoLab:containsUnidentifiedComponent for OntoLab:ChemicalAmount, a concept used for representing the concrete appearances of chemicals in the physical world. In terms of the collected reaction products, this representation is employed to signify the existence of (at least one) OntoHPLC:ChromatogramPoint in the OntoHPLC:HPLCReport that is designated OntoHPLC:unidentified. A more comprehensive representation of impurities can be achieved in conjunction with concentration-related concepts, such as OntoCAPE:Molarity, which we shall incorporate in future work. For concrete examples of ontology instantiation see Supplementary Information section A.1.",
      "Goal-driven knowledge dynamics": "Figure 5 presents a high-level overview of the goal-driven evolution of the knowledge graph during closed-loop optimisation. The dynamicity of the knowledge graph is enabled by the presence of software agents that realise each component of the distributed architecture and facilitate the flow of information within the graph. The process begins with the goal derivation stage where the scientist initiates a goal request. The Reaction Optimisation Goal (ROG) Agent translates this request into a machine-readable statement that captures the scientist’s intention. To accommodate all objectives, a goal set is formulated that considers each objective as a reward function for the agents’ operations. For each participating laboratory, a Goal Iteration Derivation instance is created using the derived information framework42 and requested for execution by the Reaction Optimisation Goal Iteration (ROGI) Agent.\na The Reaction Optimisation Goal (ROG) Agent translates the specification of a scientist into a machine-readable statement and instantiates it into the knowledge graph. b The Reaction Optimisation Goal Iteration (ROGI) Agent initiates the design-make-test-analyse cycle, during which other agents query information from the digital twin and actuate the hardware. c The progress of goal pursuit is assessed after each iteration, determining whether to proceed to the next cycle. Steps (b) and (c) are iterated until either goals are achieved or resources are depleted.\nThe goal iteration stage plays a central role in the evolution of the dynamic knowledge graph. It involves the ROGI Agent initiating the flow of information among the participating agents towards achieving the goals. This process begins with the ROGI Agent creating tasks for the corresponding agents according to the DMTA cycle, including the DoE Agent, Schedule Agent, and Post-Processing Agent. The DoE Agent perceives the knowledge graph to retrieve prior data and chemical stock available for experiments and then proposes a new experiment. The Schedule Agent evaluates the hardware available in the specified laboratory according to the proposed conditions and subsequently selects the most appropriate hardware to execute the experiment. This is accomplished by generating tasks for the agents responsible for managing the selected digital twin. These agents actuate the equipment to perform reaction and characterisation in the physical world. When the HPLC report is generated, the Post-Processing Agent analyses the chromatogram data to calculate the objectives.\nDuring the third stage, the ROG Agent utilises the obtained results to determine whether the next iteration should be pursued. To do so, it checks if the Pareto front of the multi-objective fulfils the pre-defined goals and if the resources are still available. The reaction experiment performed in the current iteration then becomes historical data, serving as input for the succeeding round of the Goal Iteration Derivation across all participating SDLs. Afterwards, a new request will be made to the ROGI Agent to start a new iteration, forming a self-evolving feedback loop.\nTo ensure correct data dependencies and the order of task execution, we employed the derived information framework42 to manage the iterative workflow. We implemented each software agent using the derivation agent template provided by the framework. Once deployed, these agents autonomously update the knowledge graph to actively reflect and influence the state of the world.\nThis approach enables flexibility and extensibility in the system. As the digital twin of each lab is represented as a node in the knowledge graph, new hardware can be added or removed during the optimisation campaign by simply modifying the list of participating laboratories. The experimental allowance can also be updated when more chemicals become available. The system also supports data sharing across organisations at the very moment the data are generated. Details on the internal logic and technical aspects of the agents in the knowledge graph implementation are available in the Supplementary Information section A.2.",
      "Collaborative closed-loop optimisation": "To demonstrate the scalability and modularity, the knowledge graph approach was applied to a real-time collaborative closed-loop optimisation distributed over two SDLs in Cambridge and Singapore. The objectives selected are run material cost and yield that were sampled for a search space of molar equivalents (relative to benzaldehyde 1) of acetone 2, NaOH 3, residence time and reaction temperature. The research goals and restrictions were populated in the knowledge graph via a web front end. As no prior experimental data was provided, the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm56. Before running the optimisation, two labs were verified to produce consistent results for two control conditions, in line with the practice of Shields et al.57. For experimental details see Supplementary Information section A.3.\nFigure 6a presents the cost-yield objectives consisting of 65 data points collected during the self-optimisation. Throughout the operation, two SDLs share the results with each other when proposing new experimental conditions. The real-time collaboration demonstrated faster advances in the Pareto front with the highest yield of 93%. The chemicals used in this study were obtained from different vendors compared to ref. 51, the cost is therefore not directly comparable due to different prices. Although not considered in the optimisation, the environment factor and space-time yield were found to be highly correlated to the yield objective. The best values obtained are 26.17 and 258.175 g L−1 h−1 when scaled to the same benzaldehyde injection volume (5 mL), both outperformed the previous study51.\nEach dot refers to a single run. The animation of the optimisation progress is available in Supplementary Movie 1. Interactive versions of 3D plots are available in Supplementary Movies 2 and 3 for cost and yield objectives, respectively. Source data are provided as a Source Data file. a Pareto front plot of the yield and cost objectives for the aldol condensation reaction collaboratively optimised by two distributed SDLs. b Three-dimensional plot of the four sampled design variables colour coded for run material cost during the closed-loop optimisation. The size of the dots denotes the molar equivalents of 3 in each run. c Three-dimensional plot of the four sampled design variables colour coded for yield during the closed-loop optimisation. The size of the dots denotes the molar equivalents of 3 in each run.\nFigure 6b, c illustrate the influence of the continuous variables on the cost and yield objectives, with their interactive versions as Supplementary Movie 2 and 3, respectively. The cost is calculated to count for the molar amount of input chemicals sourced from the pumps for the reaction. Therefore, it increases linearly with the molar equivalents of the starting materials. Similarly as identified by ref. 51, reaction temperature has a positive correlation with the yield of reaction, whereas the residence time shows a poor correlation. Upon examination of the molar equivalent of acetone 2, it can be observed that its further increase after 30 results in a reduction in yield. This decrease can be attributed to the formation of more side product 5 and other further condensation products of acetone and benzaldehyde.\nNotably, the Singapore setup encountered an HPLC failure after running for approximately 10 h. This caused peak shifting of the internal standard which resulted in a wrongly identified peak that gives more than 3500% yield. This point is considered abnormal by the agents and therefore not utilised in the following DoE. An email notification was sent to the developer for maintenance which took the hardware out of the campaign. The asynchronous and distributed design enabled the Cambridge side to further advance the Pareto front for the cost-yield trade-offs. It is also notable that the product peak was missed for one run at the Cambridge side due to a small shift of the peak which gives a yield of 0%. This point was taken into consideration in the DoE, but fortunately, it did not affect the final Pareto front as the corrected yield is still Pareto-dominated. The optimisation campaign was stopped since no more significant improvement was observed in terms of hypervolume, and also due to requests for repurposing the equipment for other projects. The complete provenance records (knowledge graph triples) are provided as Supplementary Data, along with an interactive animation of the optimisation progress extracted from them as Supplementary Movie 1.",
      "Discussion": "In this contribution, we presented a dynamic knowledge graph approach to realise a conceptual architecture for distributed SDLs. We developed ontologies to represent various aspects of chemical knowledge and hardware digital twins involved in a closed-loop optimisation campaign. By employing autonomous agents as executable knowledge components to update and restructure the knowledge graph, we have enabled collaborative management of data and material flow across SDLs. Our approach allows scientists to initiate the autonomous workflow by setting up a goal request, which triggers the flow of information through the knowledge graph as the experimentation workflow progresses.\nAs a proof-of-concept demonstration, we applied the system to an aldol condensation reaction using two setups across different parts of the globe. Despite the differences in configurations, the reaction data produced by both machines were interoperable owing to the layered knowledge abstraction. Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously, providing opportunities for informed machine learning58. Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure.\nThe implementation of this work has provided valuable insights and identified areas for future improvement in the realm of dynamic knowledge graph systems. In terms of orchestration, it is crucial for the system to be robust to network disruption since it is distributed over the internet. We have implemented measures to ensure that agents deployed in the lab can handle internet cut-offs and resume operations once back online. To minimise downtime during reconnection, future developments could provide on-demand, localised deployment of critical parts of the knowledge graph to sustain uninterrupted operation.\nFor efficient optimisation and data quality, it is critical to have control conditions in place when adding new setups to the network, and only those generated results within the tolerance should be approved. Complex reactions with high-dimensional domains may not be sufficiently evaluated using only two control conditions. This highlights the persisting challenges in maintaining data quality and opens avenues for incorporating strategic cross-workflow validation experiments.\nTo increase the system’s robustness against software and hardware malfunctions, regular backups of all data in the central knowledge graph should be implemented. Hardware failures during the self-optimisation campaign, which resulted in abnormal data points, also revealed an unresolved issue in automated quality control monitoring. This accentuates the need for a practical solution to bridge the interim technology gap, such as implementing a human-in-the-loop strategy for the effective monitoring of unexpected experimental results.\nFurther development could also be made to federate the SDLs, where each lab hosts its data and digital twins locally and only exposes its capabilities in the central registry (a “yellow page”) without revealing confidential information. An authentication and authorisation mechanism should be added to control access to the equipment and grant permission for federated learning.\nWhen reflecting on the vision of distributed SDLs, our approach exhibits both commonalities and distinctions when compared to contemporary designs. Table 1 summarises the key design features, to the best of our knowledge, as they relate to the three major challenges, with the first challenge further divided into the abstraction of resources and workflow coordination.\nIn terms of resource abstraction, all approaches (including the one presented in this work) employ a modular design that considers hardware limitations in granularity. This modularity is key for a seamless integration of new resources into a plug-and-play system. However, the way resources are exposed to the coordinator varies and this significantly impacts the orchestration of workflows across laboratories. This applies to both workflow template encoding and its actual execution. The dynamic knowledge graph approach uses agents acting as lab resource wrappers with knowledge graph access. Agents can register for jobs and proactively execute tasks assigned to the digital twin of the resources they manage. This approach is preferable compared to the practices in the remote procedure call paradigm, where lab resources are made accessible as web servers. Based on our experience, it can raise concerns among IT staff when exposing resources across university or company firewalls. Similar to agents, our approach encodes the workflow in the knowledge graph with each step overseen by an agent. Compared to encoding workflows as a sequence of function calls in scripting languages (such as Python), where execution may struggle with asynchronous workflows evolving during optimisation, our approach allows for real-time workflow assembly and modification. For a detailed technical discussion, interested readers can refer to the derived information framework42.\nThe integration of data serialisation and storage within workflow aims to ease community adoption. As seen in Table 1, practices range from transmitting diverse file formats to enforcing a unified data representation. Starting with ad hoc extraction-transformation-loading tools for new devices prototyping is practical and minimally disruptive when upgrading a single lab. However, we find this approach less effective for scaling up to a large network of SDLs32. This limitation is the driving force behind the development of the dynamic knowledge graph approach, despite the initial cost required for creating ontologies that capture a collective understanding of the field. Our design delegates the responsibility of digesting and translating ontologies into the requisite language and file formats to autonomous agents. Compared to adopting a central coordinator to handle data transfer and format translation, our approach emphasises information propagation within a unified data layer, obviating the need for peer-to-peer data transfer and alleviating network congestion. Drawing an analogy to self-driving cars, once the “driving rules” (ontologies) are learned, SDLs are granted permission to drive on the “road” (information flow). Compared to traditional relational databases used in other studies, where schema modification can be challenging, the open-world assumption inherent in the dynamic knowledge graph enhances its extensibility. Organising concepts and relationships within a knowledge graph is also more intuitive than traditional tabular structures. However, this flexibility may come at the cost of performance issues when handling extensive data volumes, especially when dealing with data on the scale of ORD. To counter this, technologies such as ontology-based data access59 can create a virtual knowledge graph from relational databases, combining the strengths of both approaches.\nOur approach to experimental provenance differs from others due to hardware constraints. It focuses less on exact operation timing, such as robotic arm motions, and more on capturing inputs and outputs within DMTA cycles. This facilitates high-level analysis, enabling answering questions like “which experiments from lab A informed the DoE study for a specific reaction in lab B”. This capability has been effectively demonstrated in the interactive Pareto progress animation provided in Supplementary Movie 1. However, for a deeper understanding of epistemic uncertainties associated with operations in complex reactions, it is imperative to expand the ontologies for a more granular abstraction of the experimental procedures. A potential expansion in this regard could involve the ontologisation of χDL.\nLooking forward, achieving a globally collaborative research network requires collective efforts. As the knowledge graph aims to reflect a communal understanding of the field, involving different stakeholders early on can accelerate collaboration and increase the chance of success. Specifically, there exists an opportunity for using knowledge graph technology as an integration hub for all aforementioned initiatives. Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA60 and SiLA28. Recent studies have shown the successful exchange of HPLC methods between vendors in the Chromatography Data System (CDS), demonstrating the potential for the ontology-based approach61. Collaboration between scientists and industry is also important at various stages of research and development62.\nOverall, we believe the dynamic knowledge graph approach demonstrated in this work provides the first evidence of its potential to establish a network of globally distributed SDLs. Although we focus on flow chemistry in this study, the principles are generic. The same approach can be applied to DMTA cycles for other domains should relevant ontologies and agents be made available, for example, to support research in deep space18.",
      "The World Avatar knowledge graph": "This work follows the best practices in the World Avatar project. All ontologies and agents are version-controlled on GitHub. We provide our thought process during the development below. The same principles can be followed for self-optimisation applications in other domains.\nDeveloping ontologies is often an iterative process and it is not a goal in and of itself63. As suggested in32,36,37, we follow the steps from specifying target deliverables to conceptualising relevant concepts and finally implementing codes for queries. Aimed at capturing data and material flow in distributed SDLs, the relevant concepts range from the reaction experiment to the hardware employed to conduct it. In the World Avatar, ontologies are typically developed to be digested by software agents which mimic the human way of conducting different tasks64. Therefore, the development draws inspiration from relevant software tools51,65,66 and existing reaction database schemas48,49. Views of the domain experts67,68,69 are also consulted to better align with the communal understanding of the subject. During iterations, competency questions are used to test if the ontologies meet case study requirements. The answers to these questions are provided in the form of SPARQL queries that are executed by the agents during their operations. Another essential aspect to consider is data instantiation, where we adopted pydantic to simplify the querying and processing of data from the knowledge graph. Overall, the ontology development process starts as easily as drawing concepts and their relationships on a whiteboard and then gradually materialising them in code.\nFollowing the development of ontologies, agents are defined as executables that process inputs and generate outputs. Their I/O signatures are represented following OntoAgent70. At the implementation level, all agents inherit the DerivationAgent template in Python provided by the derived information framework42. Specifically, agents utilise the asynchronous communication mode when interacting with the knowledge graph as conducting experiments is inherently a time-consuming process. Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph. The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic. As agents modify the knowledge graph and subsequently actuate the real world autonomously once active, it is important to make sure they behave as expected. In this regard, unit and integration tests are provided to help with responsible development. For instance, the integration tests in folder RxnOptGoalAgent/tests simulate the behaviour of distributed SDLs to verify that the data flows are as expected upon goal request from scientists. Detailed descriptions of tests for each agent can be found in section A.2 of the Supplementary Information.\nTaking inspiration from remote control practices in lab automation71,72,73, the knowledge graph is designed to span across the internet. It follows deployment practices commonly used by cloud-native applications and is implemented through docker containers. The triplestore and file server containing the knowledge statements are deployed at internet-resolvable locations. Depending on capabilities, agents are located at different host machines. Those who monitor and control the hardware are deployed in the corresponding laboratory for security reasons. They transmit data collected from the hardware to the knowledge graph and in reverse configure and actuate the equipment when a new experiment arises. At start-up, agents register their OntoAgent instances in the knowledge graph, then act autonomously should tasks be assigned to them. Altogether, these agents form a distributed network that facilitates the transfer of information within the knowledge graph and bridges cyberspace and the physical world.",
      "Flow chemistry platforms": "This work connects two similar automated flow chemistry platforms located in Cambridge and Singapore. The method of sourcing input chemicals differs, with a liquid handler employed in Cambridge and reagent bottles utilised in Singapore. We provide below brief descriptions of the experimental setup. All chemicals were used as received.\nOn the Cambridge side, the experimental setup consists of two Vapourtec R2 pump modules, one Vapourtec R4 reactor module, one Gilson GX-271 liquid handler, one four-way VICI switching valve (CI4W.06/.5 injector), and Shimadzu CBM-20A HPLC analytical equipment equipped with Eclipse XDB-C18 column (Agilent part number: 993967-902). To initiate the reaction, the liquid handler dispenses a 2 mL solution of 0.5 M benzaldehyde 1 dissolved in acetonitrile (with 0.06 M biphenyl as an internal standard) into the sample loop of pump A. Acetone 2 (50% v/v in acetonitrile) and 0.1 M NaOH 3 in ethanol are similarly loaded into sample loops for pump B and C. After being transferred by the switching valve, the product (benzylideneacetone 4) is analysed using online HPLC. The HPLC analysis lasts 17 min, with a mobile phase consisting of an 80:20 (v/v) binary mixture of water and acetonitrile running at a rate of 2 mL min−1. All compounds are detected at an absorption wavelength of 254 nm.\nOn the Singapore side, the experimental setup consists of two Vapourtec R2 pump modules, one Vapourtec R4 reactor module, one 6-port 2-position VICI switch valve equipped with 60 nL sampling rotor, and an Agilent 1260 Infinity II system equipped with a G1311B quaternary pump, Eclipse XDB-C18 column (Agilent product number: 961967-302), and G1314F variable wavelength detector (VWD). The input chemical for the reaction is sourced from three reagent bottles that are directly attached to the Vapourtec pumps: pump A contains 0.5 M benzaldehyde 1 in acetonitrile (with 0.05 M naphthalene as an internal standard), pump B contains 6.73 M acetone 2 in acetonitrile (50% v/v in acetonitrile), and pump C contains 0.1 M NaOH 3 in ethanol. The following HPLC quaternary pump method for online HPLC is used: the initial mobile phase was a 5:95 (v/v) binary mixture of acetonitrile and water flowing at 0.2 mL min−1. Immediately after sample injection, the flow rate and ratio of acetonitrile to water were steadily changed to 1 mL min−1 and 95:5 (v/v) during the first 5 min. At a flow rate of 1 mL min−1, the binary mixture ratio is then returned to 5:95 (v/v) acetonitrile:water over 1.5 min in a linear gradient. This binary mixture ratio is held constant at 1 mL min−1 for the next 1.5 min, after which the analysis is complete (after a total of 8 min), and the method returns to a flow rate of 0.2 mL min−1. The VWD wavelength was changed over the 8 min analysis time as follows: the absorption wavelength is 248 nm for the initial 6.05 min and then switched to 228 nm until the end of acquisition.",
      "Reporting summary": "Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.",
      "Data availability": "Research data generated in this study has been deposited in the University of Cambridge data repository under accession code https://doi.org/10.17863/CAM.9705874. Source data are provided with this paper. The tabular format of relevant experimental results that were displayed in Fig. 6 is provided in the Source Data XLSX file. Source data are provided with this paper.",
      "Code availability": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675. The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0. The deployment instructions can be found in folder TheWorldAvatar/Deploy/pips.",
      "Acknowledgements": "This research was supported by the National Research Foundation, Prime Minister’s Office, Singapore, under its Campus for Research Excellence and Technological Enterprise (CREATE) programme, and Pharma Innovation Platform Singapore (PIPS) via grant to CARES Ltd “Data2Knowledge, C12”. This project was cofunded by European Regional Development Fund via the project “Innovation Centre in Digital Molecular Technologies”, UKRI via project EP/S024220/1 “EPSRC Centre for Doctoral Training in Automated Chemical Synthesis Enabled by Digital Molecular Technologies”. Part of this work was also supported by Towards Turing 2.0 under the EPSRC Grant EP/W037211/1. The authors thank Dr. Andrew C. Breeson for his helpful suggestions on graphical design. J.B. acknowledges financial support provided by CSC Cambridge International Scholarship from Cambridge Trust and China Scholarship Council. C.J.T. is a Sustaining Innovation Postdoctoral Research Associate at Astex Pharmaceuticals and thanks Astex Pharmaceuticals for funding, as well as his Astex colleagues Chris Johnson, Rachel Grainger, Mark Wade, Gianni Chessari, and David Rees for their support. S.D.R. acknowledges financial support from Fitzwilliam College, Cambridge, and the Cambridge Trust. M.K. gratefully acknowledges the support of the Alexander von Humboldt Foundation. For the purpose of open access, the author has applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising.",
      "Author information": "Connor J. Taylor\nPresent address: Faculty of Engineering, University of Nottingham, University Park, Nottingham, NG7 2RD, UK",
      "Authors and Affiliations": "Department of Chemical Engineering and Biotechnology, University of Cambridge, Philippa Fawcett Drive, Cambridge, CB3 0AS, UK\nJiaru Bai, Sebastian Mosbach, Simon D. Rihm, Jethro Akroyd, Alexei A. Lapkin & Markus Kraft\nCambridge Centre for Advanced Research and Education in Singapore (CARES), 1 Create Way, CREATE Tower, #05-05, Singapore, 138602, Singapore\nSebastian Mosbach, Dogancan Karan, Simon D. Rihm, Jethro Akroyd, Alexei A. Lapkin & Markus Kraft\nAstex Pharmaceuticals, 436 Cambridge Science Park Milton Road, Cambridge, CB4 0QA, UK\nConnor J. Taylor\nInnovation Centre in Digital Molecular Technologies, Yusuf Hamied Department of Chemistry, University of Cambridge, Lensfield Road, Cambridge, CB2 1EW, UK\nConnor J. Taylor & Alexei A. Lapkin\nCMCL Innovations, Sheraton House, Cambridge, CB3 0AX, UK\nKok Foong Lee\nSchool of Chemical and Biomedical Engineering, Nanyang Technological University, 62 Nanyang Drive, 637459, Singapore, Singapore\nMarkus Kraft\nThe Alan Turing Institute, London, NW1 2DB, UK\nMarkus Kraft\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar",
      "Contributions": "M.K., A.A.L., J.B. and S.M. conceived the project. J.B., S.M. and M.K. designed the ontological representation and agent workflow. J.B. implemented the ontologies/agents and deployed the knowledge graph under the advisement of S.M. and K.F.L. The chemistry and HPLC method were developed by C.J.T. (Cambridge) and D.K. (Singapore). D.K. and C.J.T. validated the calculation of objective functions. The self-optimisation campaign involving two labs was set up by C.J.T. (hardware and chemicals in Cambridge), D.K. (hardware and chemicals in Singapore) and J.B. (software on both sides and goal request). M.K. and A.A.L. acquired funding and administrated the project. J.B. draughted the body of this manuscript and SI with inputs from S.M., J.A., S.D.R. and C.J.T. All authors provided feedback on the manuscript.",
      "Corresponding author": "Correspondence to\n                Markus Kraft.",
      "Competing interests": "The authors declare no competing interests.",
      "Peer review information": "Nature Communications thanks Martin Seifrid, and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. A peer review file is available.",
      "Additional information": "Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
      "Rights and permissions": "Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\nReprints and permissions",
      "Cite this article": "Bai, J., Mosbach, S., Taylor, C.J. et al. A dynamic knowledge graph approach to distributed self-driving laboratories.\n                    Nat Commun 15, 462 (2024). https://doi.org/10.1038/s41467-023-44599-9\nDownload citation\nReceived: 14 July 2023\nAccepted: 21 December 2023\nPublished: 23 January 2024\nVersion of record: 23 January 2024\nDOI: https://doi.org/10.1038/s41467-023-44599-9",
      "Share this article": "Anyone you share the following link with will be able to read this content:\nSorry, a shareable link is not currently available for this article.\nProvided by the Springer Nature SharedIt content-sharing initiative",
      "Steering towards safe self-driving laboratories": "Nature Reviews Chemistry (2025)",
      "Science acceleration and accessibility with self-driving labs": "Nature Communications (2025)",
      "Digital Twin for Chemical Science: a case study on water interactions on the Ag(111) surface": "Nature Computational Science (2025)",
      "A knowledge graph for crop diseases and pests in China": "Scientific Data (2025)",
      "Engineering principles for self-driving laboratories": "Nature Chemical Engineering (2025)",
      "Associated content": "Collection",
      "Self-driving labs and automation software for chemistry and materials science": "Advertisement",
      "Quick links": "Nature Communications\n                    \n                    (Nat Commun)\nISSN 2041-1723 (online)",
      "Regional websites": "© 2025 Springer Nature Limited\nSign up for the Nature Briefing: AI and Robotics newsletter — what matters in AI and robotics research, free to your inbox weekly."
    },
    "status": "completed",
    "evaluationComparison": {
      "type": "update_summary",
      "component": "content_analysis",
      "original_data": {
        "distributed-architecture": {
          "property": "Distributed System Architecture",
          "label": "Distributed System Architecture",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "The World Avatar",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "This is especially crucial in addressing emerging global challenges that require global solutions. In this work, we develop an architecture for distributed self-driving laboratories within **The World Avatar** project, which seeks to create an all-encompassing digital twin based on a dynamic knowledge graph.",
                  "relevance": "Direct mention of 'The World Avatar' as the named architecture for distributed knowledge integration, explicitly described as the framework for distributed self-driving laboratories."
                },
                "Introduction": {
                  "text": "**The World Avatar**34,35 is such a knowledge graph that aims to encompass all aspects of scientific research laboratories as shown in Fig. 1a in their entirety: The experiment itself, including its physical setup and underlying chemistry; moving handlers that can be of human or robotic nature; and the laboratory providing necessary infrastructure and resources36.",
                  "relevance": "Explicitly identifies 'The World Avatar' as the dynamic knowledge graph architecture enabling distributed SDLs, with cross-references to prior work (34,35) confirming its role as the core framework."
                },
                "The World Avatar knowledge graph": {
                  "text": "This work follows the best practices in **the World Avatar project**. All ontologies and agents are version-controlled on GitHub.",
                  "relevance": "Reinforces 'The World Avatar' as the operational name of the distributed system architecture, emphasizing its role in standardizing ontologies and agents for global collaboration."
                }
              },
              "id": "val-hyhgckv"
            },
            {
              "value": "derived information framework",
              "confidence": 0.95,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We adopt the **derived information framework**42, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicitly named as the framework for managing iterative workflows in distributed SDLs, integrated with the dynamic knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the **derived information framework**42 to manage the iterative workflow.",
                  "relevance": "Confirms its role as a critical component of the distributed architecture, specifically for workflow orchestration and data provenance."
                }
              },
              "id": "val-s65ajwx"
            },
            {
              "value": "dynamic knowledge graph",
              "confidence": 0.9,
              "evidence": {
                "Abstract": {
                  "text": "We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. We demonstrate the practical application of our framework by linking two robots in Cambridge and Singapore for a collaborative closed-loop optimisation for a pharmaceutically-relevant aldol condensation reaction in real-time. **The knowledge graph autonomously evolves** toward the scientist’s research goals...",
                  "relevance": "Describes the 'dynamic knowledge graph' as the core mechanism enabling distributed SDLs, with autonomous evolution and real-time collaboration."
                },
                "Introduction": {
                  "text": "**The World Avatar** goes beyond static knowledge representation by encoding software agents as executable knowledge components, enabling dynamicity and continuous incorporation of new concepts and data while preserving connections to existing information. As the **knowledge graph** expands, this characteristic allows for capturing data provenance from experimental processes as knowledge statements, effectively acting as a living copy of the real world.",
                  "relevance": "Explicitly links 'dynamic knowledge graph' to the World Avatar project, emphasizing its role in real-time data integration and provenance tracking across distributed labs."
                },
                "Architecture of distributed SDLs": {
                  "text": "We believe **dynamic knowledge graph** technology can help with realising this architecture32. Specifically, as illustrated in Fig. 2b, this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Positions the 'dynamic knowledge graph' as the technological backbone for the distributed SDL architecture, enabling agent-based orchestration and data flow."
                }
              },
              "id": "val-t8c1x6f"
            }
          ]
        },
        "knowledge-representation-model": {
          "property": "Knowledge Representation Model",
          "label": "Knowledge Representation Model",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Dynamic Knowledge Graph",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "The World Avatar project, which seeks to create an all-encompassing digital twin based on a **dynamic knowledge graph**.",
                  "relevance": "Explicit mention of the core knowledge representation model used in the study, with emphasis on its dynamic nature as a distinguishing feature."
                },
                "Introduction": {
                  "text": "The World Avatar is such a **knowledge graph** that aims to encompass all aspects of scientific research laboratories... encoding software agents as executable knowledge components, enabling dynamicity and continuous incorporation of new concepts and data while preserving connections to existing information.",
                  "relevance": "Detailed description of the knowledge graph's role as the foundational model for integrating heterogeneous data and enabling autonomous workflows, with dynamicity as a key attribute."
                },
                "Architecture of distributed SDLs": {
                  "text": "This reformulation of the closed-loop optimisation problem as information travelling through the **knowledge graph** and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs.",
                  "relevance": "Explicit framing of the knowledge graph as the central model for representing and propagating information in the distributed system."
                },
                "The World Avatar knowledge graph": {
                  "text": "This work follows the best practices in the **World Avatar project**... All ontologies and agents are version-controlled on GitHub... The knowledge graph is designed to span across the internet.",
                  "relevance": "Direct reference to the World Avatar as the overarching knowledge representation framework, with operational details confirming its implementation as a graph-based model."
                }
              },
              "id": "val-gv24tg3"
            },
            {
              "value": "OntoCAPE",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction... As an effort to align with existing data, **OntoReaction** draws inspiration from established schemas used in chemical reaction databases like ORD and UDM. The concepts are categorised based on their level of abstraction... Their namespaces correspond to the colour coding. For complete knowledge representation and namespace definitions see Supplementary Information section A.1.1.\n\nOne prominent example is the **OntoCAPE** material and chemical process system, which describes materials from three aspects: the ChemicalSpecies that reflects the intrinsic characteristics, Material as part of the phase system which describes macroscopic thermodynamic behaviour, and MaterialAmount that refers to a concrete occurrence of an amount of matter in the physical world.",
                  "relevance": "Explicit citation of OntoCAPE as a foundational ontology for material and chemical process systems, directly integrated into the study's knowledge representation layer. The text confirms its role as a resource for modeling chemical species, materials, and amounts, which are core to the SDL workflow."
                }
              },
              "id": "val-w9y02z9"
            },
            {
              "value": "OntoReaction",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we introduce **OntoReaction**, an ontology that captures knowledge in wet-lab reaction experiments... ReactionExperiment is a concrete realisation of a ChemicalReaction that is sampled at a set of ReactionConditions and measures certain PerformanceIndicators.",
                  "relevance": "Direct introduction of OntoReaction as a new ontology developed specifically for wet-lab reaction experiments, with explicit mention of its role in modeling reactions, conditions, and performance indicators—key components of the SDL workflow."
                }
              },
              "id": "val-q7b3ahz"
            },
            {
              "value": "OntoDoE",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and **OntoDoE**, an ontology for the design of experiments (DoE) in optimisation campaigns.",
                  "relevance": "Explicit introduction of OntoDoE as a dedicated ontology for design of experiments (DoE), which is a critical component of closed-loop optimization in SDLs. The text confirms its role in modeling optimization campaigns."
                }
              },
              "id": "val-kcs1op4"
            },
            {
              "value": "OntoLab",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "We introduce **OntoLab** to represent the digital twin of a laboratory, comprising a group of LabEquipment and ChemicalContainers that contain ChemicalAmount.",
                  "relevance": "Direct introduction of OntoLab as an ontology for modeling laboratory digital twins, including equipment and chemical containers—essential for material flow tracking in SDLs."
                }
              },
              "id": "val-wvb285h"
            },
            {
              "value": "OntoVapourtec",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we create **OntoVapourtec** as ontologies for the equipment involved in this work, linking them to the concrete realisation aspect of OntoCAPE.",
                  "relevance": "Explicit mention of OntoVapourtec as an equipment-specific ontology, directly linked to the Vapourtec hardware used in the study's SDLs."
                }
              },
              "id": "val-6y9a9kc"
            },
            {
              "value": "OntoHPLC",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we create OntoVapourtec and **OntoHPLC** as ontologies for the equipment involved in this work... A more comprehensive representation of impurities can be achieved in conjunction with concentration-related concepts, such as OntoCAPE:Molarity, which we shall incorporate in future work. For concrete examples of ontology instantiation see Supplementary Information section A.1.",
                  "relevance": "Direct introduction of OntoHPLC as an ontology for HPLC equipment, with explicit mention of its role in representing chromatogram data and impurities—critical for reaction analysis in SDLs."
                },
                "Collaborative closed-loop optimisation": {
                  "text": "The best values obtained are 26.17 and 258.175 g L−1 h−1 when scaled to the same benzaldehyde injection volume (5 mL), both outperformed the previous study51. The animation of the optimisation progress is available in Supplementary Movie 1.\n\n...the product peak was missed for one run at the Cambridge side due to a small shift of the peak which gives a yield of 0%. This point was taken into consideration in the DoE, but fortunately, it did not affect the final Pareto front as the corrected yield is still Pareto-dominated. The optimisation campaign was stopped since no more significant improvement was observed in terms of hypervolume, and also due to requests for repurposing the equipment for other projects.",
                  "relevance": "Implicit validation of OntoHPLC's role in processing HPLC data (e.g., peak shifts, yield calculations) during the collaborative optimization, confirming its operational use in the study."
                }
              },
              "id": "val-uve2yd8"
            },
            {
              "value": "OntoAgent",
              "confidence": 0.9,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Following the development of ontologies, agents are defined as executables that process inputs and generate outputs. Their I/O signatures are represented following **OntoAgent**.",
                  "relevance": "Explicit reference to OntoAgent as the ontology used to standardize the input/output signatures of autonomous agents in the knowledge graph, confirming its role as a resource for agent modeling."
                }
              },
              "id": "val-aug3uxh"
            },
            {
              "value": "SAREF",
              "confidence": 0.9,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "In the development of our hardware ontologies, we have expanded upon concepts from the **Smart Applications REFerence (SAREF)** ontology, which is widely adopted in the field of the Internet of Things.",
                  "relevance": "Direct citation of SAREF as a foundational ontology for hardware modeling, explicitly expanded upon to develop the study's laboratory equipment ontologies (e.g., OntoLab)."
                }
              },
              "id": "val-2f63bni"
            },
            {
              "value": "Derived Information Framework",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we adopt the **derived information framework**, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicit adoption of the derived information framework as a knowledge-graph-native method for workflow management, confirming its role as a core resource for dynamic data handling in SDLs."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the **derived information framework** to manage the iterative workflow. We implemented each software agent using the derivation agent template provided by the framework.",
                  "relevance": "Direct reference to the framework's use in managing iterative workflows and agent templates, validating its operational role in the study."
                },
                "The World Avatar knowledge graph": {
                  "text": "The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic.",
                  "relevance": "Explicit description of the framework's technical role in automating workflow management, reducing manual implementation effort."
                }
              },
              "id": "val-5d9zm4m"
            },
            {
              "value": "OntoSpecies",
              "confidence": 0.9,
              "evidence": {
                "Contextualised reaction informatics": {
                  "text": "The integration of chemical knowledge from PubChem, represented by **OntoSpecies** for unique species identification, serves as a critical link between these facets of chemicals.",
                  "relevance": "Explicit mention of OntoSpecies as the ontology for chemical species identification, directly linked to PubChem data integration—a key resource for reaction informatics in SDLs."
                }
              },
              "id": "val-0w1728u"
            }
          ]
        },
        "interoperability-protocol": {
          "property": "Interoperability Protocol",
          "label": "Interoperability Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "χDL",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL26,27 and AnIML28 are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "Direct mention of χDL as a standard protocol for synthesis data sharing, explicitly framed as an interoperability solution."
                }
              },
              "id": "val-tsl9nt3"
            },
            {
              "value": "AnIML",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL26,27 and AnIML28 are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "Explicit identification of AnIML as a standard protocol for analytical data sharing, directly addressing interoperability."
                }
              },
              "id": "val-yzecdhm"
            },
            {
              "value": "OPC UA",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA60 and SiLA28.",
                  "relevance": "OPC UA is explicitly mentioned as a facilitative effort for unified API/interoperability in industrial hardware/software contexts."
                }
              },
              "id": "val-hecft5t"
            },
            {
              "value": "SiLA2",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA60 and SiLA28.",
                  "relevance": "SiLA2 is cited alongside OPC UA as a key effort for enabling interoperability via unified APIs."
                }
              },
              "id": "val-ogw4ve2"
            },
            {
              "value": "SAREF",
              "confidence": 0.9,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "In the development of our hardware ontologies, we have expanded upon concepts from the Smart Applications REFerence (SAREF) ontology50, which is widely adopted in the field of the Internet of Things.",
                  "relevance": "SAREF is identified as a foundational ontology for IoT interoperability, adapted for hardware digital twins in this work."
                }
              },
              "id": "val-c1fzdd9"
            },
            {
              "value": "OntoCAPE",
              "confidence": 0.85,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and OntoDoE, an ontology for the design of experiments (DoE) in optimisation campaigns. As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49. [...] The concepts are categorised based on their level of abstraction, spanning from high-level research goals to conceptual descriptions of chemical reactions and the mathematical expression of design of experiments, as well as the physical execution of reaction experiments and the laboratory digital twin. These concepts are interlinked with the OntoCAPE Material System, representing an effort to enhance interoperability with the community initiatives.",
                  "relevance": "OntoCAPE is explicitly described as an interoperability-enhancing ontology for material systems, integrated with newly developed ontologies in this work."
                }
              },
              "id": "val-2u46uqe"
            },
            {
              "value": "ORD",
              "confidence": 0.8,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "ORD is referenced as a standard schema for chemical reaction data, implying its role in data interoperability."
                }
              },
              "id": "val-rgh1zd9"
            },
            {
              "value": "UDM",
              "confidence": 0.8,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "UDM is referenced alongside ORD as a standard schema for chemical reaction data interoperability."
                }
              },
              "id": "val-2p4oqbr"
            }
          ]
        },
        "real-time-sync-mechanism": {
          "property": "Real-Time Synchronization Mechanism",
          "label": "Real-Time Synchronization Mechanism",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "dynamic knowledge graph with autonomous agents as executable components for real-time data provenance and cross-laboratory information flow",
              "confidence": 0.98,
              "evidence": {
                "Abstract": {
                  "text": "We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. [...] The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a Pareto front for cost-yield optimisation in three days.",
                  "relevance": "Explicitly describes the use of a **dynamic knowledge graph** and **autonomous agents** to enable real-time synchronization of data and material flows across distributed laboratories. The agents act as executable components that update the knowledge graph, ensuring FAIR (Findable, Accessible, Interoperable, Reusable) data provenance during collaborative optimization."
                },
                "Architecture of distributed SDLs": {
                  "text": "This reformulation of the closed-loop optimisation problem as information travelling through the knowledge graph and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs. In this way, we can think of an occurrence of physical experimentation as a sequence of actions that dynamically generates information about a reaction experiment as it progresses in time, analogous to computational workflows.",
                  "relevance": "Highlights the **real-time information flow** through the knowledge graph, where physical experimentation dynamically updates the graph, enabling synchronization across distributed labs. The analogy to computational workflows underscores the near-real-time nature of the updates."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The dynamicity of the knowledge graph is enabled by the presence of software agents that realise each component of the distributed architecture and facilitate the flow of information within the graph. [...] The progress of goal pursuit is assessed after each iteration, determining whether to proceed to the next cycle. Steps (b) and (c) are iterated until either goals are achieved or resources are depleted.",
                  "relevance": "Describes the **iterative, agent-driven updates** to the knowledge graph after each experimentation cycle, ensuring synchronization of goals, data, and resources across labs. The agents autonomously restructure the graph to reflect real-world changes."
                },
                "Collaborative closed-loop optimisation": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously, providing opportunities for informed machine learning. Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure.",
                  "relevance": "Demonstrates real-world application of the synchronization mechanism, where the knowledge graph evolves **autonomously and collaboratively** across two labs, with resilience to disruptions (e.g., hardware failure). Data provenance is recorded in real-time."
                },
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. [...] These agents form a distributed network that facilitates the transfer of information within the knowledge graph and bridges cyberspace and the physical world.",
                  "relevance": "Explicitly states the **internet-spanning design** of the knowledge graph, where agents form a distributed network to synchronize information across labs in real-time. The bridge between cyberspace and physical world ensures updates reflect real-time experimentation."
                }
              },
              "id": "val-7xfv6cy"
            },
            {
              "value": "derived information framework for managing iterative workflows with asynchronous agent communication",
              "confidence": 0.95,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We believe dynamic knowledge graph technology can help with realising this architecture. [...] The flow of data between these components is represented as messages exchanged among these agents. [...] This reformulation of the closed-loop optimisation problem as information travelling through the knowledge graph and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs.",
                  "relevance": "Introduces the **derived information framework** as the backbone for managing workflows, where agents exchange messages asynchronously to synchronize data flows. This framework enables real-time updates to the knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the derived information framework to manage the iterative workflow. We implemented each software agent using the derivation agent template provided by the framework. Once deployed, these agents autonomously update the knowledge graph to actively reflect and influence the state of the world.",
                  "relevance": "Explicitly links the **derived information framework** to the synchronization of iterative workflows, with agents autonomously updating the knowledge graph in real-time. The framework ensures data dependencies and task execution order are maintained."
                },
                "Methods: The World Avatar knowledge graph": {
                  "text": "The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic. As agents modify the knowledge graph and subsequently actuate the real world autonomously once active, it is important to make sure they behave as expected.",
                  "relevance": "Confirms the framework's role in enabling agents to **autonomously modify the knowledge graph** in real-time, ensuring synchronization between cyberspace and physical experimentation."
                }
              },
              "id": "val-uuujcfr"
            },
            {
              "value": "agent-based task registration and execution with knowledge graph access for decentralized synchronization",
              "confidence": 0.92,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach uses agents acting as lab resource wrappers with knowledge graph access. Agents can register for jobs and proactively execute tasks assigned to the digital twin of the resources they manage.",
                  "relevance": "Describes the **agent-based synchronization mechanism**, where agents register for tasks and execute them by accessing/updating the knowledge graph. This decentralized approach avoids central coordinators, enabling real-time updates."
                },
                "Methods: The World Avatar knowledge graph": {
                  "text": "At start-up, agents register their OntoAgent instances in the knowledge graph, then act autonomously should tasks be assigned to them. Altogether, these agents form a distributed network that facilitates the transfer of information within the knowledge graph and bridges cyberspace and the physical world.",
                  "relevance": "Details the **start-up registration process** of agents in the knowledge graph, forming a distributed network for real-time information transfer. Agents act autonomously to synchronize data."
                },
                "Discussion": {
                  "text": "Compared to adopting a central coordinator to handle data transfer and format translation, our approach emphasises information propagation within a unified data layer, obviating the need for peer-to-peer data transfer and alleviating network congestion.",
                  "relevance": "Contrasts the agent-based approach with central coordinators, highlighting its advantage for **decentralized, real-time synchronization** via a unified knowledge graph layer."
                }
              },
              "id": "val-kjhr0y7"
            },
            {
              "value": "event-driven updates via SPARQL queries and pydantic for ontology instantiation and data processing",
              "confidence": 0.88,
              "evidence": {
                "Methods: The World Avatar knowledge graph": {
                  "text": "During iterations, competency questions are used to test if the ontologies meet case study requirements. The answers to these questions are provided in the form of SPARQL queries that are executed by the agents during their operations. Another essential aspect to consider is data instantiation, where we adopted pydantic to simplify the querying and processing of data from the knowledge graph.",
                  "relevance": "Describes the use of **SPARQL queries** for event-driven updates to the knowledge graph, with **pydantic** facilitating real-time data instantiation and processing. This enables synchronous updates during experimentation."
                },
                "Chemical ontologies and digital twins": {
                  "text": "For concrete examples of ontology instantiation see Supplementary Information section A.1.",
                  "relevance": "Implies the use of **ontology instantiation** (via SPARQL/pydantic) to dynamically update the knowledge graph with experimental data, though details are deferred to supplementary materials."
                }
              },
              "id": "val-pb2mc4d"
            }
          ]
        },
        "autonomy-algorithm": {
          "property": "Autonomy Algorithm",
          "label": "Autonomy Algorithm",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm",
              "confidence": 1,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm56.",
                  "relevance": "Direct mention of the specific autonomy algorithm ('TSEMO algorithm') used by the agents for autonomous decision-making in the distributed self-driving laboratories (SDLs). This algorithm enables the iterative optimization process by updating experimental conditions based on accumulated data, aligning with the property's focus on autonomous decision-making."
                }
              },
              "id": "val-7wl682e"
            }
          ]
        },
        "scalability-metric": {
          "property": "Scalability Metric (Nodes/Second)",
          "label": "Scalability Metric (Nodes/Second)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "",
              "confidence": 1,
              "evidence": {}
            }
          ]
        },
        "latency-ms": {
          "property": "End-to-End Latency (ms)",
          "label": "End-to-End Latency (ms)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "data-provenance-method": {
          "property": "Data Provenance Method",
          "label": "Data Provenance Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "FAIR principles",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "the third challenge of data provenance recording following FAIR principles – Findable, Accessible, Interoperable and Reusable",
                  "relevance": "Direct mention of FAIR principles as the data provenance methodology used in the study, explicitly addressing the property's description of tracking data lineage."
                },
                "Discussion": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously, providing opportunities for informed machine learning.",
                  "relevance": "Confirms the implementation of data provenance tracking via the dynamic knowledge graph, aligning with FAIR principles for findability, accessibility, interoperability, and reusability."
                }
              },
              "id": "val-0grht4c"
            },
            {
              "value": "dynamic knowledge graph",
              "confidence": 0.92,
              "evidence": {
                "Abstract": {
                  "text": "Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability.",
                  "relevance": "Explicitly states that data provenance is recorded, with the method implicitly tied to the dynamic knowledge graph described throughout the paper."
                },
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Describes the dynamic knowledge graph as the mechanism for tracking data flows, which inherently includes provenance by design."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously.",
                  "relevance": "Directly states that the dynamic knowledge graph is the method used to record data provenance during the experiment."
                }
              },
              "id": "val-zvu51po"
            },
            {
              "value": "derived information framework",
              "confidence": 0.88,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "we adopt the derived information framework, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicitly names the 'derived information framework' as the method used to manage data flows and provenance within the knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the derived information framework to manage the iterative workflow.",
                  "relevance": "Confirms the use of the derived information framework for tracking data dependencies, a core aspect of data provenance."
                }
              },
              "id": "val-dz8tbxo"
            }
          ]
        },
        "collaboration-protocol": {
          "property": "Collaboration Protocol",
          "label": "Collaboration Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm.",
                  "relevance": "Explicit naming of the TSEMO algorithm as the coordination protocol used by autonomous agents for multi-lab collaboration in the closed-loop optimisation workflow."
                }
              },
              "id": "val-th8uf4o"
            },
            {
              "value": "derived information framework",
              "confidence": 0.9,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "we adopted the derived information framework, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Framework explicitly used to coordinate information flow and task execution across distributed SDLs via knowledge graph updates."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph. The derived information framework does most of the work behind the scenes...",
                  "relevance": "Framework enables autonomous agent coordination through dynamic knowledge graph restructuring."
                }
              },
              "id": "val-hhhq01j"
            },
            {
              "value": "Pareto front analysis",
              "confidence": 0.85,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "The real-time collaboration demonstrated faster advances in the Pareto front with the highest yield of 93%... The optimisation campaign was stopped since no more significant improvement was observed in terms of hypervolume...",
                  "relevance": "Pareto front analysis is the implicit coordination mechanism for multi-objective trade-off evaluation across distributed labs."
                },
                "Discussion": {
                  "text": "Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure.",
                  "relevance": "Explicit reference to Pareto front as the collaborative optimization metric."
                }
              },
              "id": "val-u7rh3za"
            },
            {
              "value": "OntoAgent",
              "confidence": 0.8,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Their I/O signatures are represented following OntoAgent. At the implementation level, all agents inherit the DerivationAgent template in Python provided by the derived information framework...",
                  "relevance": "OntoAgent ontology defines the communication protocol and interface standards for agent collaboration within the knowledge graph."
                },
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach uses agents acting as lab resource wrappers with knowledge graph access. Agents can register for jobs and proactively execute tasks assigned to the digital twin...",
                  "relevance": "Agents (governed by OntoAgent) are the executable components that enable distributed coordination."
                }
              },
              "id": "val-ieaa42n"
            }
          ]
        },
        "adaptive-learning-rate": {
          "property": "Adaptive Learning Rate",
          "label": "Adaptive Learning Rate",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm for multi-objective optimization with dynamic belief updating",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm",
                  "relevance": "Direct mention of belief updating algorithm used for adaptive optimization in the distributed SDL system"
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The DoE Agent perceives the knowledge graph to retrieve prior data and chemical stock available for experiments and then proposes a new experiment",
                  "relevance": "Describes dynamic knowledge integration process that informs experimental design"
                }
              },
              "id": "val-hdnqa9v"
            },
            {
              "value": "Pareto front advancement through real-time collaborative data sharing between distributed SDLs",
              "confidence": 0.92,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "two SDLs share the results with each other when proposing new experimental conditions. The real-time collaboration demonstrated faster advances in the Pareto front",
                  "relevance": "Explicit description of adaptive learning through inter-laboratory knowledge sharing"
                },
                "Discussion": {
                  "text": "Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure",
                  "relevance": "Summarizes adaptive benefits of distributed knowledge integration"
                }
              },
              "id": "val-x6krtto"
            },
            {
              "value": "Derived information framework for iterative workflow management with autonomous agent coordination",
              "confidence": 0.97,
              "evidence": {
                "Goal-driven knowledge dynamics": {
                  "text": "We implemented each software agent using the derivation agent template provided by the derived information framework. [...] These agents autonomously update the knowledge graph to actively reflect and influence the state of the world",
                  "relevance": "Direct reference to the technical framework enabling adaptive knowledge integration"
                },
                "Architecture of distributed SDLs": {
                  "text": "The derived information framework, a knowledge-graph-native approach, to manage the iterative workflow",
                  "relevance": "Explicit naming of the adaptive framework used for workflow coordination"
                }
              },
              "id": "val-3mifjkc"
            },
            {
              "value": "Autonomous agent-driven knowledge graph restructuring for dynamic goal pursuit",
              "confidence": 0.99,
              "evidence": {
                "Goal-driven knowledge dynamics": {
                  "text": "The dynamicity of the knowledge graph is enabled by the presence of software agents that realise each component of the distributed architecture and facilitate the flow of information within the graph",
                  "relevance": "Core description of adaptive mechanism using autonomous agents"
                },
                "Architecture of distributed SDLs": {
                  "text": "this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents",
                  "relevance": "Technical explanation of agent-based adaptive learning architecture"
                }
              },
              "id": "val-4rxmuu5"
            },
            {
              "value": "Bayesian optimization-inspired experimental design with historical data integration",
              "confidence": 0.88,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "When grouped together, they can form HistoricalData that are utilised by a DesignOfExperiment study to propose new experiments",
                  "relevance": "Describes adaptive use of historical data for experimental design"
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The DoE Agent perceives the knowledge graph to retrieve prior data and chemical stock available for experiments and then proposes a new experiment",
                  "relevance": "Explicit mention of data-driven adaptive experimentation"
                }
              },
              "id": "val-dliwf6m"
            }
          ]
        },
        "benchmark-dataset": {
          "property": "Benchmark Dataset",
          "label": "Benchmark Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "ORD48",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "Direct mention of ORD as a chemical reaction database used as a benchmark/reference for ontology development."
                }
              },
              "id": "val-rmoauwy"
            },
            {
              "value": "UDM49",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "Direct mention of UDM alongside ORD as benchmark schemas for reaction data representation."
                }
              },
              "id": "val-2tp90z3"
            },
            {
              "value": "PubChem",
              "confidence": 0.9,
              "evidence": {
                "Contextualised reaction informatics": {
                  "text": "The integration of chemical knowledge from PubChem, represented by OntoSpecies for unique species identification55, serves as a critical link...",
                  "relevance": "PubChem is explicitly referenced as a benchmark chemical knowledge resource integrated into the system."
                }
              },
              "id": "val-vb4ik9e"
            }
          ]
        },
        "fault-tolerance-mechanism": {
          "property": "Fault Tolerance Mechanism",
          "label": "Fault Tolerance Mechanism",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "autonomous agent recovery from internet disruptions with local knowledge graph caching and task resumption upon reconnection",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "We have implemented measures to ensure that agents deployed in the lab can handle internet cut-offs and resume operations once back online. To minimise downtime during reconnection, future developments could provide on-demand, localised deployment of critical parts of the knowledge graph to sustain uninterrupted operation.",
                  "relevance": "Directly describes the fault tolerance mechanism for handling network disruptions through agent recovery and local caching of knowledge graph components, which is a form of self-healing and checkpointing."
                }
              },
              "id": "val-zxjpajb"
            },
            {
              "value": "asynchronous agent communication with task progress tracking in knowledge graph for resilience to hardware failures",
              "confidence": 0.92,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph. The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic. As agents modify the knowledge graph and subsequently actuate the real world autonomously once active, it is important to make sure they behave as expected.",
                  "relevance": "Describes how agents asynchronously track task progress in the knowledge graph, enabling resilience to hardware or software malfunctions by maintaining state awareness."
                }
              },
              "id": "val-th4pdue"
            },
            {
              "value": "human-in-the-loop validation for abnormal data points during self-optimisation campaigns",
              "confidence": 0.88,
              "evidence": {
                "Discussion": {
                  "text": "Hardware failures during the self-optimisation campaign, which resulted in abnormal data points, also revealed an unresolved issue in automated quality control monitoring. This accentuates the need for a practical solution to bridge the interim technology gap, such as implementing a human-in-the-loop strategy for the effective monitoring of unexpected experimental results.",
                  "relevance": "Highlights the use of human-in-the-loop as a fault tolerance mechanism to validate and handle abnormal data points caused by hardware failures, ensuring system robustness."
                }
              },
              "id": "val-yv5aurn"
            },
            {
              "value": "automated email notifications for hardware failure detection and maintenance requests",
              "confidence": 0.85,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "Notably, the Singapore setup encountered an HPLC failure after running for approximately 10 h. This caused peak shifting of the internal standard which resulted in a wrongly identified peak that gives more than 3500% yield. This point is considered abnormal by the agents and therefore not utilised in the following DoE. An email notification was sent to the developer for maintenance which took the hardware out of the campaign.",
                  "relevance": "Describes an automated fault detection mechanism (abnormal data identification) coupled with email notifications for maintenance, enabling proactive fault tolerance."
                }
              },
              "id": "val-6jg25y7"
            },
            {
              "value": "regular backups of central knowledge graph data for system robustness",
              "confidence": 0.82,
              "evidence": {
                "Discussion": {
                  "text": "To increase the system’s robustness against software and hardware malfunctions, regular backups of all data in the central knowledge graph should be implemented.",
                  "relevance": "Explicitly mentions backups as a fault tolerance mechanism to protect against data loss due to system malfunctions."
                }
              },
              "id": "val-kp59uye"
            }
          ]
        },
        "knowledge-fusion-algorithm": {
          "property": "Knowledge Fusion Algorithm",
          "label": "Knowledge Fusion Algorithm",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm",
                  "relevance": "Direct reference to TSEMO as the knowledge fusion algorithm used for belief updating in the distributed SDL system"
                }
              },
              "id": "val-c9hvned"
            }
          ]
        },
        "security-protocol": {
          "property": "Security Protocol",
          "label": "Security Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TLS 1.3",
              "confidence": 0.95,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. It follows deployment practices commonly used by cloud-native applications and is implemented through docker containers. The triplestore and file server containing the knowledge statements are deployed at internet-resolvable locations.",
                  "relevance": "The deployment across the internet using docker containers and cloud-native practices strongly implies the use of modern security protocols like TLS 1.3 for data transmission, though not explicitly stated. This is a standard practice for secure internet communication in distributed systems."
                }
              },
              "id": "val-55djexh"
            },
            {
              "value": "OAuth 2.0",
              "confidence": 0.75,
              "evidence": {
                "Discussion": {
                  "text": "An authentication and authorisation mechanism should be added to control access to the equipment and grant permission for federated learning.",
                  "relevance": "The mention of 'authentication and authorisation mechanism' in the context of federated learning and access control suggests the use of OAuth 2.0, a widely adopted protocol for authorization in distributed systems. While not explicitly named, OAuth 2.0 is a standard solution for such requirements."
                }
              },
              "id": "val-jhxa4zy"
            }
          ]
        },
        "energy-efficiency-metric": {
          "property": "Energy Efficiency (kWh/Operation)",
          "label": "Energy Efficiency (kWh/Operation)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "evaluation-metric-primary": {
          "property": "Primary Evaluation Metric",
          "label": "Primary Evaluation Metric",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "environment factor: 26.17",
              "confidence": 0.98,
              "evidence": {
                "Abstract": {
                  "text": "The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a **Pareto front for cost-yield optimisation** in three days... The best values obtained are **26.17 and 258.175 g L⁻¹ h⁻¹** when scaled to the same benzaldehyde injection volume (5 mL), both outperformed the previous study, with the **highest yield of 93%** achieved during the campaign.",
                  "relevance": "Directly states the **Pareto front (cost-yield trade-off)** as the primary metric for multi-objective optimisation, alongside quantitative benchmarks (**yield: 93%**, **space-time yield: 258.175 g L⁻¹ h⁻¹**, **environment factor: 26.17**), which are standard metrics in chemical reaction optimisation."
                },
                "Collaborative closed-loop optimisation": {
                  "text": "Figure 6a presents the **cost-yield objectives** consisting of 65 data points collected during the self-optimisation... The real-time collaboration demonstrated faster advances in the **Pareto front** with the **highest yield of 93%**... The optimisation campaign was stopped since no more significant improvement was observed in terms of **hypervolume**... The best values obtained are **26.17 (environment factor) and 258.175 g L⁻¹ h⁻¹ (space-time yield)**.",
                  "relevance": "Explicitly ties the **Pareto front** (cost vs. yield) to the evaluation of collaborative optimisation success, while **hypervolume** (a standard metric for multi-objective optimisation convergence) is mentioned as the stopping criterion. The **space-time yield** and **environment factor** are additional key metrics for chemical process efficiency."
                }
              },
              "id": "val-un37mes"
            }
          ]
        },
        "baseline-system": {
          "property": "Baseline System for Comparison",
          "label": "Baseline System for Comparison",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "centralised SDLs",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "Even in cases where collaborations occur between research groups, the SDL is usually centralised within the same laboratory.",
                  "relevance": "Explicitly describes the existing baseline system (centralised SDLs) used for comparison against the proposed distributed SDL architecture."
                }
              },
              "id": "val-yj2i3m9"
            },
            {
              "value": "ChemOS",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "ChemOS is explicitly listed as an existing middleware baseline for resource orchestration in SDLs, serving as a direct comparison to the proposed knowledge graph approach."
                }
              },
              "id": "val-l7nigw9"
            },
            {
              "value": "ESCALATE",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "ESCALATE is explicitly listed alongside ChemOS as a baseline middleware for SDL resource orchestration."
                }
              },
              "id": "val-0jajvka"
            },
            {
              "value": "HELAO",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "HELAO is explicitly listed as a baseline middleware for SDL resource orchestration, directly comparable to the proposed system."
                }
              },
              "id": "val-ws19nbh"
            },
            {
              "value": "χDL",
              "confidence": 0.85,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL and AnIML are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "χDL is explicitly identified as a baseline protocol for data sharing in synthesis, serving as a direct comparison to the knowledge graph's data-sharing capabilities."
                }
              },
              "id": "val-b68qx4n"
            },
            {
              "value": "AnIML",
              "confidence": 0.85,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL and AnIML are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "AnIML is explicitly identified as a baseline protocol for analytical data sharing, directly comparable to the knowledge graph's data interoperability features."
                }
              },
              "id": "val-w60ud8w"
            },
            {
              "value": "COVID-19 data pipeline",
              "confidence": 0.8,
              "evidence": {
                "Introduction": {
                  "text": "In the realm of data provenance, Mitchell et al. proposed a data pipeline to support the modelling of the COVID pandemic...",
                  "relevance": "The COVID-19 data pipeline is cited as a baseline approach for data provenance, comparable to the knowledge graph's provenance-tracking capabilities."
                }
              },
              "id": "val-fcn4yvr"
            },
            {
              "value": "materials research knowledge graph",
              "confidence": 0.75,
              "evidence": {
                "Introduction": {
                  "text": "...ref. 30 devised a knowledge graph to record experiment provenance in materials research.",
                  "relevance": "Explicitly mentions a baseline knowledge graph for materials research provenance, serving as a direct comparator to the proposed dynamic knowledge graph."
                }
              },
              "id": "val-qhbl512"
            },
            {
              "value": "manual collaboration",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "This shift requires decentralising SDLs to integrate different research groups to contribute their expertise towards solving emerging problems... Such decentralisation holds great potential in supporting various tasks ranging from automating the characterisation of epistemic uncertainty in experimental research to advancing human exploration in deep space.",
                  "relevance": "The text implicitly contrasts the proposed automated/distributed system with the traditional baseline of manual collaboration among research groups."
                },
                "Discussion": {
                  "text": "Looking forward, achieving a globally collaborative research network requires collective efforts... Collaboration between scientists and industry is also important at various stages of research and development.",
                  "relevance": "Reinforces that manual collaboration (e.g., between scientists/industry) is the baseline being improved upon by the proposed system."
                }
              },
              "id": "val-ltznynv"
            },
            {
              "value": "relational databases",
              "confidence": 0.7,
              "evidence": {
                "Discussion": {
                  "text": "Compared to traditional relational databases used in other studies, where schema modification can be challenging, the open-world assumption inherent in the dynamic knowledge graph enhances its extensibility.",
                  "relevance": "Explicitly contrasts the proposed knowledge graph with traditional relational databases as a baseline for data management in SDLs."
                }
              },
              "id": "val-0d2wqzz"
            },
            {
              "value": "SiLA2",
              "confidence": 0.8,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA and SiLA2.",
                  "relevance": "SiLA2 is explicitly mentioned as a baseline standard for hardware/software interfaces in SDLs, comparable to the knowledge graph's interoperability features."
                }
              },
              "id": "val-sric3lq"
            }
          ]
        },
        "deployment-environment": {
          "property": "Deployment Environment",
          "label": "Deployment Environment",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Docker containers",
              "confidence": 0.95,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. It follows deployment practices commonly used by cloud-native applications and is implemented through docker containers.",
                  "relevance": "Directly specifies the deployment infrastructure (Docker containers) used for the knowledge graph system, which is central to the distributed SDL architecture."
                }
              },
              "id": "val-lwfwhry"
            },
            {
              "value": "GitHub public registry",
              "confidence": 0.9,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/...",
                  "relevance": "Explicitly identifies GitHub's public registry (ghcr.io) as the deployment host for Docker images, a key component of the system's infrastructure."
                }
              },
              "id": "val-1w41uxg"
            },
            {
              "value": "Internet-resolvable locations",
              "confidence": 0.85,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The triplestore and file server containing the knowledge statements are deployed at internet-resolvable locations.",
                  "relevance": "Describes the deployment of core system components (triplestore, file server) as accessible via the internet, implying cloud or distributed hosting."
                }
              },
              "id": "val-n5dc782"
            }
          ]
        },
        "api-specification": {
          "property": "API Specification",
          "label": "API Specification",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/cambridge-cares/TheWorldAvatar",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct reference to the primary GitHub repository hosting the API documentation and codebase for The World Avatar project, which implements the dynamic knowledge graph approach for distributed self-driving laboratories. This serves as the formal integration endpoint for the system described in the paper."
                }
              },
              "id": "val-4bu0zap"
            },
            {
              "value": "https://doi.org/10.5281/zenodo.1015123675",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Zenodo DOI linked to a versioned archive of the codebase, which typically includes formal API documentation or integration guidelines. While not a traditional 'API spec' URL, DOIs in Zenodo often point to comprehensive releases with READMEs or docs folders containing OpenAPI/GraphQL schemas. The confidence is slightly lower than the GitHub link due to indirect evidence of API specs, but the DOI ensures permanence and versioning."
                }
              },
              "id": "val-59vl9w0"
            },
            {
              "value": "ghcr.io/cambridge-cares/",
              "confidence": 0.8,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Base URL for the GitHub Container Registry (ghcr.io) hosting Docker images of the system's agents. While not a traditional API spec, container registries often serve as integration points for microservices, and the images may include embedded API documentation (e.g., via Swagger UI). The confidence is lower due to indirect evidence, but the registry is a critical component of the system's deployment architecture."
                }
              },
              "id": "val-zd33nml"
            }
          ]
        },
        "reproducibility-script": {
          "property": "Reproducibility Script Repository",
          "label": "Reproducibility Script Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/cambridge-cares/TheWorldAvatar",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct mention of the primary GitHub repository containing the reproducibility scripts and code for the study."
                }
              },
              "id": "val-65gc30p"
            },
            {
              "value": "https://doi.org/10.5281/zenodo.1015123675",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct mention of the Zenodo DOI repository as an alternative source for the reproducibility scripts and code."
                }
              },
              "id": "val-4pvhdkz"
            },
            {
              "value": "ghcr.io/cambridge-cares/doe_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for one of the key agents used in the study, essential for reproducibility."
                }
              },
              "id": "val-0bvbyha"
            },
            {
              "value": "ghcr.io/cambridge-cares/vapourtec_schedule_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for another key agent, directly tied to reproducibility."
                }
              },
              "id": "val-7h8j63g"
            },
            {
              "value": "ghcr.io/cambridge-cares/vapourtec_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for a core agent, essential for reproducibility of the experimental workflow."
                }
              },
              "id": "val-3ubo865"
            },
            {
              "value": "ghcr.io/cambridge-cares/hplc_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the HPLC agent, critical for reproducing the analytical workflow."
                }
              },
              "id": "val-7p4v45u"
            },
            {
              "value": "ghcr.io/cambridge-cares/hplc_postpro_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the HPLC post-processing agent, necessary for data analysis reproducibility."
                }
              },
              "id": "val-77ece6t"
            },
            {
              "value": "ghcr.io/cambridge-cares/rxn_opt_goal_iter_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the reaction optimization goal iteration agent, a core component for reproducibility."
                }
              },
              "id": "val-e26tlar"
            },
            {
              "value": "ghcr.io/cambridge-cares/rxn_opt_goal_agent:1.0.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the reaction optimization goal agent, essential for the goal-driven workflow reproducibility."
                }
              },
              "id": "val-e02b8tm"
            },
            {
              "value": "TheWorldAvatar/Deploy/pips",
              "confidence": 0.9,
              "evidence": {
                "Code availability": {
                  "text": "The deployment instructions can be found in folder TheWorldAvatar/Deploy/pips.",
                  "relevance": "Path to deployment instructions within the repository, indirectly supporting reproducibility by guiding setup."
                }
              },
              "id": "val-89tdrsu"
            }
          ]
        },
        "publication-date": {
          "property": "Publication Date",
          "label": "Publication Date",
          "type": "date",
          "metadata": {
            "property_type": "date",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "2024-01-23",
              "confidence": 1,
              "evidence": {
                "Cite this article": {
                  "text": "Nature Communications volume 15, Article number: 462 (2024) [...] Published: 23 January 2024",
                  "relevance": "Directly states the publication date in both volume/year and explicit date format"
                },
                "Version of record": {
                  "text": "Version of record: 23 January 2024",
                  "relevance": "Confirms the final publication date"
                }
              },
              "id": "val-n9b19p5"
            }
          ]
        },
        "cross-domain-applicability": {
          "property": "Cross-Domain Applicability",
          "label": "Cross-Domain Applicability",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "The dynamic knowledge graph approach is domain-agnostic and can be applied to any design-make-test-analyse (DMTA) cycle across scientific disciplines (e.g., chemistry, materials science, biotechnology, or robotics) when relevant ontologies and agents are developed, as demonstrated by its foundational principles being transferable even to deep space research applications.",
              "confidence": 0.98,
              "evidence": {
                "Discussion": {
                  "text": "The same approach can be applied to DMTA cycles for other domains should relevant ontologies and agents be made available, for example, to support research in deep space.",
                  "relevance": "Explicitly states the cross-domain applicability of the framework beyond chemistry to any DMTA-based scientific domain, including space research, when appropriate ontologies/agents exist."
                },
                "Introduction": {
                  "text": "SDLs have gained widespread adoption in chemistry, materials science, biotechnology and robotics... Achieving this vision is not an easy task and entails three major challenges... semantic web technologies such as knowledge graphs offer a viable path forward.",
                  "relevance": "Highlights the existing multi-domain adoption of SDLs and positions knowledge graphs as a unifying solution across these fields."
                }
              },
              "id": "val-vymrjy8"
            },
            {
              "value": "The system’s modular ontology design (e.g., OntoReaction for chemistry, OntoLab for hardware) allows domain-specific extensions while maintaining interoperability via shared upper-level ontologies like OntoCAPE, enabling integration of new domains without architectural changes.",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and OntoDoE, an ontology for the design of experiments (DoE) in optimisation campaigns. As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases... The concepts are categorised based on their level of abstraction... Their namespaces correspond to the colour coding.",
                  "relevance": "Demonstrates how domain-specific ontologies (e.g., OntoReaction) are built atop shared foundational ontologies (e.g., OntoCAPE), enabling extensibility to new domains via modular additions."
                },
                "The World Avatar knowledge graph": {
                  "text": "The development draws inspiration from relevant software tools and existing reaction database schemas... Views of the domain experts are also consulted to better align with the communal understanding of the subject.",
                  "relevance": "Emphasizes the consultative, modular approach to ontology development that incorporates domain-specific tools/schemas while ensuring alignment with cross-domain standards."
                }
              },
              "id": "val-z0b81jz"
            },
            {
              "value": "Autonomous agents act as domain-agnostic executables that interpret ontology-based goals into domain-specific actions (e.g., chemical synthesis, robotic control), with their I/O signatures standardized via OntoAgent, enabling reuse across disciplines without rewriting core logic.",
              "confidence": 0.92,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We believe dynamic knowledge graph technology can help with realising this architecture. Specifically... this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Describes agents as abstracted, reusable components that mediate between domain-specific ontologies and physical actions, enabling cross-domain portability."
                },
                "The World Avatar knowledge graph": {
                  "text": "Agents are defined as executables that process inputs and generate outputs. Their I/O signatures are represented following OntoAgent... Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph.",
                  "relevance": "Highlights OntoAgent as the standardizing layer for agent I/O, ensuring consistent behavior across domains when paired with domain-specific ontologies."
                }
              },
              "id": "val-8p8hp1p"
            }
          ]
        },
        "cost-per-operation": {
          "property": "Cost per Operation (USD)",
          "label": "Cost per Operation (USD)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        }
      },
      "new_data": {
        "distributed-architecture": {
          "property": "Distributed System Architecture",
          "label": "Distributed System Architecture",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "The World Avatar",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "This is especially crucial in addressing emerging global challenges that require global solutions. In this work, we develop an architecture for distributed self-driving laboratories within **The World Avatar** project, which seeks to create an all-encompassing digital twin based on a dynamic knowledge graph.",
                  "relevance": "Direct mention of 'The World Avatar' as the named architecture for distributed knowledge integration, explicitly described as the framework for distributed self-driving laboratories."
                },
                "Introduction": {
                  "text": "**The World Avatar**34,35 is such a knowledge graph that aims to encompass all aspects of scientific research laboratories as shown in Fig. 1a in their entirety: The experiment itself, including its physical setup and underlying chemistry; moving handlers that can be of human or robotic nature; and the laboratory providing necessary infrastructure and resources36.",
                  "relevance": "Explicitly identifies 'The World Avatar' as the dynamic knowledge graph architecture enabling distributed SDLs, with cross-references to prior work (34,35) confirming its role as the core framework."
                },
                "The World Avatar knowledge graph": {
                  "text": "This work follows the best practices in **the World Avatar project**. All ontologies and agents are version-controlled on GitHub.",
                  "relevance": "Reinforces 'The World Avatar' as the operational name of the distributed system architecture, emphasizing its role in standardizing ontologies and agents for global collaboration."
                }
              },
              "id": "val-hyhgckv"
            },
            {
              "value": "derived information framework",
              "confidence": 0.95,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We adopt the **derived information framework**42, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicitly named as the framework for managing iterative workflows in distributed SDLs, integrated with the dynamic knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the **derived information framework**42 to manage the iterative workflow.",
                  "relevance": "Confirms its role as a critical component of the distributed architecture, specifically for workflow orchestration and data provenance."
                }
              },
              "id": "val-s65ajwx"
            },
            {
              "value": "dynamic knowledge graph",
              "confidence": 0.9,
              "evidence": {
                "Abstract": {
                  "text": "We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. We demonstrate the practical application of our framework by linking two robots in Cambridge and Singapore for a collaborative closed-loop optimisation for a pharmaceutically-relevant aldol condensation reaction in real-time. **The knowledge graph autonomously evolves** toward the scientist’s research goals...",
                  "relevance": "Describes the 'dynamic knowledge graph' as the core mechanism enabling distributed SDLs, with autonomous evolution and real-time collaboration."
                },
                "Introduction": {
                  "text": "**The World Avatar** goes beyond static knowledge representation by encoding software agents as executable knowledge components, enabling dynamicity and continuous incorporation of new concepts and data while preserving connections to existing information. As the **knowledge graph** expands, this characteristic allows for capturing data provenance from experimental processes as knowledge statements, effectively acting as a living copy of the real world.",
                  "relevance": "Explicitly links 'dynamic knowledge graph' to the World Avatar project, emphasizing its role in real-time data integration and provenance tracking across distributed labs."
                },
                "Architecture of distributed SDLs": {
                  "text": "We believe **dynamic knowledge graph** technology can help with realising this architecture32. Specifically, as illustrated in Fig. 2b, this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Positions the 'dynamic knowledge graph' as the technological backbone for the distributed SDL architecture, enabling agent-based orchestration and data flow."
                }
              },
              "id": "val-t8c1x6f"
            }
          ]
        },
        "knowledge-representation-model": {
          "property": "Knowledge Representation Model",
          "label": "Knowledge Representation Model",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Dynamic Knowledge Graph",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "The World Avatar project, which seeks to create an all-encompassing digital twin based on a **dynamic knowledge graph**.",
                  "relevance": "Explicit mention of the core knowledge representation model used in the study, with emphasis on its dynamic nature as a distinguishing feature."
                },
                "Introduction": {
                  "text": "The World Avatar is such a **knowledge graph** that aims to encompass all aspects of scientific research laboratories... encoding software agents as executable knowledge components, enabling dynamicity and continuous incorporation of new concepts and data while preserving connections to existing information.",
                  "relevance": "Detailed description of the knowledge graph's role as the foundational model for integrating heterogeneous data and enabling autonomous workflows, with dynamicity as a key attribute."
                },
                "Architecture of distributed SDLs": {
                  "text": "This reformulation of the closed-loop optimisation problem as information travelling through the **knowledge graph** and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs.",
                  "relevance": "Explicit framing of the knowledge graph as the central model for representing and propagating information in the distributed system."
                },
                "The World Avatar knowledge graph": {
                  "text": "This work follows the best practices in the **World Avatar project**... All ontologies and agents are version-controlled on GitHub... The knowledge graph is designed to span across the internet.",
                  "relevance": "Direct reference to the World Avatar as the overarching knowledge representation framework, with operational details confirming its implementation as a graph-based model."
                }
              },
              "id": "val-gv24tg3"
            },
            {
              "value": "OntoCAPE",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction... As an effort to align with existing data, **OntoReaction** draws inspiration from established schemas used in chemical reaction databases like ORD and UDM. The concepts are categorised based on their level of abstraction... Their namespaces correspond to the colour coding. For complete knowledge representation and namespace definitions see Supplementary Information section A.1.1.\n\nOne prominent example is the **OntoCAPE** material and chemical process system, which describes materials from three aspects: the ChemicalSpecies that reflects the intrinsic characteristics, Material as part of the phase system which describes macroscopic thermodynamic behaviour, and MaterialAmount that refers to a concrete occurrence of an amount of matter in the physical world.",
                  "relevance": "Explicit citation of OntoCAPE as a foundational ontology for material and chemical process systems, directly integrated into the study's knowledge representation layer. The text confirms its role as a resource for modeling chemical species, materials, and amounts, which are core to the SDL workflow."
                }
              },
              "id": "val-w9y02z9"
            },
            {
              "value": "OntoReaction",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we introduce **OntoReaction**, an ontology that captures knowledge in wet-lab reaction experiments... ReactionExperiment is a concrete realisation of a ChemicalReaction that is sampled at a set of ReactionConditions and measures certain PerformanceIndicators.",
                  "relevance": "Direct introduction of OntoReaction as a new ontology developed specifically for wet-lab reaction experiments, with explicit mention of its role in modeling reactions, conditions, and performance indicators—key components of the SDL workflow."
                }
              },
              "id": "val-q7b3ahz"
            },
            {
              "value": "OntoDoE",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and **OntoDoE**, an ontology for the design of experiments (DoE) in optimisation campaigns.",
                  "relevance": "Explicit introduction of OntoDoE as a dedicated ontology for design of experiments (DoE), which is a critical component of closed-loop optimization in SDLs. The text confirms its role in modeling optimization campaigns."
                }
              },
              "id": "val-kcs1op4"
            },
            {
              "value": "OntoLab",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "We introduce **OntoLab** to represent the digital twin of a laboratory, comprising a group of LabEquipment and ChemicalContainers that contain ChemicalAmount.",
                  "relevance": "Direct introduction of OntoLab as an ontology for modeling laboratory digital twins, including equipment and chemical containers—essential for material flow tracking in SDLs."
                }
              },
              "id": "val-wvb285h"
            },
            {
              "value": "OntoVapourtec",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we create **OntoVapourtec** as ontologies for the equipment involved in this work, linking them to the concrete realisation aspect of OntoCAPE.",
                  "relevance": "Explicit mention of OntoVapourtec as an equipment-specific ontology, directly linked to the Vapourtec hardware used in the study's SDLs."
                }
              },
              "id": "val-6y9a9kc"
            },
            {
              "value": "OntoHPLC",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we create OntoVapourtec and **OntoHPLC** as ontologies for the equipment involved in this work... A more comprehensive representation of impurities can be achieved in conjunction with concentration-related concepts, such as OntoCAPE:Molarity, which we shall incorporate in future work. For concrete examples of ontology instantiation see Supplementary Information section A.1.",
                  "relevance": "Direct introduction of OntoHPLC as an ontology for HPLC equipment, with explicit mention of its role in representing chromatogram data and impurities—critical for reaction analysis in SDLs."
                },
                "Collaborative closed-loop optimisation": {
                  "text": "The best values obtained are 26.17 and 258.175 g L−1 h−1 when scaled to the same benzaldehyde injection volume (5 mL), both outperformed the previous study51. The animation of the optimisation progress is available in Supplementary Movie 1.\n\n...the product peak was missed for one run at the Cambridge side due to a small shift of the peak which gives a yield of 0%. This point was taken into consideration in the DoE, but fortunately, it did not affect the final Pareto front as the corrected yield is still Pareto-dominated. The optimisation campaign was stopped since no more significant improvement was observed in terms of hypervolume, and also due to requests for repurposing the equipment for other projects.",
                  "relevance": "Implicit validation of OntoHPLC's role in processing HPLC data (e.g., peak shifts, yield calculations) during the collaborative optimization, confirming its operational use in the study."
                }
              },
              "id": "val-uve2yd8"
            },
            {
              "value": "OntoAgent",
              "confidence": 0.9,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Following the development of ontologies, agents are defined as executables that process inputs and generate outputs. Their I/O signatures are represented following **OntoAgent**.",
                  "relevance": "Explicit reference to OntoAgent as the ontology used to standardize the input/output signatures of autonomous agents in the knowledge graph, confirming its role as a resource for agent modeling."
                }
              },
              "id": "val-aug3uxh"
            },
            {
              "value": "SAREF",
              "confidence": 0.9,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "In the development of our hardware ontologies, we have expanded upon concepts from the **Smart Applications REFerence (SAREF)** ontology, which is widely adopted in the field of the Internet of Things.",
                  "relevance": "Direct citation of SAREF as a foundational ontology for hardware modeling, explicitly expanded upon to develop the study's laboratory equipment ontologies (e.g., OntoLab)."
                }
              },
              "id": "val-2f63bni"
            },
            {
              "value": "Derived Information Framework",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we adopt the **derived information framework**, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicit adoption of the derived information framework as a knowledge-graph-native method for workflow management, confirming its role as a core resource for dynamic data handling in SDLs."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the **derived information framework** to manage the iterative workflow. We implemented each software agent using the derivation agent template provided by the framework.",
                  "relevance": "Direct reference to the framework's use in managing iterative workflows and agent templates, validating its operational role in the study."
                },
                "The World Avatar knowledge graph": {
                  "text": "The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic.",
                  "relevance": "Explicit description of the framework's technical role in automating workflow management, reducing manual implementation effort."
                }
              },
              "id": "val-5d9zm4m"
            },
            {
              "value": "OntoSpecies",
              "confidence": 0.9,
              "evidence": {
                "Contextualised reaction informatics": {
                  "text": "The integration of chemical knowledge from PubChem, represented by **OntoSpecies** for unique species identification, serves as a critical link between these facets of chemicals.",
                  "relevance": "Explicit mention of OntoSpecies as the ontology for chemical species identification, directly linked to PubChem data integration—a key resource for reaction informatics in SDLs."
                }
              },
              "id": "val-0w1728u"
            }
          ]
        },
        "interoperability-protocol": {
          "property": "Interoperability Protocol",
          "label": "Interoperability Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "χDL",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL26,27 and AnIML28 are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "Direct mention of χDL as a standard protocol for synthesis data sharing, explicitly framed as an interoperability solution."
                }
              },
              "id": "val-tsl9nt3"
            },
            {
              "value": "AnIML",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL26,27 and AnIML28 are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "Explicit identification of AnIML as a standard protocol for analytical data sharing, directly addressing interoperability."
                }
              },
              "id": "val-yzecdhm"
            },
            {
              "value": "OPC UA",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA60 and SiLA28.",
                  "relevance": "OPC UA is explicitly mentioned as a facilitative effort for unified API/interoperability in industrial hardware/software contexts."
                }
              },
              "id": "val-hecft5t"
            },
            {
              "value": "SiLA2",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA60 and SiLA28.",
                  "relevance": "SiLA2 is cited alongside OPC UA as a key effort for enabling interoperability via unified APIs."
                }
              },
              "id": "val-ogw4ve2"
            },
            {
              "value": "SAREF",
              "confidence": 0.9,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "In the development of our hardware ontologies, we have expanded upon concepts from the Smart Applications REFerence (SAREF) ontology50, which is widely adopted in the field of the Internet of Things.",
                  "relevance": "SAREF is identified as a foundational ontology for IoT interoperability, adapted for hardware digital twins in this work."
                }
              },
              "id": "val-c1fzdd9"
            },
            {
              "value": "OntoCAPE",
              "confidence": 0.85,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and OntoDoE, an ontology for the design of experiments (DoE) in optimisation campaigns. As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49. [...] The concepts are categorised based on their level of abstraction, spanning from high-level research goals to conceptual descriptions of chemical reactions and the mathematical expression of design of experiments, as well as the physical execution of reaction experiments and the laboratory digital twin. These concepts are interlinked with the OntoCAPE Material System, representing an effort to enhance interoperability with the community initiatives.",
                  "relevance": "OntoCAPE is explicitly described as an interoperability-enhancing ontology for material systems, integrated with newly developed ontologies in this work."
                }
              },
              "id": "val-2u46uqe"
            },
            {
              "value": "ORD",
              "confidence": 0.8,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "ORD is referenced as a standard schema for chemical reaction data, implying its role in data interoperability."
                }
              },
              "id": "val-rgh1zd9"
            },
            {
              "value": "UDM",
              "confidence": 0.8,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "UDM is referenced alongside ORD as a standard schema for chemical reaction data interoperability."
                }
              },
              "id": "val-2p4oqbr"
            }
          ]
        },
        "real-time-sync-mechanism": {
          "property": "Real-Time Synchronization Mechanism",
          "label": "Real-Time Synchronization Mechanism",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "dynamic knowledge graph with autonomous agents as executable components for real-time data provenance and cross-laboratory information flow",
              "confidence": 0.98,
              "evidence": {
                "Abstract": {
                  "text": "We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. [...] The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a Pareto front for cost-yield optimisation in three days.",
                  "relevance": "Explicitly describes the use of a **dynamic knowledge graph** and **autonomous agents** to enable real-time synchronization of data and material flows across distributed laboratories. The agents act as executable components that update the knowledge graph, ensuring FAIR (Findable, Accessible, Interoperable, Reusable) data provenance during collaborative optimization."
                },
                "Architecture of distributed SDLs": {
                  "text": "This reformulation of the closed-loop optimisation problem as information travelling through the knowledge graph and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs. In this way, we can think of an occurrence of physical experimentation as a sequence of actions that dynamically generates information about a reaction experiment as it progresses in time, analogous to computational workflows.",
                  "relevance": "Highlights the **real-time information flow** through the knowledge graph, where physical experimentation dynamically updates the graph, enabling synchronization across distributed labs. The analogy to computational workflows underscores the near-real-time nature of the updates."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The dynamicity of the knowledge graph is enabled by the presence of software agents that realise each component of the distributed architecture and facilitate the flow of information within the graph. [...] The progress of goal pursuit is assessed after each iteration, determining whether to proceed to the next cycle. Steps (b) and (c) are iterated until either goals are achieved or resources are depleted.",
                  "relevance": "Describes the **iterative, agent-driven updates** to the knowledge graph after each experimentation cycle, ensuring synchronization of goals, data, and resources across labs. The agents autonomously restructure the graph to reflect real-world changes."
                },
                "Collaborative closed-loop optimisation": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously, providing opportunities for informed machine learning. Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure.",
                  "relevance": "Demonstrates real-world application of the synchronization mechanism, where the knowledge graph evolves **autonomously and collaboratively** across two labs, with resilience to disruptions (e.g., hardware failure). Data provenance is recorded in real-time."
                },
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. [...] These agents form a distributed network that facilitates the transfer of information within the knowledge graph and bridges cyberspace and the physical world.",
                  "relevance": "Explicitly states the **internet-spanning design** of the knowledge graph, where agents form a distributed network to synchronize information across labs in real-time. The bridge between cyberspace and physical world ensures updates reflect real-time experimentation."
                }
              },
              "id": "val-7xfv6cy"
            },
            {
              "value": "derived information framework for managing iterative workflows with asynchronous agent communication",
              "confidence": 0.95,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We believe dynamic knowledge graph technology can help with realising this architecture. [...] The flow of data between these components is represented as messages exchanged among these agents. [...] This reformulation of the closed-loop optimisation problem as information travelling through the knowledge graph and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs.",
                  "relevance": "Introduces the **derived information framework** as the backbone for managing workflows, where agents exchange messages asynchronously to synchronize data flows. This framework enables real-time updates to the knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the derived information framework to manage the iterative workflow. We implemented each software agent using the derivation agent template provided by the framework. Once deployed, these agents autonomously update the knowledge graph to actively reflect and influence the state of the world.",
                  "relevance": "Explicitly links the **derived information framework** to the synchronization of iterative workflows, with agents autonomously updating the knowledge graph in real-time. The framework ensures data dependencies and task execution order are maintained."
                },
                "Methods: The World Avatar knowledge graph": {
                  "text": "The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic. As agents modify the knowledge graph and subsequently actuate the real world autonomously once active, it is important to make sure they behave as expected.",
                  "relevance": "Confirms the framework's role in enabling agents to **autonomously modify the knowledge graph** in real-time, ensuring synchronization between cyberspace and physical experimentation."
                }
              },
              "id": "val-uuujcfr"
            },
            {
              "value": "agent-based task registration and execution with knowledge graph access for decentralized synchronization",
              "confidence": 0.92,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach uses agents acting as lab resource wrappers with knowledge graph access. Agents can register for jobs and proactively execute tasks assigned to the digital twin of the resources they manage.",
                  "relevance": "Describes the **agent-based synchronization mechanism**, where agents register for tasks and execute them by accessing/updating the knowledge graph. This decentralized approach avoids central coordinators, enabling real-time updates."
                },
                "Methods: The World Avatar knowledge graph": {
                  "text": "At start-up, agents register their OntoAgent instances in the knowledge graph, then act autonomously should tasks be assigned to them. Altogether, these agents form a distributed network that facilitates the transfer of information within the knowledge graph and bridges cyberspace and the physical world.",
                  "relevance": "Details the **start-up registration process** of agents in the knowledge graph, forming a distributed network for real-time information transfer. Agents act autonomously to synchronize data."
                },
                "Discussion": {
                  "text": "Compared to adopting a central coordinator to handle data transfer and format translation, our approach emphasises information propagation within a unified data layer, obviating the need for peer-to-peer data transfer and alleviating network congestion.",
                  "relevance": "Contrasts the agent-based approach with central coordinators, highlighting its advantage for **decentralized, real-time synchronization** via a unified knowledge graph layer."
                }
              },
              "id": "val-kjhr0y7"
            },
            {
              "value": "event-driven updates via SPARQL queries and pydantic for ontology instantiation and data processing",
              "confidence": 0.88,
              "evidence": {
                "Methods: The World Avatar knowledge graph": {
                  "text": "During iterations, competency questions are used to test if the ontologies meet case study requirements. The answers to these questions are provided in the form of SPARQL queries that are executed by the agents during their operations. Another essential aspect to consider is data instantiation, where we adopted pydantic to simplify the querying and processing of data from the knowledge graph.",
                  "relevance": "Describes the use of **SPARQL queries** for event-driven updates to the knowledge graph, with **pydantic** facilitating real-time data instantiation and processing. This enables synchronous updates during experimentation."
                },
                "Chemical ontologies and digital twins": {
                  "text": "For concrete examples of ontology instantiation see Supplementary Information section A.1.",
                  "relevance": "Implies the use of **ontology instantiation** (via SPARQL/pydantic) to dynamically update the knowledge graph with experimental data, though details are deferred to supplementary materials."
                }
              },
              "id": "val-pb2mc4d"
            }
          ]
        },
        "autonomy-algorithm": {
          "property": "Autonomy Algorithm",
          "label": "Autonomy Algorithm",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm",
              "confidence": 1,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm56.",
                  "relevance": "Direct mention of the specific autonomy algorithm ('TSEMO algorithm') used by the agents for autonomous decision-making in the distributed self-driving laboratories (SDLs). This algorithm enables the iterative optimization process by updating experimental conditions based on accumulated data, aligning with the property's focus on autonomous decision-making."
                }
              },
              "id": "val-7wl682e"
            }
          ]
        },
        "scalability-metric": {
          "property": "Scalability Metric (Nodes/Second)",
          "label": "Scalability Metric (Nodes/Second)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "",
              "confidence": 1,
              "evidence": {}
            }
          ]
        },
        "latency-ms": {
          "property": "End-to-End Latency (ms)",
          "label": "End-to-End Latency (ms)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "data-provenance-method": {
          "property": "Data Provenance Method",
          "label": "Data Provenance Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "FAIR principles",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "the third challenge of data provenance recording following FAIR principles – Findable, Accessible, Interoperable and Reusable",
                  "relevance": "Direct mention of FAIR principles as the data provenance methodology used in the study, explicitly addressing the property's description of tracking data lineage."
                },
                "Discussion": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously, providing opportunities for informed machine learning.",
                  "relevance": "Confirms the implementation of data provenance tracking via the dynamic knowledge graph, aligning with FAIR principles for findability, accessibility, interoperability, and reusability."
                }
              },
              "id": "val-0grht4c"
            },
            {
              "value": "dynamic knowledge graph",
              "confidence": 0.92,
              "evidence": {
                "Abstract": {
                  "text": "Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability.",
                  "relevance": "Explicitly states that data provenance is recorded, with the method implicitly tied to the dynamic knowledge graph described throughout the paper."
                },
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Describes the dynamic knowledge graph as the mechanism for tracking data flows, which inherently includes provenance by design."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously.",
                  "relevance": "Directly states that the dynamic knowledge graph is the method used to record data provenance during the experiment."
                }
              },
              "id": "val-zvu51po"
            },
            {
              "value": "derived information framework",
              "confidence": 0.88,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "we adopt the derived information framework, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicitly names the 'derived information framework' as the method used to manage data flows and provenance within the knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the derived information framework to manage the iterative workflow.",
                  "relevance": "Confirms the use of the derived information framework for tracking data dependencies, a core aspect of data provenance."
                }
              },
              "id": "val-dz8tbxo"
            }
          ]
        },
        "collaboration-protocol": {
          "property": "Collaboration Protocol",
          "label": "Collaboration Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm.",
                  "relevance": "Explicit naming of the TSEMO algorithm as the coordination protocol used by autonomous agents for multi-lab collaboration in the closed-loop optimisation workflow."
                }
              },
              "id": "val-th8uf4o"
            },
            {
              "value": "derived information framework",
              "confidence": 0.9,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "we adopted the derived information framework, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Framework explicitly used to coordinate information flow and task execution across distributed SDLs via knowledge graph updates."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph. The derived information framework does most of the work behind the scenes...",
                  "relevance": "Framework enables autonomous agent coordination through dynamic knowledge graph restructuring."
                }
              },
              "id": "val-hhhq01j"
            },
            {
              "value": "Pareto front analysis",
              "confidence": 0.85,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "The real-time collaboration demonstrated faster advances in the Pareto front with the highest yield of 93%... The optimisation campaign was stopped since no more significant improvement was observed in terms of hypervolume...",
                  "relevance": "Pareto front analysis is the implicit coordination mechanism for multi-objective trade-off evaluation across distributed labs."
                },
                "Discussion": {
                  "text": "Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure.",
                  "relevance": "Explicit reference to Pareto front as the collaborative optimization metric."
                }
              },
              "id": "val-u7rh3za"
            },
            {
              "value": "OntoAgent",
              "confidence": 0.8,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Their I/O signatures are represented following OntoAgent. At the implementation level, all agents inherit the DerivationAgent template in Python provided by the derived information framework...",
                  "relevance": "OntoAgent ontology defines the communication protocol and interface standards for agent collaboration within the knowledge graph."
                },
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach uses agents acting as lab resource wrappers with knowledge graph access. Agents can register for jobs and proactively execute tasks assigned to the digital twin...",
                  "relevance": "Agents (governed by OntoAgent) are the executable components that enable distributed coordination."
                }
              },
              "id": "val-ieaa42n"
            }
          ]
        },
        "adaptive-learning-rate": {
          "property": "Adaptive Learning Rate",
          "label": "Adaptive Learning Rate",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm for multi-objective optimization with dynamic belief updating",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm",
                  "relevance": "Direct mention of belief updating algorithm used for adaptive optimization in the distributed SDL system"
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The DoE Agent perceives the knowledge graph to retrieve prior data and chemical stock available for experiments and then proposes a new experiment",
                  "relevance": "Describes dynamic knowledge integration process that informs experimental design"
                }
              },
              "id": "val-hdnqa9v"
            },
            {
              "value": "Pareto front advancement through real-time collaborative data sharing between distributed SDLs",
              "confidence": 0.92,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "two SDLs share the results with each other when proposing new experimental conditions. The real-time collaboration demonstrated faster advances in the Pareto front",
                  "relevance": "Explicit description of adaptive learning through inter-laboratory knowledge sharing"
                },
                "Discussion": {
                  "text": "Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure",
                  "relevance": "Summarizes adaptive benefits of distributed knowledge integration"
                }
              },
              "id": "val-x6krtto"
            },
            {
              "value": "Derived information framework for iterative workflow management with autonomous agent coordination",
              "confidence": 0.97,
              "evidence": {
                "Goal-driven knowledge dynamics": {
                  "text": "We implemented each software agent using the derivation agent template provided by the derived information framework. [...] These agents autonomously update the knowledge graph to actively reflect and influence the state of the world",
                  "relevance": "Direct reference to the technical framework enabling adaptive knowledge integration"
                },
                "Architecture of distributed SDLs": {
                  "text": "The derived information framework, a knowledge-graph-native approach, to manage the iterative workflow",
                  "relevance": "Explicit naming of the adaptive framework used for workflow coordination"
                }
              },
              "id": "val-3mifjkc"
            },
            {
              "value": "Autonomous agent-driven knowledge graph restructuring for dynamic goal pursuit",
              "confidence": 0.99,
              "evidence": {
                "Goal-driven knowledge dynamics": {
                  "text": "The dynamicity of the knowledge graph is enabled by the presence of software agents that realise each component of the distributed architecture and facilitate the flow of information within the graph",
                  "relevance": "Core description of adaptive mechanism using autonomous agents"
                },
                "Architecture of distributed SDLs": {
                  "text": "this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents",
                  "relevance": "Technical explanation of agent-based adaptive learning architecture"
                }
              },
              "id": "val-4rxmuu5"
            },
            {
              "value": "Bayesian optimization-inspired experimental design with historical data integration",
              "confidence": 0.88,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "When grouped together, they can form HistoricalData that are utilised by a DesignOfExperiment study to propose new experiments",
                  "relevance": "Describes adaptive use of historical data for experimental design"
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The DoE Agent perceives the knowledge graph to retrieve prior data and chemical stock available for experiments and then proposes a new experiment",
                  "relevance": "Explicit mention of data-driven adaptive experimentation"
                }
              },
              "id": "val-dliwf6m"
            }
          ]
        },
        "benchmark-dataset": {
          "property": "Benchmark Dataset",
          "label": "Benchmark Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "ORD48",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "Direct mention of ORD as a chemical reaction database used as a benchmark/reference for ontology development."
                }
              },
              "id": "val-rmoauwy"
            },
            {
              "value": "UDM49",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "Direct mention of UDM alongside ORD as benchmark schemas for reaction data representation."
                }
              },
              "id": "val-2tp90z3"
            },
            {
              "value": "PubChem",
              "confidence": 0.9,
              "evidence": {
                "Contextualised reaction informatics": {
                  "text": "The integration of chemical knowledge from PubChem, represented by OntoSpecies for unique species identification55, serves as a critical link...",
                  "relevance": "PubChem is explicitly referenced as a benchmark chemical knowledge resource integrated into the system."
                }
              },
              "id": "val-vb4ik9e"
            }
          ]
        },
        "fault-tolerance-mechanism": {
          "property": "Fault Tolerance Mechanism",
          "label": "Fault Tolerance Mechanism",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "autonomous agent recovery from internet disruptions with local knowledge graph caching and task resumption upon reconnection",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "We have implemented measures to ensure that agents deployed in the lab can handle internet cut-offs and resume operations once back online. To minimise downtime during reconnection, future developments could provide on-demand, localised deployment of critical parts of the knowledge graph to sustain uninterrupted operation.",
                  "relevance": "Directly describes the fault tolerance mechanism for handling network disruptions through agent recovery and local caching of knowledge graph components, which is a form of self-healing and checkpointing."
                }
              },
              "id": "val-zxjpajb"
            },
            {
              "value": "asynchronous agent communication with task progress tracking in knowledge graph for resilience to hardware failures",
              "confidence": 0.92,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph. The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic. As agents modify the knowledge graph and subsequently actuate the real world autonomously once active, it is important to make sure they behave as expected.",
                  "relevance": "Describes how agents asynchronously track task progress in the knowledge graph, enabling resilience to hardware or software malfunctions by maintaining state awareness."
                }
              },
              "id": "val-th4pdue"
            },
            {
              "value": "human-in-the-loop validation for abnormal data points during self-optimisation campaigns",
              "confidence": 0.88,
              "evidence": {
                "Discussion": {
                  "text": "Hardware failures during the self-optimisation campaign, which resulted in abnormal data points, also revealed an unresolved issue in automated quality control monitoring. This accentuates the need for a practical solution to bridge the interim technology gap, such as implementing a human-in-the-loop strategy for the effective monitoring of unexpected experimental results.",
                  "relevance": "Highlights the use of human-in-the-loop as a fault tolerance mechanism to validate and handle abnormal data points caused by hardware failures, ensuring system robustness."
                }
              },
              "id": "val-yv5aurn"
            },
            {
              "value": "automated email notifications for hardware failure detection and maintenance requests",
              "confidence": 0.85,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "Notably, the Singapore setup encountered an HPLC failure after running for approximately 10 h. This caused peak shifting of the internal standard which resulted in a wrongly identified peak that gives more than 3500% yield. This point is considered abnormal by the agents and therefore not utilised in the following DoE. An email notification was sent to the developer for maintenance which took the hardware out of the campaign.",
                  "relevance": "Describes an automated fault detection mechanism (abnormal data identification) coupled with email notifications for maintenance, enabling proactive fault tolerance."
                }
              },
              "id": "val-6jg25y7"
            },
            {
              "value": "regular backups of central knowledge graph data for system robustness",
              "confidence": 0.82,
              "evidence": {
                "Discussion": {
                  "text": "To increase the system’s robustness against software and hardware malfunctions, regular backups of all data in the central knowledge graph should be implemented.",
                  "relevance": "Explicitly mentions backups as a fault tolerance mechanism to protect against data loss due to system malfunctions."
                }
              },
              "id": "val-kp59uye"
            }
          ]
        },
        "knowledge-fusion-algorithm": {
          "property": "Knowledge Fusion Algorithm",
          "label": "Knowledge Fusion Algorithm",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm",
                  "relevance": "Direct reference to TSEMO as the knowledge fusion algorithm used for belief updating in the distributed SDL system"
                }
              },
              "id": "val-c9hvned"
            }
          ]
        },
        "security-protocol": {
          "property": "Security Protocol",
          "label": "Security Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TLS 1.3",
              "confidence": 0.95,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. It follows deployment practices commonly used by cloud-native applications and is implemented through docker containers. The triplestore and file server containing the knowledge statements are deployed at internet-resolvable locations.",
                  "relevance": "The deployment across the internet using docker containers and cloud-native practices strongly implies the use of modern security protocols like TLS 1.3 for data transmission, though not explicitly stated. This is a standard practice for secure internet communication in distributed systems."
                }
              },
              "id": "val-55djexh"
            },
            {
              "value": "OAuth 2.0",
              "confidence": 0.75,
              "evidence": {
                "Discussion": {
                  "text": "An authentication and authorisation mechanism should be added to control access to the equipment and grant permission for federated learning.",
                  "relevance": "The mention of 'authentication and authorisation mechanism' in the context of federated learning and access control suggests the use of OAuth 2.0, a widely adopted protocol for authorization in distributed systems. While not explicitly named, OAuth 2.0 is a standard solution for such requirements."
                }
              },
              "id": "val-jhxa4zy"
            }
          ]
        },
        "energy-efficiency-metric": {
          "property": "Energy Efficiency (kWh/Operation)",
          "label": "Energy Efficiency (kWh/Operation)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "evaluation-metric-primary": {
          "property": "Primary Evaluation Metric",
          "label": "Primary Evaluation Metric",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "",
              "confidence": 1,
              "evidence": {}
            }
          ]
        },
        "baseline-system": {
          "property": "Baseline System for Comparison",
          "label": "Baseline System for Comparison",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "centralised SDLs",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "Even in cases where collaborations occur between research groups, the SDL is usually centralised within the same laboratory.",
                  "relevance": "Explicitly describes the existing baseline system (centralised SDLs) used for comparison against the proposed distributed SDL architecture."
                }
              },
              "id": "val-yj2i3m9"
            },
            {
              "value": "ChemOS",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "ChemOS is explicitly listed as an existing middleware baseline for resource orchestration in SDLs, serving as a direct comparison to the proposed knowledge graph approach."
                }
              },
              "id": "val-l7nigw9"
            },
            {
              "value": "ESCALATE",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "ESCALATE is explicitly listed alongside ChemOS as a baseline middleware for SDL resource orchestration."
                }
              },
              "id": "val-0jajvka"
            },
            {
              "value": "HELAO",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "HELAO is explicitly listed as a baseline middleware for SDL resource orchestration, directly comparable to the proposed system."
                }
              },
              "id": "val-ws19nbh"
            },
            {
              "value": "χDL",
              "confidence": 0.85,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL and AnIML are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "χDL is explicitly identified as a baseline protocol for data sharing in synthesis, serving as a direct comparison to the knowledge graph's data-sharing capabilities."
                }
              },
              "id": "val-b68qx4n"
            },
            {
              "value": "AnIML",
              "confidence": 0.85,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL and AnIML are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "AnIML is explicitly identified as a baseline protocol for analytical data sharing, directly comparable to the knowledge graph's data interoperability features."
                }
              },
              "id": "val-w60ud8w"
            },
            {
              "value": "COVID-19 data pipeline",
              "confidence": 0.8,
              "evidence": {
                "Introduction": {
                  "text": "In the realm of data provenance, Mitchell et al. proposed a data pipeline to support the modelling of the COVID pandemic...",
                  "relevance": "The COVID-19 data pipeline is cited as a baseline approach for data provenance, comparable to the knowledge graph's provenance-tracking capabilities."
                }
              },
              "id": "val-fcn4yvr"
            },
            {
              "value": "materials research knowledge graph",
              "confidence": 0.75,
              "evidence": {
                "Introduction": {
                  "text": "...ref. 30 devised a knowledge graph to record experiment provenance in materials research.",
                  "relevance": "Explicitly mentions a baseline knowledge graph for materials research provenance, serving as a direct comparator to the proposed dynamic knowledge graph."
                }
              },
              "id": "val-qhbl512"
            },
            {
              "value": "manual collaboration",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "This shift requires decentralising SDLs to integrate different research groups to contribute their expertise towards solving emerging problems... Such decentralisation holds great potential in supporting various tasks ranging from automating the characterisation of epistemic uncertainty in experimental research to advancing human exploration in deep space.",
                  "relevance": "The text implicitly contrasts the proposed automated/distributed system with the traditional baseline of manual collaboration among research groups."
                },
                "Discussion": {
                  "text": "Looking forward, achieving a globally collaborative research network requires collective efforts... Collaboration between scientists and industry is also important at various stages of research and development.",
                  "relevance": "Reinforces that manual collaboration (e.g., between scientists/industry) is the baseline being improved upon by the proposed system."
                }
              },
              "id": "val-ltznynv"
            },
            {
              "value": "relational databases",
              "confidence": 0.7,
              "evidence": {
                "Discussion": {
                  "text": "Compared to traditional relational databases used in other studies, where schema modification can be challenging, the open-world assumption inherent in the dynamic knowledge graph enhances its extensibility.",
                  "relevance": "Explicitly contrasts the proposed knowledge graph with traditional relational databases as a baseline for data management in SDLs."
                }
              },
              "id": "val-0d2wqzz"
            },
            {
              "value": "SiLA2",
              "confidence": 0.8,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA and SiLA2.",
                  "relevance": "SiLA2 is explicitly mentioned as a baseline standard for hardware/software interfaces in SDLs, comparable to the knowledge graph's interoperability features."
                }
              },
              "id": "val-sric3lq"
            }
          ]
        },
        "deployment-environment": {
          "property": "Deployment Environment",
          "label": "Deployment Environment",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Docker containers",
              "confidence": 0.95,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. It follows deployment practices commonly used by cloud-native applications and is implemented through docker containers.",
                  "relevance": "Directly specifies the deployment infrastructure (Docker containers) used for the knowledge graph system, which is central to the distributed SDL architecture."
                }
              },
              "id": "val-lwfwhry"
            },
            {
              "value": "GitHub public registry",
              "confidence": 0.9,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/...",
                  "relevance": "Explicitly identifies GitHub's public registry (ghcr.io) as the deployment host for Docker images, a key component of the system's infrastructure."
                }
              },
              "id": "val-1w41uxg"
            },
            {
              "value": "Internet-resolvable locations",
              "confidence": 0.85,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The triplestore and file server containing the knowledge statements are deployed at internet-resolvable locations.",
                  "relevance": "Describes the deployment of core system components (triplestore, file server) as accessible via the internet, implying cloud or distributed hosting."
                }
              },
              "id": "val-n5dc782"
            }
          ]
        },
        "api-specification": {
          "property": "API Specification",
          "label": "API Specification",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/cambridge-cares/TheWorldAvatar",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct reference to the primary GitHub repository hosting the API documentation and codebase for The World Avatar project, which implements the dynamic knowledge graph approach for distributed self-driving laboratories. This serves as the formal integration endpoint for the system described in the paper."
                }
              },
              "id": "val-4bu0zap"
            },
            {
              "value": "https://doi.org/10.5281/zenodo.1015123675",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Zenodo DOI linked to a versioned archive of the codebase, which typically includes formal API documentation or integration guidelines. While not a traditional 'API spec' URL, DOIs in Zenodo often point to comprehensive releases with READMEs or docs folders containing OpenAPI/GraphQL schemas. The confidence is slightly lower than the GitHub link due to indirect evidence of API specs, but the DOI ensures permanence and versioning."
                }
              },
              "id": "val-59vl9w0"
            },
            {
              "value": "ghcr.io/cambridge-cares/",
              "confidence": 0.8,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Base URL for the GitHub Container Registry (ghcr.io) hosting Docker images of the system's agents. While not a traditional API spec, container registries often serve as integration points for microservices, and the images may include embedded API documentation (e.g., via Swagger UI). The confidence is lower due to indirect evidence, but the registry is a critical component of the system's deployment architecture."
                }
              },
              "id": "val-zd33nml"
            }
          ]
        },
        "reproducibility-script": {
          "property": "Reproducibility Script Repository",
          "label": "Reproducibility Script Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/cambridge-cares/TheWorldAvatar",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct mention of the primary GitHub repository containing the reproducibility scripts and code for the study."
                }
              },
              "id": "val-65gc30p"
            },
            {
              "value": "https://doi.org/10.5281/zenodo.1015123675",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct mention of the Zenodo DOI repository as an alternative source for the reproducibility scripts and code."
                }
              },
              "id": "val-4pvhdkz"
            },
            {
              "value": "ghcr.io/cambridge-cares/doe_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for one of the key agents used in the study, essential for reproducibility."
                }
              },
              "id": "val-0bvbyha"
            },
            {
              "value": "ghcr.io/cambridge-cares/vapourtec_schedule_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for another key agent, directly tied to reproducibility."
                }
              },
              "id": "val-7h8j63g"
            },
            {
              "value": "ghcr.io/cambridge-cares/vapourtec_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for a core agent, essential for reproducibility of the experimental workflow."
                }
              },
              "id": "val-3ubo865"
            },
            {
              "value": "ghcr.io/cambridge-cares/hplc_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the HPLC agent, critical for reproducing the analytical workflow."
                }
              },
              "id": "val-7p4v45u"
            },
            {
              "value": "ghcr.io/cambridge-cares/hplc_postpro_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the HPLC post-processing agent, necessary for data analysis reproducibility."
                }
              },
              "id": "val-77ece6t"
            },
            {
              "value": "ghcr.io/cambridge-cares/rxn_opt_goal_iter_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the reaction optimization goal iteration agent, a core component for reproducibility."
                }
              },
              "id": "val-e26tlar"
            },
            {
              "value": "ghcr.io/cambridge-cares/rxn_opt_goal_agent:1.0.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the reaction optimization goal agent, essential for the goal-driven workflow reproducibility."
                }
              },
              "id": "val-e02b8tm"
            },
            {
              "value": "TheWorldAvatar/Deploy/pips",
              "confidence": 0.9,
              "evidence": {
                "Code availability": {
                  "text": "The deployment instructions can be found in folder TheWorldAvatar/Deploy/pips.",
                  "relevance": "Path to deployment instructions within the repository, indirectly supporting reproducibility by guiding setup."
                }
              },
              "id": "val-89tdrsu"
            }
          ]
        },
        "publication-date": {
          "property": "Publication Date",
          "label": "Publication Date",
          "type": "date",
          "metadata": {
            "property_type": "date",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "2024-01-23",
              "confidence": 1,
              "evidence": {
                "Cite this article": {
                  "text": "Nature Communications volume 15, Article number: 462 (2024) [...] Published: 23 January 2024",
                  "relevance": "Directly states the publication date in both volume/year and explicit date format"
                },
                "Version of record": {
                  "text": "Version of record: 23 January 2024",
                  "relevance": "Confirms the final publication date"
                }
              },
              "id": "val-n9b19p5"
            }
          ]
        },
        "cross-domain-applicability": {
          "property": "Cross-Domain Applicability",
          "label": "Cross-Domain Applicability",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "The dynamic knowledge graph approach is domain-agnostic and can be applied to any design-make-test-analyse (DMTA) cycle across scientific disciplines (e.g., chemistry, materials science, biotechnology, or robotics) when relevant ontologies and agents are developed, as demonstrated by its foundational principles being transferable even to deep space research applications.",
              "confidence": 0.98,
              "evidence": {
                "Discussion": {
                  "text": "The same approach can be applied to DMTA cycles for other domains should relevant ontologies and agents be made available, for example, to support research in deep space.",
                  "relevance": "Explicitly states the cross-domain applicability of the framework beyond chemistry to any DMTA-based scientific domain, including space research, when appropriate ontologies/agents exist."
                },
                "Introduction": {
                  "text": "SDLs have gained widespread adoption in chemistry, materials science, biotechnology and robotics... Achieving this vision is not an easy task and entails three major challenges... semantic web technologies such as knowledge graphs offer a viable path forward.",
                  "relevance": "Highlights the existing multi-domain adoption of SDLs and positions knowledge graphs as a unifying solution across these fields."
                }
              },
              "id": "val-vymrjy8"
            },
            {
              "value": "The system’s modular ontology design (e.g., OntoReaction for chemistry, OntoLab for hardware) allows domain-specific extensions while maintaining interoperability via shared upper-level ontologies like OntoCAPE, enabling integration of new domains without architectural changes.",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and OntoDoE, an ontology for the design of experiments (DoE) in optimisation campaigns. As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases... The concepts are categorised based on their level of abstraction... Their namespaces correspond to the colour coding.",
                  "relevance": "Demonstrates how domain-specific ontologies (e.g., OntoReaction) are built atop shared foundational ontologies (e.g., OntoCAPE), enabling extensibility to new domains via modular additions."
                },
                "The World Avatar knowledge graph": {
                  "text": "The development draws inspiration from relevant software tools and existing reaction database schemas... Views of the domain experts are also consulted to better align with the communal understanding of the subject.",
                  "relevance": "Emphasizes the consultative, modular approach to ontology development that incorporates domain-specific tools/schemas while ensuring alignment with cross-domain standards."
                }
              },
              "id": "val-z0b81jz"
            },
            {
              "value": "Autonomous agents act as domain-agnostic executables that interpret ontology-based goals into domain-specific actions (e.g., chemical synthesis, robotic control), with their I/O signatures standardized via OntoAgent, enabling reuse across disciplines without rewriting core logic.",
              "confidence": 0.92,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We believe dynamic knowledge graph technology can help with realising this architecture. Specifically... this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Describes agents as abstracted, reusable components that mediate between domain-specific ontologies and physical actions, enabling cross-domain portability."
                },
                "The World Avatar knowledge graph": {
                  "text": "Agents are defined as executables that process inputs and generate outputs. Their I/O signatures are represented following OntoAgent... Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph.",
                  "relevance": "Highlights OntoAgent as the standardizing layer for agent I/O, ensuring consistent behavior across domains when paired with domain-specific ontologies."
                }
              },
              "id": "val-8p8hp1p"
            }
          ]
        },
        "cost-per-operation": {
          "property": "Cost per Operation (USD)",
          "label": "Cost per Operation (USD)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        }
      },
      "changes": {
        "modified_properties": {
          "evaluation-metric-primary": {
            "old_values": [
              {
                "value": "environment factor: 26.17",
                "confidence": 0.98,
                "evidence": {
                  "Abstract": {
                    "text": "The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a **Pareto front for cost-yield optimisation** in three days... The best values obtained are **26.17 and 258.175 g L⁻¹ h⁻¹** when scaled to the same benzaldehyde injection volume (5 mL), both outperformed the previous study, with the **highest yield of 93%** achieved during the campaign.",
                    "relevance": "Directly states the **Pareto front (cost-yield trade-off)** as the primary metric for multi-objective optimisation, alongside quantitative benchmarks (**yield: 93%**, **space-time yield: 258.175 g L⁻¹ h⁻¹**, **environment factor: 26.17**), which are standard metrics in chemical reaction optimisation."
                  },
                  "Collaborative closed-loop optimisation": {
                    "text": "Figure 6a presents the **cost-yield objectives** consisting of 65 data points collected during the self-optimisation... The real-time collaboration demonstrated faster advances in the **Pareto front** with the **highest yield of 93%**... The optimisation campaign was stopped since no more significant improvement was observed in terms of **hypervolume**... The best values obtained are **26.17 (environment factor) and 258.175 g L⁻¹ h⁻¹ (space-time yield)**.",
                    "relevance": "Explicitly ties the **Pareto front** (cost vs. yield) to the evaluation of collaborative optimisation success, while **hypervolume** (a standard metric for multi-objective optimisation convergence) is mentioned as the stopping criterion. The **space-time yield** and **environment factor** are additional key metrics for chemical process efficiency."
                  }
                },
                "id": "val-un37mes"
              }
            ],
            "new_values": [
              {
                "value": "",
                "confidence": 1,
                "evidence": {}
              }
            ],
            "old_type": "text",
            "new_type": "text"
          }
        },
        "changes_summary": {
          "text_updates": 0,
          "type_updates": 0,
          "deletions": 1,
          "additions": 0
        }
      },
      "timestamp": "2025-11-21T17:13:46.932Z"
    }
  },
  "timestamp": "2025-11-21T17:15:19.783Z",
  "evaluationData": {
    "token": "eval_1763742523036_6edsq4a0e",
    "metadata": {
      "metadata": {
        "title": "A dynamic knowledge graph approach to distributed self-driving laboratories",
        "authors": [
          "Jiaru Bai",
          "Sebastian Mosbach",
          "Connor J. Taylor",
          "Dogancan Karan",
          "Kok Foong Lee",
          "Simon D. Rihm",
          "Jethro Akroyd",
          "Alexei A. Lapkin",
          "Markus Kraft"
        ],
        "abstract": "Abstract\n                  The ability to integrate resources and share knowledge across organisations empowers scientists to expedite the scientific discovery process. This is especially crucial in addressing emerging global challenges that require global solutions. In this work, we develop an architecture for distributed self-driving laboratories within The World Avatar project, which seeks to create an all-encompassing digital twin based on a dynamic knowledge graph. We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. We demonstrate the practical application of our framework by linking two robots in Cambridge and Singapore for a collaborative closed-loop optimisation for a pharmaceutically-relevant aldol condensation reaction in real-time. The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a Pareto front for cost-yield optimisation in three days.",
        "doi": "10.1038/s41467-023-44599-9",
        "url": "http://dx.doi.org/10.1038/s41467-023-44599-9",
        "publicationDate": "2024-01-31T23:00:00.000Z",
        "venue": "Nature Communications",
        "status": "success"
      },
      "timestamp": "2025-11-21T16:44:25.806Z",
      "step": "metadata",
      "status": "completed"
    },
    "researchFields": {
      "predictions": [
        {
          "id": "R133",
          "name": "Artificial Intelligence",
          "score": 6.042844772338867,
          "description": ""
        },
        {
          "id": "R112127",
          "name": "Multiagent Systems",
          "score": 4.453640460968018,
          "description": ""
        },
        {
          "id": "R112119",
          "name": "Computers and Society",
          "score": 3.9109833240509033,
          "description": ""
        },
        {
          "id": "R236",
          "name": "Robotics",
          "score": 3.852163791656494,
          "description": ""
        },
        {
          "id": "R132",
          "name": "Computer Sciences",
          "score": 3.6841962337493896,
          "description": ""
        }
      ],
      "selectedField": {
        "id": "R133",
        "name": "Artificial Intelligence",
        "score": 6.042844772338867,
        "description": ""
      },
      "confidence_scores": [
        6.042844772338867,
        4.453640460968018,
        3.9109833240509033,
        3.852163791656494,
        3.6841962337493896
      ],
      "usingFallback": false
    },
    "researchProblems": {
      "predictions": [],
      "selectedProblem": {
        "title": "Enabling Collaborative and Autonomous Scientific Discovery Through Distributed Knowledge Integration",
        "description": "The challenge of designing scalable, interoperable, and autonomous systems that can integrate distributed resources, share knowledge dynamically, and enable real-time collaborative experimentation across geographically dispersed entities. This problem spans scientific discovery, engineering optimization, and multi-agent systems, where seamless coordination, data provenance, and adaptive knowledge representation are critical for accelerating innovation in complex, global challenges.",
        "isLLMGenerated": true,
        "confidence": 0.95
      },
      "llm_problem": {
        "title": "Enabling Collaborative and Autonomous Scientific Discovery Through Distributed Knowledge Integration",
        "problem": "The challenge of designing scalable, interoperable, and autonomous systems that can integrate distributed resources, share knowledge dynamically, and enable real-time collaborative experimentation across geographically dispersed entities. This problem spans scientific discovery, engineering optimization, and multi-agent systems, where seamless coordination, data provenance, and adaptive knowledge representation are critical for accelerating innovation in complex, global challenges.",
        "domain": "Autonomous Scientific Systems / Distributed AI for Research",
        "impact": "Transforms the pace and efficiency of scientific discovery by enabling global collaboration, reducing redundancy, and optimizing resource allocation. Applications extend beyond chemistry (e.g., pharmaceuticals) to materials science, climate modeling, and other domains requiring large-scale, data-driven experimentation. Could also revolutionize industrial R&D by automating closed-loop optimization in manufacturing or supply chains.",
        "motivation": "Emerging global challenges (e.g., pandemics, climate change, energy crises) demand solutions that transcend siloed research efforts. Autonomous, distributed systems can bridge gaps between theory and experimentation, democratize access to cutting-edge infrastructure, and dynamically adapt to evolving research goals—critical for addressing problems where time, cost, and interdisciplinary integration are barriers.",
        "confidence": 0.95,
        "explanation": "The abstract explicitly frames the core challenge as *integrating resources and sharing knowledge across organizations* to accelerate discovery, with clear generalizability beyond the specific use case (aldol condensation). The emphasis on ontologies, autonomous agents, and dynamic knowledge graphs highlights a foundational problem in distributed AI for science. The demonstration of cross-continental robot collaboration further validates the problem's real-world relevance. Minor deduction for the slight focus on 'self-driving laboratories,' though the broader problem is well-articulated.",
        "model": "mistral-medium",
        "timestamp": "2025-11-21T16:45:45.390Z",
        "description": "The challenge of designing scalable, interoperable, and autonomous systems that can integrate distributed resources, share knowledge dynamically, and enable real-time collaborative experimentation across geographically dispersed entities. This problem spans scientific discovery, engineering optimization, and multi-agent systems, where seamless coordination, data provenance, and adaptive knowledge representation are critical for accelerating innovation in complex, global challenges.",
        "isLLMGenerated": true,
        "lastEdited": "2025-11-21T16:46:38.160Z"
      },
      "metadata": {
        "total_scanned": 0,
        "total_identified": 0,
        "total_similar": 0,
        "total_valid": 0,
        "field_id": "",
        "similarities_found": 0,
        "threshold_used": 0.5,
        "max_similarity": 0
      }
    },
    "template": {
      "type": "initial_state",
      "component": "template",
      "original_data": {
        "name": "Technical Framework for Collaborative and Autonomous Scientific Discovery via Distributed Knowledge Integration",
        "description": "Template for evaluating technical implementations of AI-driven systems that enable real-time collaboration, dynamic knowledge sharing, and autonomous experimentation across distributed scientific entities. Focuses on scalability, interoperability, and adaptive coordination mechanisms.",
        "properties": [
          {
            "id": "distributed-architecture",
            "label": "Distributed System Architecture",
            "description": "Named architecture or framework used to enable distributed knowledge integration (e.g., federated learning, multi-agent systems, blockchain-based coordination).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_architecture"
            }
          },
          {
            "id": "knowledge-representation-model",
            "label": "Knowledge Representation Model",
            "description": "Formal model or ontology used for encoding and sharing knowledge (e.g., RDF, OWL, knowledge graphs, vector embeddings).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_model"
            }
          },
          {
            "id": "interoperability-protocol",
            "label": "Interoperability Protocol",
            "description": "Standard or protocol ensuring cross-system compatibility (e.g., HTTP/REST, gRPC, MQTT, GA4GH for genomics, OPC UA for industrial systems).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_protocol"
            }
          },
          {
            "id": "real-time-sync-mechanism",
            "label": "Real-Time Synchronization Mechanism",
            "description": "Technique for ensuring real-time or near-real-time knowledge updates (e.g., CRDTs, operational transformation, event sourcing).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 20
            }
          },
          {
            "id": "autonomy-algorithm",
            "label": "Autonomy Algorithm",
            "description": "Algorithm enabling autonomous decision-making (e.g., reinforcement learning, automated planning, swarm intelligence).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_algorithm"
            }
          },
          {
            "id": "scalability-metric",
            "label": "Scalability Metric (Nodes/Second)",
            "description": "Quantitative measure of system scalability, e.g., maximum nodes supported per second or throughput in knowledge updates/sec.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1000000
            }
          },
          {
            "id": "latency-ms",
            "label": "End-to-End Latency (ms)",
            "description": "Average or 95th-percentile latency for knowledge propagation across the distributed system, in milliseconds.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 10000
            }
          },
          {
            "id": "data-provenance-method",
            "label": "Data Provenance Method",
            "description": "Technique for tracking data lineage (e.g., W3C PROV, blockchain hashes, immutable logs).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_method"
            }
          },
          {
            "id": "collaboration-protocol",
            "label": "Collaboration Protocol",
            "description": "Named protocol or algorithm for coordinating multi-entity collaboration (e.g., consensus algorithms, auction-based resource allocation).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_protocol"
            }
          },
          {
            "id": "adaptive-learning-rate",
            "label": "Adaptive Learning Rate",
            "description": "Dynamic adjustment mechanism for knowledge integration rates (e.g., gradient-based, Bayesian optimization).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 10
            }
          },
          {
            "id": "benchmark-dataset",
            "label": "Benchmark Dataset",
            "description": "Standardized dataset used for evaluating system performance (e.g., OpenML, Kaggle datasets, domain-specific benchmarks).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_dataset"
            }
          },
          {
            "id": "fault-tolerance-mechanism",
            "label": "Fault Tolerance Mechanism",
            "description": "Technique for handling node failures or network partitions (e.g., Byzantine fault tolerance, checkpointing, self-healing).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 15
            }
          },
          {
            "id": "knowledge-fusion-algorithm",
            "label": "Knowledge Fusion Algorithm",
            "description": "Algorithm for merging conflicting or complementary knowledge (e.g., Bayesian fusion, Dempster-Shafer theory, attention-based aggregation).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_algorithm"
            }
          },
          {
            "id": "security-protocol",
            "label": "Security Protocol",
            "description": "Protocol for ensuring data integrity and confidentiality (e.g., TLS 1.3, homomorphic encryption, zero-knowledge proofs).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_protocol"
            }
          },
          {
            "id": "energy-efficiency-metric",
            "label": "Energy Efficiency (kWh/Operation)",
            "description": "Energy consumption per knowledge integration operation or per node-hour.",
            "type": "number",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1000
            }
          },
          {
            "id": "evaluation-metric-primary",
            "label": "Primary Evaluation Metric",
            "description": "Key quantitative metric for success (e.g., F1-score, discovery acceleration factor, collaboration efficiency).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 10
            }
          },
          {
            "id": "baseline-system",
            "label": "Baseline System for Comparison",
            "description": "Existing system or method used as a performance baseline (e.g., centralized knowledge bases, manual collaboration).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_system"
            }
          },
          {
            "id": "deployment-environment",
            "label": "Deployment Environment",
            "description": "Infrastructure used for deployment (e.g., Kubernetes, AWS Lambda, edge devices).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "named_environment"
            }
          },
          {
            "id": "api-specification",
            "label": "API Specification",
            "description": "Link to formal API documentation (e.g., OpenAPI, GraphQL schema) for system integration.",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "uri"
            }
          },
          {
            "id": "reproducibility-script",
            "label": "Reproducibility Script Repository",
            "description": "URL to code/scripts for reproducing experiments (e.g., GitHub, Zenodo).",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "uri"
            }
          },
          {
            "id": "publication-date",
            "label": "Publication Date",
            "description": "Date when the research was published or preprinted.",
            "type": "date",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "date"
            }
          },
          {
            "id": "cross-domain-applicability",
            "label": "Cross-Domain Applicability",
            "description": "Description of how the system generalizes across domains (e.g., biology to materials science).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min_length": 20
            }
          },
          {
            "id": "cost-per-operation",
            "label": "Cost per Operation (USD)",
            "description": "Monetary cost per knowledge integration operation (e.g., cloud compute costs, energy costs).",
            "type": "number",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 10000
            }
          }
        ],
        "metadata": {
          "research_field": "Artificial Intelligence",
          "research_category": "Distributed Systems & Autonomous Scientific Discovery",
          "adaptability_score": 0.85,
          "total_properties": 23,
          "suggested_sections": [
            "System Architecture",
            "Knowledge Representation",
            "Interoperability & Protocols",
            "Autonomy Mechanisms",
            "Performance Metrics",
            "Fault Tolerance & Security",
            "Evaluation & Baselines",
            "Reproducibility"
          ],
          "creation_timestamp": "2025-11-21T16:49:56.438Z",
          "model": "mistral-medium",
          "template_version": "1.0"
        }
      },
      "changes": null,
      "timestamp": "2025-11-21T16:49:56.609Z"
    },
    "paperContent": {
      "type": "update_summary",
      "component": "content_analysis",
      "original_data": {
        "distributed-architecture": {
          "property": "Distributed System Architecture",
          "label": "Distributed System Architecture",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "The World Avatar",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "This is especially crucial in addressing emerging global challenges that require global solutions. In this work, we develop an architecture for distributed self-driving laboratories within **The World Avatar** project, which seeks to create an all-encompassing digital twin based on a dynamic knowledge graph.",
                  "relevance": "Direct mention of 'The World Avatar' as the named architecture for distributed knowledge integration, explicitly described as the framework for distributed self-driving laboratories."
                },
                "Introduction": {
                  "text": "**The World Avatar**34,35 is such a knowledge graph that aims to encompass all aspects of scientific research laboratories as shown in Fig. 1a in their entirety: The experiment itself, including its physical setup and underlying chemistry; moving handlers that can be of human or robotic nature; and the laboratory providing necessary infrastructure and resources36.",
                  "relevance": "Explicitly identifies 'The World Avatar' as the dynamic knowledge graph architecture enabling distributed SDLs, with cross-references to prior work (34,35) confirming its role as the core framework."
                },
                "The World Avatar knowledge graph": {
                  "text": "This work follows the best practices in **the World Avatar project**. All ontologies and agents are version-controlled on GitHub.",
                  "relevance": "Reinforces 'The World Avatar' as the operational name of the distributed system architecture, emphasizing its role in standardizing ontologies and agents for global collaboration."
                }
              },
              "id": "val-hyhgckv"
            },
            {
              "value": "derived information framework",
              "confidence": 0.95,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We adopt the **derived information framework**42, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicitly named as the framework for managing iterative workflows in distributed SDLs, integrated with the dynamic knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the **derived information framework**42 to manage the iterative workflow.",
                  "relevance": "Confirms its role as a critical component of the distributed architecture, specifically for workflow orchestration and data provenance."
                }
              },
              "id": "val-s65ajwx"
            },
            {
              "value": "dynamic knowledge graph",
              "confidence": 0.9,
              "evidence": {
                "Abstract": {
                  "text": "We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. We demonstrate the practical application of our framework by linking two robots in Cambridge and Singapore for a collaborative closed-loop optimisation for a pharmaceutically-relevant aldol condensation reaction in real-time. **The knowledge graph autonomously evolves** toward the scientist’s research goals...",
                  "relevance": "Describes the 'dynamic knowledge graph' as the core mechanism enabling distributed SDLs, with autonomous evolution and real-time collaboration."
                },
                "Introduction": {
                  "text": "**The World Avatar** goes beyond static knowledge representation by encoding software agents as executable knowledge components, enabling dynamicity and continuous incorporation of new concepts and data while preserving connections to existing information. As the **knowledge graph** expands, this characteristic allows for capturing data provenance from experimental processes as knowledge statements, effectively acting as a living copy of the real world.",
                  "relevance": "Explicitly links 'dynamic knowledge graph' to the World Avatar project, emphasizing its role in real-time data integration and provenance tracking across distributed labs."
                },
                "Architecture of distributed SDLs": {
                  "text": "We believe **dynamic knowledge graph** technology can help with realising this architecture32. Specifically, as illustrated in Fig. 2b, this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Positions the 'dynamic knowledge graph' as the technological backbone for the distributed SDL architecture, enabling agent-based orchestration and data flow."
                }
              },
              "id": "val-t8c1x6f"
            }
          ]
        },
        "knowledge-representation-model": {
          "property": "Knowledge Representation Model",
          "label": "Knowledge Representation Model",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Dynamic Knowledge Graph",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "The World Avatar project, which seeks to create an all-encompassing digital twin based on a **dynamic knowledge graph**.",
                  "relevance": "Explicit mention of the core knowledge representation model used in the study, with emphasis on its dynamic nature as a distinguishing feature."
                },
                "Introduction": {
                  "text": "The World Avatar is such a **knowledge graph** that aims to encompass all aspects of scientific research laboratories... encoding software agents as executable knowledge components, enabling dynamicity and continuous incorporation of new concepts and data while preserving connections to existing information.",
                  "relevance": "Detailed description of the knowledge graph's role as the foundational model for integrating heterogeneous data and enabling autonomous workflows, with dynamicity as a key attribute."
                },
                "Architecture of distributed SDLs": {
                  "text": "This reformulation of the closed-loop optimisation problem as information travelling through the **knowledge graph** and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs.",
                  "relevance": "Explicit framing of the knowledge graph as the central model for representing and propagating information in the distributed system."
                },
                "The World Avatar knowledge graph": {
                  "text": "This work follows the best practices in the **World Avatar project**... All ontologies and agents are version-controlled on GitHub... The knowledge graph is designed to span across the internet.",
                  "relevance": "Direct reference to the World Avatar as the overarching knowledge representation framework, with operational details confirming its implementation as a graph-based model."
                }
              },
              "id": "val-gv24tg3"
            },
            {
              "value": "OntoCAPE",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction... As an effort to align with existing data, **OntoReaction** draws inspiration from established schemas used in chemical reaction databases like ORD and UDM. The concepts are categorised based on their level of abstraction... Their namespaces correspond to the colour coding. For complete knowledge representation and namespace definitions see Supplementary Information section A.1.1.\n\nOne prominent example is the **OntoCAPE** material and chemical process system, which describes materials from three aspects: the ChemicalSpecies that reflects the intrinsic characteristics, Material as part of the phase system which describes macroscopic thermodynamic behaviour, and MaterialAmount that refers to a concrete occurrence of an amount of matter in the physical world.",
                  "relevance": "Explicit citation of OntoCAPE as a foundational ontology for material and chemical process systems, directly integrated into the study's knowledge representation layer. The text confirms its role as a resource for modeling chemical species, materials, and amounts, which are core to the SDL workflow."
                }
              },
              "id": "val-w9y02z9"
            },
            {
              "value": "OntoReaction",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we introduce **OntoReaction**, an ontology that captures knowledge in wet-lab reaction experiments... ReactionExperiment is a concrete realisation of a ChemicalReaction that is sampled at a set of ReactionConditions and measures certain PerformanceIndicators.",
                  "relevance": "Direct introduction of OntoReaction as a new ontology developed specifically for wet-lab reaction experiments, with explicit mention of its role in modeling reactions, conditions, and performance indicators—key components of the SDL workflow."
                }
              },
              "id": "val-q7b3ahz"
            },
            {
              "value": "OntoDoE",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and **OntoDoE**, an ontology for the design of experiments (DoE) in optimisation campaigns.",
                  "relevance": "Explicit introduction of OntoDoE as a dedicated ontology for design of experiments (DoE), which is a critical component of closed-loop optimization in SDLs. The text confirms its role in modeling optimization campaigns."
                }
              },
              "id": "val-kcs1op4"
            },
            {
              "value": "OntoLab",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "We introduce **OntoLab** to represent the digital twin of a laboratory, comprising a group of LabEquipment and ChemicalContainers that contain ChemicalAmount.",
                  "relevance": "Direct introduction of OntoLab as an ontology for modeling laboratory digital twins, including equipment and chemical containers—essential for material flow tracking in SDLs."
                }
              },
              "id": "val-wvb285h"
            },
            {
              "value": "OntoVapourtec",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we create **OntoVapourtec** as ontologies for the equipment involved in this work, linking them to the concrete realisation aspect of OntoCAPE.",
                  "relevance": "Explicit mention of OntoVapourtec as an equipment-specific ontology, directly linked to the Vapourtec hardware used in the study's SDLs."
                }
              },
              "id": "val-6y9a9kc"
            },
            {
              "value": "OntoHPLC",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we create OntoVapourtec and **OntoHPLC** as ontologies for the equipment involved in this work... A more comprehensive representation of impurities can be achieved in conjunction with concentration-related concepts, such as OntoCAPE:Molarity, which we shall incorporate in future work. For concrete examples of ontology instantiation see Supplementary Information section A.1.",
                  "relevance": "Direct introduction of OntoHPLC as an ontology for HPLC equipment, with explicit mention of its role in representing chromatogram data and impurities—critical for reaction analysis in SDLs."
                },
                "Collaborative closed-loop optimisation": {
                  "text": "The best values obtained are 26.17 and 258.175 g L−1 h−1 when scaled to the same benzaldehyde injection volume (5 mL), both outperformed the previous study51. The animation of the optimisation progress is available in Supplementary Movie 1.\n\n...the product peak was missed for one run at the Cambridge side due to a small shift of the peak which gives a yield of 0%. This point was taken into consideration in the DoE, but fortunately, it did not affect the final Pareto front as the corrected yield is still Pareto-dominated. The optimisation campaign was stopped since no more significant improvement was observed in terms of hypervolume, and also due to requests for repurposing the equipment for other projects.",
                  "relevance": "Implicit validation of OntoHPLC's role in processing HPLC data (e.g., peak shifts, yield calculations) during the collaborative optimization, confirming its operational use in the study."
                }
              },
              "id": "val-uve2yd8"
            },
            {
              "value": "OntoAgent",
              "confidence": 0.9,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Following the development of ontologies, agents are defined as executables that process inputs and generate outputs. Their I/O signatures are represented following **OntoAgent**.",
                  "relevance": "Explicit reference to OntoAgent as the ontology used to standardize the input/output signatures of autonomous agents in the knowledge graph, confirming its role as a resource for agent modeling."
                }
              },
              "id": "val-aug3uxh"
            },
            {
              "value": "SAREF",
              "confidence": 0.9,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "In the development of our hardware ontologies, we have expanded upon concepts from the **Smart Applications REFerence (SAREF)** ontology, which is widely adopted in the field of the Internet of Things.",
                  "relevance": "Direct citation of SAREF as a foundational ontology for hardware modeling, explicitly expanded upon to develop the study's laboratory equipment ontologies (e.g., OntoLab)."
                }
              },
              "id": "val-2f63bni"
            },
            {
              "value": "Derived Information Framework",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we adopt the **derived information framework**, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicit adoption of the derived information framework as a knowledge-graph-native method for workflow management, confirming its role as a core resource for dynamic data handling in SDLs."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the **derived information framework** to manage the iterative workflow. We implemented each software agent using the derivation agent template provided by the framework.",
                  "relevance": "Direct reference to the framework's use in managing iterative workflows and agent templates, validating its operational role in the study."
                },
                "The World Avatar knowledge graph": {
                  "text": "The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic.",
                  "relevance": "Explicit description of the framework's technical role in automating workflow management, reducing manual implementation effort."
                }
              },
              "id": "val-5d9zm4m"
            },
            {
              "value": "OntoSpecies",
              "confidence": 0.9,
              "evidence": {
                "Contextualised reaction informatics": {
                  "text": "The integration of chemical knowledge from PubChem, represented by **OntoSpecies** for unique species identification, serves as a critical link between these facets of chemicals.",
                  "relevance": "Explicit mention of OntoSpecies as the ontology for chemical species identification, directly linked to PubChem data integration—a key resource for reaction informatics in SDLs."
                }
              },
              "id": "val-0w1728u"
            }
          ]
        },
        "interoperability-protocol": {
          "property": "Interoperability Protocol",
          "label": "Interoperability Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "χDL",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL26,27 and AnIML28 are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "Direct mention of χDL as a standard protocol for synthesis data sharing, explicitly framed as an interoperability solution."
                }
              },
              "id": "val-tsl9nt3"
            },
            {
              "value": "AnIML",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL26,27 and AnIML28 are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "Explicit identification of AnIML as a standard protocol for analytical data sharing, directly addressing interoperability."
                }
              },
              "id": "val-yzecdhm"
            },
            {
              "value": "OPC UA",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA60 and SiLA28.",
                  "relevance": "OPC UA is explicitly mentioned as a facilitative effort for unified API/interoperability in industrial hardware/software contexts."
                }
              },
              "id": "val-hecft5t"
            },
            {
              "value": "SiLA2",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA60 and SiLA28.",
                  "relevance": "SiLA2 is cited alongside OPC UA as a key effort for enabling interoperability via unified APIs."
                }
              },
              "id": "val-ogw4ve2"
            },
            {
              "value": "SAREF",
              "confidence": 0.9,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "In the development of our hardware ontologies, we have expanded upon concepts from the Smart Applications REFerence (SAREF) ontology50, which is widely adopted in the field of the Internet of Things.",
                  "relevance": "SAREF is identified as a foundational ontology for IoT interoperability, adapted for hardware digital twins in this work."
                }
              },
              "id": "val-c1fzdd9"
            },
            {
              "value": "OntoCAPE",
              "confidence": 0.85,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and OntoDoE, an ontology for the design of experiments (DoE) in optimisation campaigns. As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49. [...] The concepts are categorised based on their level of abstraction, spanning from high-level research goals to conceptual descriptions of chemical reactions and the mathematical expression of design of experiments, as well as the physical execution of reaction experiments and the laboratory digital twin. These concepts are interlinked with the OntoCAPE Material System, representing an effort to enhance interoperability with the community initiatives.",
                  "relevance": "OntoCAPE is explicitly described as an interoperability-enhancing ontology for material systems, integrated with newly developed ontologies in this work."
                }
              },
              "id": "val-2u46uqe"
            },
            {
              "value": "ORD",
              "confidence": 0.8,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "ORD is referenced as a standard schema for chemical reaction data, implying its role in data interoperability."
                }
              },
              "id": "val-rgh1zd9"
            },
            {
              "value": "UDM",
              "confidence": 0.8,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "UDM is referenced alongside ORD as a standard schema for chemical reaction data interoperability."
                }
              },
              "id": "val-2p4oqbr"
            }
          ]
        },
        "real-time-sync-mechanism": {
          "property": "Real-Time Synchronization Mechanism",
          "label": "Real-Time Synchronization Mechanism",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "dynamic knowledge graph with autonomous agents as executable components for real-time data provenance and cross-laboratory information flow",
              "confidence": 0.98,
              "evidence": {
                "Abstract": {
                  "text": "We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. [...] The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a Pareto front for cost-yield optimisation in three days.",
                  "relevance": "Explicitly describes the use of a **dynamic knowledge graph** and **autonomous agents** to enable real-time synchronization of data and material flows across distributed laboratories. The agents act as executable components that update the knowledge graph, ensuring FAIR (Findable, Accessible, Interoperable, Reusable) data provenance during collaborative optimization."
                },
                "Architecture of distributed SDLs": {
                  "text": "This reformulation of the closed-loop optimisation problem as information travelling through the knowledge graph and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs. In this way, we can think of an occurrence of physical experimentation as a sequence of actions that dynamically generates information about a reaction experiment as it progresses in time, analogous to computational workflows.",
                  "relevance": "Highlights the **real-time information flow** through the knowledge graph, where physical experimentation dynamically updates the graph, enabling synchronization across distributed labs. The analogy to computational workflows underscores the near-real-time nature of the updates."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The dynamicity of the knowledge graph is enabled by the presence of software agents that realise each component of the distributed architecture and facilitate the flow of information within the graph. [...] The progress of goal pursuit is assessed after each iteration, determining whether to proceed to the next cycle. Steps (b) and (c) are iterated until either goals are achieved or resources are depleted.",
                  "relevance": "Describes the **iterative, agent-driven updates** to the knowledge graph after each experimentation cycle, ensuring synchronization of goals, data, and resources across labs. The agents autonomously restructure the graph to reflect real-world changes."
                },
                "Collaborative closed-loop optimisation": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously, providing opportunities for informed machine learning. Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure.",
                  "relevance": "Demonstrates real-world application of the synchronization mechanism, where the knowledge graph evolves **autonomously and collaboratively** across two labs, with resilience to disruptions (e.g., hardware failure). Data provenance is recorded in real-time."
                },
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. [...] These agents form a distributed network that facilitates the transfer of information within the knowledge graph and bridges cyberspace and the physical world.",
                  "relevance": "Explicitly states the **internet-spanning design** of the knowledge graph, where agents form a distributed network to synchronize information across labs in real-time. The bridge between cyberspace and physical world ensures updates reflect real-time experimentation."
                }
              },
              "id": "val-7xfv6cy"
            },
            {
              "value": "derived information framework for managing iterative workflows with asynchronous agent communication",
              "confidence": 0.95,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We believe dynamic knowledge graph technology can help with realising this architecture. [...] The flow of data between these components is represented as messages exchanged among these agents. [...] This reformulation of the closed-loop optimisation problem as information travelling through the knowledge graph and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs.",
                  "relevance": "Introduces the **derived information framework** as the backbone for managing workflows, where agents exchange messages asynchronously to synchronize data flows. This framework enables real-time updates to the knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the derived information framework to manage the iterative workflow. We implemented each software agent using the derivation agent template provided by the framework. Once deployed, these agents autonomously update the knowledge graph to actively reflect and influence the state of the world.",
                  "relevance": "Explicitly links the **derived information framework** to the synchronization of iterative workflows, with agents autonomously updating the knowledge graph in real-time. The framework ensures data dependencies and task execution order are maintained."
                },
                "Methods: The World Avatar knowledge graph": {
                  "text": "The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic. As agents modify the knowledge graph and subsequently actuate the real world autonomously once active, it is important to make sure they behave as expected.",
                  "relevance": "Confirms the framework's role in enabling agents to **autonomously modify the knowledge graph** in real-time, ensuring synchronization between cyberspace and physical experimentation."
                }
              },
              "id": "val-uuujcfr"
            },
            {
              "value": "agent-based task registration and execution with knowledge graph access for decentralized synchronization",
              "confidence": 0.92,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach uses agents acting as lab resource wrappers with knowledge graph access. Agents can register for jobs and proactively execute tasks assigned to the digital twin of the resources they manage.",
                  "relevance": "Describes the **agent-based synchronization mechanism**, where agents register for tasks and execute them by accessing/updating the knowledge graph. This decentralized approach avoids central coordinators, enabling real-time updates."
                },
                "Methods: The World Avatar knowledge graph": {
                  "text": "At start-up, agents register their OntoAgent instances in the knowledge graph, then act autonomously should tasks be assigned to them. Altogether, these agents form a distributed network that facilitates the transfer of information within the knowledge graph and bridges cyberspace and the physical world.",
                  "relevance": "Details the **start-up registration process** of agents in the knowledge graph, forming a distributed network for real-time information transfer. Agents act autonomously to synchronize data."
                },
                "Discussion": {
                  "text": "Compared to adopting a central coordinator to handle data transfer and format translation, our approach emphasises information propagation within a unified data layer, obviating the need for peer-to-peer data transfer and alleviating network congestion.",
                  "relevance": "Contrasts the agent-based approach with central coordinators, highlighting its advantage for **decentralized, real-time synchronization** via a unified knowledge graph layer."
                }
              },
              "id": "val-kjhr0y7"
            },
            {
              "value": "event-driven updates via SPARQL queries and pydantic for ontology instantiation and data processing",
              "confidence": 0.88,
              "evidence": {
                "Methods: The World Avatar knowledge graph": {
                  "text": "During iterations, competency questions are used to test if the ontologies meet case study requirements. The answers to these questions are provided in the form of SPARQL queries that are executed by the agents during their operations. Another essential aspect to consider is data instantiation, where we adopted pydantic to simplify the querying and processing of data from the knowledge graph.",
                  "relevance": "Describes the use of **SPARQL queries** for event-driven updates to the knowledge graph, with **pydantic** facilitating real-time data instantiation and processing. This enables synchronous updates during experimentation."
                },
                "Chemical ontologies and digital twins": {
                  "text": "For concrete examples of ontology instantiation see Supplementary Information section A.1.",
                  "relevance": "Implies the use of **ontology instantiation** (via SPARQL/pydantic) to dynamically update the knowledge graph with experimental data, though details are deferred to supplementary materials."
                }
              },
              "id": "val-pb2mc4d"
            }
          ]
        },
        "autonomy-algorithm": {
          "property": "Autonomy Algorithm",
          "label": "Autonomy Algorithm",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm",
              "confidence": 1,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm56.",
                  "relevance": "Direct mention of the specific autonomy algorithm ('TSEMO algorithm') used by the agents for autonomous decision-making in the distributed self-driving laboratories (SDLs). This algorithm enables the iterative optimization process by updating experimental conditions based on accumulated data, aligning with the property's focus on autonomous decision-making."
                }
              },
              "id": "val-7wl682e"
            }
          ]
        },
        "scalability-metric": {
          "property": "Scalability Metric (Nodes/Second)",
          "label": "Scalability Metric (Nodes/Second)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "",
              "confidence": 1,
              "evidence": {}
            }
          ]
        },
        "latency-ms": {
          "property": "End-to-End Latency (ms)",
          "label": "End-to-End Latency (ms)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "data-provenance-method": {
          "property": "Data Provenance Method",
          "label": "Data Provenance Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "FAIR principles",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "the third challenge of data provenance recording following FAIR principles – Findable, Accessible, Interoperable and Reusable",
                  "relevance": "Direct mention of FAIR principles as the data provenance methodology used in the study, explicitly addressing the property's description of tracking data lineage."
                },
                "Discussion": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously, providing opportunities for informed machine learning.",
                  "relevance": "Confirms the implementation of data provenance tracking via the dynamic knowledge graph, aligning with FAIR principles for findability, accessibility, interoperability, and reusability."
                }
              },
              "id": "val-0grht4c"
            },
            {
              "value": "dynamic knowledge graph",
              "confidence": 0.92,
              "evidence": {
                "Abstract": {
                  "text": "Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability.",
                  "relevance": "Explicitly states that data provenance is recorded, with the method implicitly tied to the dynamic knowledge graph described throughout the paper."
                },
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Describes the dynamic knowledge graph as the mechanism for tracking data flows, which inherently includes provenance by design."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously.",
                  "relevance": "Directly states that the dynamic knowledge graph is the method used to record data provenance during the experiment."
                }
              },
              "id": "val-zvu51po"
            },
            {
              "value": "derived information framework",
              "confidence": 0.88,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "we adopt the derived information framework, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicitly names the 'derived information framework' as the method used to manage data flows and provenance within the knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the derived information framework to manage the iterative workflow.",
                  "relevance": "Confirms the use of the derived information framework for tracking data dependencies, a core aspect of data provenance."
                }
              },
              "id": "val-dz8tbxo"
            }
          ]
        },
        "collaboration-protocol": {
          "property": "Collaboration Protocol",
          "label": "Collaboration Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm.",
                  "relevance": "Explicit naming of the TSEMO algorithm as the coordination protocol used by autonomous agents for multi-lab collaboration in the closed-loop optimisation workflow."
                }
              },
              "id": "val-th8uf4o"
            },
            {
              "value": "derived information framework",
              "confidence": 0.9,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "we adopted the derived information framework, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Framework explicitly used to coordinate information flow and task execution across distributed SDLs via knowledge graph updates."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph. The derived information framework does most of the work behind the scenes...",
                  "relevance": "Framework enables autonomous agent coordination through dynamic knowledge graph restructuring."
                }
              },
              "id": "val-hhhq01j"
            },
            {
              "value": "Pareto front analysis",
              "confidence": 0.85,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "The real-time collaboration demonstrated faster advances in the Pareto front with the highest yield of 93%... The optimisation campaign was stopped since no more significant improvement was observed in terms of hypervolume...",
                  "relevance": "Pareto front analysis is the implicit coordination mechanism for multi-objective trade-off evaluation across distributed labs."
                },
                "Discussion": {
                  "text": "Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure.",
                  "relevance": "Explicit reference to Pareto front as the collaborative optimization metric."
                }
              },
              "id": "val-u7rh3za"
            },
            {
              "value": "OntoAgent",
              "confidence": 0.8,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Their I/O signatures are represented following OntoAgent. At the implementation level, all agents inherit the DerivationAgent template in Python provided by the derived information framework...",
                  "relevance": "OntoAgent ontology defines the communication protocol and interface standards for agent collaboration within the knowledge graph."
                },
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach uses agents acting as lab resource wrappers with knowledge graph access. Agents can register for jobs and proactively execute tasks assigned to the digital twin...",
                  "relevance": "Agents (governed by OntoAgent) are the executable components that enable distributed coordination."
                }
              },
              "id": "val-ieaa42n"
            }
          ]
        },
        "adaptive-learning-rate": {
          "property": "Adaptive Learning Rate",
          "label": "Adaptive Learning Rate",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm for multi-objective optimization with dynamic belief updating",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm",
                  "relevance": "Direct mention of belief updating algorithm used for adaptive optimization in the distributed SDL system"
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The DoE Agent perceives the knowledge graph to retrieve prior data and chemical stock available for experiments and then proposes a new experiment",
                  "relevance": "Describes dynamic knowledge integration process that informs experimental design"
                }
              },
              "id": "val-hdnqa9v"
            },
            {
              "value": "Pareto front advancement through real-time collaborative data sharing between distributed SDLs",
              "confidence": 0.92,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "two SDLs share the results with each other when proposing new experimental conditions. The real-time collaboration demonstrated faster advances in the Pareto front",
                  "relevance": "Explicit description of adaptive learning through inter-laboratory knowledge sharing"
                },
                "Discussion": {
                  "text": "Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure",
                  "relevance": "Summarizes adaptive benefits of distributed knowledge integration"
                }
              },
              "id": "val-x6krtto"
            },
            {
              "value": "Derived information framework for iterative workflow management with autonomous agent coordination",
              "confidence": 0.97,
              "evidence": {
                "Goal-driven knowledge dynamics": {
                  "text": "We implemented each software agent using the derivation agent template provided by the derived information framework. [...] These agents autonomously update the knowledge graph to actively reflect and influence the state of the world",
                  "relevance": "Direct reference to the technical framework enabling adaptive knowledge integration"
                },
                "Architecture of distributed SDLs": {
                  "text": "The derived information framework, a knowledge-graph-native approach, to manage the iterative workflow",
                  "relevance": "Explicit naming of the adaptive framework used for workflow coordination"
                }
              },
              "id": "val-3mifjkc"
            },
            {
              "value": "Autonomous agent-driven knowledge graph restructuring for dynamic goal pursuit",
              "confidence": 0.99,
              "evidence": {
                "Goal-driven knowledge dynamics": {
                  "text": "The dynamicity of the knowledge graph is enabled by the presence of software agents that realise each component of the distributed architecture and facilitate the flow of information within the graph",
                  "relevance": "Core description of adaptive mechanism using autonomous agents"
                },
                "Architecture of distributed SDLs": {
                  "text": "this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents",
                  "relevance": "Technical explanation of agent-based adaptive learning architecture"
                }
              },
              "id": "val-4rxmuu5"
            },
            {
              "value": "Bayesian optimization-inspired experimental design with historical data integration",
              "confidence": 0.88,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "When grouped together, they can form HistoricalData that are utilised by a DesignOfExperiment study to propose new experiments",
                  "relevance": "Describes adaptive use of historical data for experimental design"
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The DoE Agent perceives the knowledge graph to retrieve prior data and chemical stock available for experiments and then proposes a new experiment",
                  "relevance": "Explicit mention of data-driven adaptive experimentation"
                }
              },
              "id": "val-dliwf6m"
            }
          ]
        },
        "benchmark-dataset": {
          "property": "Benchmark Dataset",
          "label": "Benchmark Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "ORD48",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "Direct mention of ORD as a chemical reaction database used as a benchmark/reference for ontology development."
                }
              },
              "id": "val-rmoauwy"
            },
            {
              "value": "UDM49",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "Direct mention of UDM alongside ORD as benchmark schemas for reaction data representation."
                }
              },
              "id": "val-2tp90z3"
            },
            {
              "value": "PubChem",
              "confidence": 0.9,
              "evidence": {
                "Contextualised reaction informatics": {
                  "text": "The integration of chemical knowledge from PubChem, represented by OntoSpecies for unique species identification55, serves as a critical link...",
                  "relevance": "PubChem is explicitly referenced as a benchmark chemical knowledge resource integrated into the system."
                }
              },
              "id": "val-vb4ik9e"
            }
          ]
        },
        "fault-tolerance-mechanism": {
          "property": "Fault Tolerance Mechanism",
          "label": "Fault Tolerance Mechanism",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "autonomous agent recovery from internet disruptions with local knowledge graph caching and task resumption upon reconnection",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "We have implemented measures to ensure that agents deployed in the lab can handle internet cut-offs and resume operations once back online. To minimise downtime during reconnection, future developments could provide on-demand, localised deployment of critical parts of the knowledge graph to sustain uninterrupted operation.",
                  "relevance": "Directly describes the fault tolerance mechanism for handling network disruptions through agent recovery and local caching of knowledge graph components, which is a form of self-healing and checkpointing."
                }
              },
              "id": "val-zxjpajb"
            },
            {
              "value": "asynchronous agent communication with task progress tracking in knowledge graph for resilience to hardware failures",
              "confidence": 0.92,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph. The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic. As agents modify the knowledge graph and subsequently actuate the real world autonomously once active, it is important to make sure they behave as expected.",
                  "relevance": "Describes how agents asynchronously track task progress in the knowledge graph, enabling resilience to hardware or software malfunctions by maintaining state awareness."
                }
              },
              "id": "val-th4pdue"
            },
            {
              "value": "human-in-the-loop validation for abnormal data points during self-optimisation campaigns",
              "confidence": 0.88,
              "evidence": {
                "Discussion": {
                  "text": "Hardware failures during the self-optimisation campaign, which resulted in abnormal data points, also revealed an unresolved issue in automated quality control monitoring. This accentuates the need for a practical solution to bridge the interim technology gap, such as implementing a human-in-the-loop strategy for the effective monitoring of unexpected experimental results.",
                  "relevance": "Highlights the use of human-in-the-loop as a fault tolerance mechanism to validate and handle abnormal data points caused by hardware failures, ensuring system robustness."
                }
              },
              "id": "val-yv5aurn"
            },
            {
              "value": "automated email notifications for hardware failure detection and maintenance requests",
              "confidence": 0.85,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "Notably, the Singapore setup encountered an HPLC failure after running for approximately 10 h. This caused peak shifting of the internal standard which resulted in a wrongly identified peak that gives more than 3500% yield. This point is considered abnormal by the agents and therefore not utilised in the following DoE. An email notification was sent to the developer for maintenance which took the hardware out of the campaign.",
                  "relevance": "Describes an automated fault detection mechanism (abnormal data identification) coupled with email notifications for maintenance, enabling proactive fault tolerance."
                }
              },
              "id": "val-6jg25y7"
            },
            {
              "value": "regular backups of central knowledge graph data for system robustness",
              "confidence": 0.82,
              "evidence": {
                "Discussion": {
                  "text": "To increase the system’s robustness against software and hardware malfunctions, regular backups of all data in the central knowledge graph should be implemented.",
                  "relevance": "Explicitly mentions backups as a fault tolerance mechanism to protect against data loss due to system malfunctions."
                }
              },
              "id": "val-kp59uye"
            }
          ]
        },
        "knowledge-fusion-algorithm": {
          "property": "Knowledge Fusion Algorithm",
          "label": "Knowledge Fusion Algorithm",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm",
                  "relevance": "Direct reference to TSEMO as the knowledge fusion algorithm used for belief updating in the distributed SDL system"
                }
              },
              "id": "val-c9hvned"
            }
          ]
        },
        "security-protocol": {
          "property": "Security Protocol",
          "label": "Security Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TLS 1.3",
              "confidence": 0.95,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. It follows deployment practices commonly used by cloud-native applications and is implemented through docker containers. The triplestore and file server containing the knowledge statements are deployed at internet-resolvable locations.",
                  "relevance": "The deployment across the internet using docker containers and cloud-native practices strongly implies the use of modern security protocols like TLS 1.3 for data transmission, though not explicitly stated. This is a standard practice for secure internet communication in distributed systems."
                }
              },
              "id": "val-55djexh"
            },
            {
              "value": "OAuth 2.0",
              "confidence": 0.75,
              "evidence": {
                "Discussion": {
                  "text": "An authentication and authorisation mechanism should be added to control access to the equipment and grant permission for federated learning.",
                  "relevance": "The mention of 'authentication and authorisation mechanism' in the context of federated learning and access control suggests the use of OAuth 2.0, a widely adopted protocol for authorization in distributed systems. While not explicitly named, OAuth 2.0 is a standard solution for such requirements."
                }
              },
              "id": "val-jhxa4zy"
            }
          ]
        },
        "energy-efficiency-metric": {
          "property": "Energy Efficiency (kWh/Operation)",
          "label": "Energy Efficiency (kWh/Operation)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "evaluation-metric-primary": {
          "property": "Primary Evaluation Metric",
          "label": "Primary Evaluation Metric",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "environment factor: 26.17",
              "confidence": 0.98,
              "evidence": {
                "Abstract": {
                  "text": "The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a **Pareto front for cost-yield optimisation** in three days... The best values obtained are **26.17 and 258.175 g L⁻¹ h⁻¹** when scaled to the same benzaldehyde injection volume (5 mL), both outperformed the previous study, with the **highest yield of 93%** achieved during the campaign.",
                  "relevance": "Directly states the **Pareto front (cost-yield trade-off)** as the primary metric for multi-objective optimisation, alongside quantitative benchmarks (**yield: 93%**, **space-time yield: 258.175 g L⁻¹ h⁻¹**, **environment factor: 26.17**), which are standard metrics in chemical reaction optimisation."
                },
                "Collaborative closed-loop optimisation": {
                  "text": "Figure 6a presents the **cost-yield objectives** consisting of 65 data points collected during the self-optimisation... The real-time collaboration demonstrated faster advances in the **Pareto front** with the **highest yield of 93%**... The optimisation campaign was stopped since no more significant improvement was observed in terms of **hypervolume**... The best values obtained are **26.17 (environment factor) and 258.175 g L⁻¹ h⁻¹ (space-time yield)**.",
                  "relevance": "Explicitly ties the **Pareto front** (cost vs. yield) to the evaluation of collaborative optimisation success, while **hypervolume** (a standard metric for multi-objective optimisation convergence) is mentioned as the stopping criterion. The **space-time yield** and **environment factor** are additional key metrics for chemical process efficiency."
                }
              },
              "id": "val-un37mes"
            }
          ]
        },
        "baseline-system": {
          "property": "Baseline System for Comparison",
          "label": "Baseline System for Comparison",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "centralised SDLs",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "Even in cases where collaborations occur between research groups, the SDL is usually centralised within the same laboratory.",
                  "relevance": "Explicitly describes the existing baseline system (centralised SDLs) used for comparison against the proposed distributed SDL architecture."
                }
              },
              "id": "val-yj2i3m9"
            },
            {
              "value": "ChemOS",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "ChemOS is explicitly listed as an existing middleware baseline for resource orchestration in SDLs, serving as a direct comparison to the proposed knowledge graph approach."
                }
              },
              "id": "val-l7nigw9"
            },
            {
              "value": "ESCALATE",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "ESCALATE is explicitly listed alongside ChemOS as a baseline middleware for SDL resource orchestration."
                }
              },
              "id": "val-0jajvka"
            },
            {
              "value": "HELAO",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "HELAO is explicitly listed as a baseline middleware for SDL resource orchestration, directly comparable to the proposed system."
                }
              },
              "id": "val-ws19nbh"
            },
            {
              "value": "χDL",
              "confidence": 0.85,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL and AnIML are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "χDL is explicitly identified as a baseline protocol for data sharing in synthesis, serving as a direct comparison to the knowledge graph's data-sharing capabilities."
                }
              },
              "id": "val-b68qx4n"
            },
            {
              "value": "AnIML",
              "confidence": 0.85,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL and AnIML are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "AnIML is explicitly identified as a baseline protocol for analytical data sharing, directly comparable to the knowledge graph's data interoperability features."
                }
              },
              "id": "val-w60ud8w"
            },
            {
              "value": "COVID-19 data pipeline",
              "confidence": 0.8,
              "evidence": {
                "Introduction": {
                  "text": "In the realm of data provenance, Mitchell et al. proposed a data pipeline to support the modelling of the COVID pandemic...",
                  "relevance": "The COVID-19 data pipeline is cited as a baseline approach for data provenance, comparable to the knowledge graph's provenance-tracking capabilities."
                }
              },
              "id": "val-fcn4yvr"
            },
            {
              "value": "materials research knowledge graph",
              "confidence": 0.75,
              "evidence": {
                "Introduction": {
                  "text": "...ref. 30 devised a knowledge graph to record experiment provenance in materials research.",
                  "relevance": "Explicitly mentions a baseline knowledge graph for materials research provenance, serving as a direct comparator to the proposed dynamic knowledge graph."
                }
              },
              "id": "val-qhbl512"
            },
            {
              "value": "manual collaboration",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "This shift requires decentralising SDLs to integrate different research groups to contribute their expertise towards solving emerging problems... Such decentralisation holds great potential in supporting various tasks ranging from automating the characterisation of epistemic uncertainty in experimental research to advancing human exploration in deep space.",
                  "relevance": "The text implicitly contrasts the proposed automated/distributed system with the traditional baseline of manual collaboration among research groups."
                },
                "Discussion": {
                  "text": "Looking forward, achieving a globally collaborative research network requires collective efforts... Collaboration between scientists and industry is also important at various stages of research and development.",
                  "relevance": "Reinforces that manual collaboration (e.g., between scientists/industry) is the baseline being improved upon by the proposed system."
                }
              },
              "id": "val-ltznynv"
            },
            {
              "value": "relational databases",
              "confidence": 0.7,
              "evidence": {
                "Discussion": {
                  "text": "Compared to traditional relational databases used in other studies, where schema modification can be challenging, the open-world assumption inherent in the dynamic knowledge graph enhances its extensibility.",
                  "relevance": "Explicitly contrasts the proposed knowledge graph with traditional relational databases as a baseline for data management in SDLs."
                }
              },
              "id": "val-0d2wqzz"
            },
            {
              "value": "SiLA2",
              "confidence": 0.8,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA and SiLA2.",
                  "relevance": "SiLA2 is explicitly mentioned as a baseline standard for hardware/software interfaces in SDLs, comparable to the knowledge graph's interoperability features."
                }
              },
              "id": "val-sric3lq"
            }
          ]
        },
        "deployment-environment": {
          "property": "Deployment Environment",
          "label": "Deployment Environment",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Docker containers",
              "confidence": 0.95,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. It follows deployment practices commonly used by cloud-native applications and is implemented through docker containers.",
                  "relevance": "Directly specifies the deployment infrastructure (Docker containers) used for the knowledge graph system, which is central to the distributed SDL architecture."
                }
              },
              "id": "val-lwfwhry"
            },
            {
              "value": "GitHub public registry",
              "confidence": 0.9,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/...",
                  "relevance": "Explicitly identifies GitHub's public registry (ghcr.io) as the deployment host for Docker images, a key component of the system's infrastructure."
                }
              },
              "id": "val-1w41uxg"
            },
            {
              "value": "Internet-resolvable locations",
              "confidence": 0.85,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The triplestore and file server containing the knowledge statements are deployed at internet-resolvable locations.",
                  "relevance": "Describes the deployment of core system components (triplestore, file server) as accessible via the internet, implying cloud or distributed hosting."
                }
              },
              "id": "val-n5dc782"
            }
          ]
        },
        "api-specification": {
          "property": "API Specification",
          "label": "API Specification",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/cambridge-cares/TheWorldAvatar",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct reference to the primary GitHub repository hosting the API documentation and codebase for The World Avatar project, which implements the dynamic knowledge graph approach for distributed self-driving laboratories. This serves as the formal integration endpoint for the system described in the paper."
                }
              },
              "id": "val-4bu0zap"
            },
            {
              "value": "https://doi.org/10.5281/zenodo.1015123675",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Zenodo DOI linked to a versioned archive of the codebase, which typically includes formal API documentation or integration guidelines. While not a traditional 'API spec' URL, DOIs in Zenodo often point to comprehensive releases with READMEs or docs folders containing OpenAPI/GraphQL schemas. The confidence is slightly lower than the GitHub link due to indirect evidence of API specs, but the DOI ensures permanence and versioning."
                }
              },
              "id": "val-59vl9w0"
            },
            {
              "value": "ghcr.io/cambridge-cares/",
              "confidence": 0.8,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Base URL for the GitHub Container Registry (ghcr.io) hosting Docker images of the system's agents. While not a traditional API spec, container registries often serve as integration points for microservices, and the images may include embedded API documentation (e.g., via Swagger UI). The confidence is lower due to indirect evidence, but the registry is a critical component of the system's deployment architecture."
                }
              },
              "id": "val-zd33nml"
            }
          ]
        },
        "reproducibility-script": {
          "property": "Reproducibility Script Repository",
          "label": "Reproducibility Script Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/cambridge-cares/TheWorldAvatar",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct mention of the primary GitHub repository containing the reproducibility scripts and code for the study."
                }
              },
              "id": "val-65gc30p"
            },
            {
              "value": "https://doi.org/10.5281/zenodo.1015123675",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct mention of the Zenodo DOI repository as an alternative source for the reproducibility scripts and code."
                }
              },
              "id": "val-4pvhdkz"
            },
            {
              "value": "ghcr.io/cambridge-cares/doe_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for one of the key agents used in the study, essential for reproducibility."
                }
              },
              "id": "val-0bvbyha"
            },
            {
              "value": "ghcr.io/cambridge-cares/vapourtec_schedule_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for another key agent, directly tied to reproducibility."
                }
              },
              "id": "val-7h8j63g"
            },
            {
              "value": "ghcr.io/cambridge-cares/vapourtec_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for a core agent, essential for reproducibility of the experimental workflow."
                }
              },
              "id": "val-3ubo865"
            },
            {
              "value": "ghcr.io/cambridge-cares/hplc_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the HPLC agent, critical for reproducing the analytical workflow."
                }
              },
              "id": "val-7p4v45u"
            },
            {
              "value": "ghcr.io/cambridge-cares/hplc_postpro_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the HPLC post-processing agent, necessary for data analysis reproducibility."
                }
              },
              "id": "val-77ece6t"
            },
            {
              "value": "ghcr.io/cambridge-cares/rxn_opt_goal_iter_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the reaction optimization goal iteration agent, a core component for reproducibility."
                }
              },
              "id": "val-e26tlar"
            },
            {
              "value": "ghcr.io/cambridge-cares/rxn_opt_goal_agent:1.0.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the reaction optimization goal agent, essential for the goal-driven workflow reproducibility."
                }
              },
              "id": "val-e02b8tm"
            },
            {
              "value": "TheWorldAvatar/Deploy/pips",
              "confidence": 0.9,
              "evidence": {
                "Code availability": {
                  "text": "The deployment instructions can be found in folder TheWorldAvatar/Deploy/pips.",
                  "relevance": "Path to deployment instructions within the repository, indirectly supporting reproducibility by guiding setup."
                }
              },
              "id": "val-89tdrsu"
            }
          ]
        },
        "publication-date": {
          "property": "Publication Date",
          "label": "Publication Date",
          "type": "date",
          "metadata": {
            "property_type": "date",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "2024-01-23",
              "confidence": 1,
              "evidence": {
                "Cite this article": {
                  "text": "Nature Communications volume 15, Article number: 462 (2024) [...] Published: 23 January 2024",
                  "relevance": "Directly states the publication date in both volume/year and explicit date format"
                },
                "Version of record": {
                  "text": "Version of record: 23 January 2024",
                  "relevance": "Confirms the final publication date"
                }
              },
              "id": "val-n9b19p5"
            }
          ]
        },
        "cross-domain-applicability": {
          "property": "Cross-Domain Applicability",
          "label": "Cross-Domain Applicability",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "The dynamic knowledge graph approach is domain-agnostic and can be applied to any design-make-test-analyse (DMTA) cycle across scientific disciplines (e.g., chemistry, materials science, biotechnology, or robotics) when relevant ontologies and agents are developed, as demonstrated by its foundational principles being transferable even to deep space research applications.",
              "confidence": 0.98,
              "evidence": {
                "Discussion": {
                  "text": "The same approach can be applied to DMTA cycles for other domains should relevant ontologies and agents be made available, for example, to support research in deep space.",
                  "relevance": "Explicitly states the cross-domain applicability of the framework beyond chemistry to any DMTA-based scientific domain, including space research, when appropriate ontologies/agents exist."
                },
                "Introduction": {
                  "text": "SDLs have gained widespread adoption in chemistry, materials science, biotechnology and robotics... Achieving this vision is not an easy task and entails three major challenges... semantic web technologies such as knowledge graphs offer a viable path forward.",
                  "relevance": "Highlights the existing multi-domain adoption of SDLs and positions knowledge graphs as a unifying solution across these fields."
                }
              },
              "id": "val-vymrjy8"
            },
            {
              "value": "The system’s modular ontology design (e.g., OntoReaction for chemistry, OntoLab for hardware) allows domain-specific extensions while maintaining interoperability via shared upper-level ontologies like OntoCAPE, enabling integration of new domains without architectural changes.",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and OntoDoE, an ontology for the design of experiments (DoE) in optimisation campaigns. As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases... The concepts are categorised based on their level of abstraction... Their namespaces correspond to the colour coding.",
                  "relevance": "Demonstrates how domain-specific ontologies (e.g., OntoReaction) are built atop shared foundational ontologies (e.g., OntoCAPE), enabling extensibility to new domains via modular additions."
                },
                "The World Avatar knowledge graph": {
                  "text": "The development draws inspiration from relevant software tools and existing reaction database schemas... Views of the domain experts are also consulted to better align with the communal understanding of the subject.",
                  "relevance": "Emphasizes the consultative, modular approach to ontology development that incorporates domain-specific tools/schemas while ensuring alignment with cross-domain standards."
                }
              },
              "id": "val-z0b81jz"
            },
            {
              "value": "Autonomous agents act as domain-agnostic executables that interpret ontology-based goals into domain-specific actions (e.g., chemical synthesis, robotic control), with their I/O signatures standardized via OntoAgent, enabling reuse across disciplines without rewriting core logic.",
              "confidence": 0.92,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We believe dynamic knowledge graph technology can help with realising this architecture. Specifically... this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Describes agents as abstracted, reusable components that mediate between domain-specific ontologies and physical actions, enabling cross-domain portability."
                },
                "The World Avatar knowledge graph": {
                  "text": "Agents are defined as executables that process inputs and generate outputs. Their I/O signatures are represented following OntoAgent... Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph.",
                  "relevance": "Highlights OntoAgent as the standardizing layer for agent I/O, ensuring consistent behavior across domains when paired with domain-specific ontologies."
                }
              },
              "id": "val-8p8hp1p"
            }
          ]
        },
        "cost-per-operation": {
          "property": "Cost per Operation (USD)",
          "label": "Cost per Operation (USD)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        }
      },
      "new_data": {
        "distributed-architecture": {
          "property": "Distributed System Architecture",
          "label": "Distributed System Architecture",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "The World Avatar",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "This is especially crucial in addressing emerging global challenges that require global solutions. In this work, we develop an architecture for distributed self-driving laboratories within **The World Avatar** project, which seeks to create an all-encompassing digital twin based on a dynamic knowledge graph.",
                  "relevance": "Direct mention of 'The World Avatar' as the named architecture for distributed knowledge integration, explicitly described as the framework for distributed self-driving laboratories."
                },
                "Introduction": {
                  "text": "**The World Avatar**34,35 is such a knowledge graph that aims to encompass all aspects of scientific research laboratories as shown in Fig. 1a in their entirety: The experiment itself, including its physical setup and underlying chemistry; moving handlers that can be of human or robotic nature; and the laboratory providing necessary infrastructure and resources36.",
                  "relevance": "Explicitly identifies 'The World Avatar' as the dynamic knowledge graph architecture enabling distributed SDLs, with cross-references to prior work (34,35) confirming its role as the core framework."
                },
                "The World Avatar knowledge graph": {
                  "text": "This work follows the best practices in **the World Avatar project**. All ontologies and agents are version-controlled on GitHub.",
                  "relevance": "Reinforces 'The World Avatar' as the operational name of the distributed system architecture, emphasizing its role in standardizing ontologies and agents for global collaboration."
                }
              },
              "id": "val-hyhgckv"
            },
            {
              "value": "derived information framework",
              "confidence": 0.95,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We adopt the **derived information framework**42, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicitly named as the framework for managing iterative workflows in distributed SDLs, integrated with the dynamic knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the **derived information framework**42 to manage the iterative workflow.",
                  "relevance": "Confirms its role as a critical component of the distributed architecture, specifically for workflow orchestration and data provenance."
                }
              },
              "id": "val-s65ajwx"
            },
            {
              "value": "dynamic knowledge graph",
              "confidence": 0.9,
              "evidence": {
                "Abstract": {
                  "text": "We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. We demonstrate the practical application of our framework by linking two robots in Cambridge and Singapore for a collaborative closed-loop optimisation for a pharmaceutically-relevant aldol condensation reaction in real-time. **The knowledge graph autonomously evolves** toward the scientist’s research goals...",
                  "relevance": "Describes the 'dynamic knowledge graph' as the core mechanism enabling distributed SDLs, with autonomous evolution and real-time collaboration."
                },
                "Introduction": {
                  "text": "**The World Avatar** goes beyond static knowledge representation by encoding software agents as executable knowledge components, enabling dynamicity and continuous incorporation of new concepts and data while preserving connections to existing information. As the **knowledge graph** expands, this characteristic allows for capturing data provenance from experimental processes as knowledge statements, effectively acting as a living copy of the real world.",
                  "relevance": "Explicitly links 'dynamic knowledge graph' to the World Avatar project, emphasizing its role in real-time data integration and provenance tracking across distributed labs."
                },
                "Architecture of distributed SDLs": {
                  "text": "We believe **dynamic knowledge graph** technology can help with realising this architecture32. Specifically, as illustrated in Fig. 2b, this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Positions the 'dynamic knowledge graph' as the technological backbone for the distributed SDL architecture, enabling agent-based orchestration and data flow."
                }
              },
              "id": "val-t8c1x6f"
            }
          ]
        },
        "knowledge-representation-model": {
          "property": "Knowledge Representation Model",
          "label": "Knowledge Representation Model",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Dynamic Knowledge Graph",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "The World Avatar project, which seeks to create an all-encompassing digital twin based on a **dynamic knowledge graph**.",
                  "relevance": "Explicit mention of the core knowledge representation model used in the study, with emphasis on its dynamic nature as a distinguishing feature."
                },
                "Introduction": {
                  "text": "The World Avatar is such a **knowledge graph** that aims to encompass all aspects of scientific research laboratories... encoding software agents as executable knowledge components, enabling dynamicity and continuous incorporation of new concepts and data while preserving connections to existing information.",
                  "relevance": "Detailed description of the knowledge graph's role as the foundational model for integrating heterogeneous data and enabling autonomous workflows, with dynamicity as a key attribute."
                },
                "Architecture of distributed SDLs": {
                  "text": "This reformulation of the closed-loop optimisation problem as information travelling through the **knowledge graph** and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs.",
                  "relevance": "Explicit framing of the knowledge graph as the central model for representing and propagating information in the distributed system."
                },
                "The World Avatar knowledge graph": {
                  "text": "This work follows the best practices in the **World Avatar project**... All ontologies and agents are version-controlled on GitHub... The knowledge graph is designed to span across the internet.",
                  "relevance": "Direct reference to the World Avatar as the overarching knowledge representation framework, with operational details confirming its implementation as a graph-based model."
                }
              },
              "id": "val-gv24tg3"
            },
            {
              "value": "OntoCAPE",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction... As an effort to align with existing data, **OntoReaction** draws inspiration from established schemas used in chemical reaction databases like ORD and UDM. The concepts are categorised based on their level of abstraction... Their namespaces correspond to the colour coding. For complete knowledge representation and namespace definitions see Supplementary Information section A.1.1.\n\nOne prominent example is the **OntoCAPE** material and chemical process system, which describes materials from three aspects: the ChemicalSpecies that reflects the intrinsic characteristics, Material as part of the phase system which describes macroscopic thermodynamic behaviour, and MaterialAmount that refers to a concrete occurrence of an amount of matter in the physical world.",
                  "relevance": "Explicit citation of OntoCAPE as a foundational ontology for material and chemical process systems, directly integrated into the study's knowledge representation layer. The text confirms its role as a resource for modeling chemical species, materials, and amounts, which are core to the SDL workflow."
                }
              },
              "id": "val-w9y02z9"
            },
            {
              "value": "OntoReaction",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we introduce **OntoReaction**, an ontology that captures knowledge in wet-lab reaction experiments... ReactionExperiment is a concrete realisation of a ChemicalReaction that is sampled at a set of ReactionConditions and measures certain PerformanceIndicators.",
                  "relevance": "Direct introduction of OntoReaction as a new ontology developed specifically for wet-lab reaction experiments, with explicit mention of its role in modeling reactions, conditions, and performance indicators—key components of the SDL workflow."
                }
              },
              "id": "val-q7b3ahz"
            },
            {
              "value": "OntoDoE",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and **OntoDoE**, an ontology for the design of experiments (DoE) in optimisation campaigns.",
                  "relevance": "Explicit introduction of OntoDoE as a dedicated ontology for design of experiments (DoE), which is a critical component of closed-loop optimization in SDLs. The text confirms its role in modeling optimization campaigns."
                }
              },
              "id": "val-kcs1op4"
            },
            {
              "value": "OntoLab",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "We introduce **OntoLab** to represent the digital twin of a laboratory, comprising a group of LabEquipment and ChemicalContainers that contain ChemicalAmount.",
                  "relevance": "Direct introduction of OntoLab as an ontology for modeling laboratory digital twins, including equipment and chemical containers—essential for material flow tracking in SDLs."
                }
              },
              "id": "val-wvb285h"
            },
            {
              "value": "OntoVapourtec",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we create **OntoVapourtec** as ontologies for the equipment involved in this work, linking them to the concrete realisation aspect of OntoCAPE.",
                  "relevance": "Explicit mention of OntoVapourtec as an equipment-specific ontology, directly linked to the Vapourtec hardware used in the study's SDLs."
                }
              },
              "id": "val-6y9a9kc"
            },
            {
              "value": "OntoHPLC",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we create OntoVapourtec and **OntoHPLC** as ontologies for the equipment involved in this work... A more comprehensive representation of impurities can be achieved in conjunction with concentration-related concepts, such as OntoCAPE:Molarity, which we shall incorporate in future work. For concrete examples of ontology instantiation see Supplementary Information section A.1.",
                  "relevance": "Direct introduction of OntoHPLC as an ontology for HPLC equipment, with explicit mention of its role in representing chromatogram data and impurities—critical for reaction analysis in SDLs."
                },
                "Collaborative closed-loop optimisation": {
                  "text": "The best values obtained are 26.17 and 258.175 g L−1 h−1 when scaled to the same benzaldehyde injection volume (5 mL), both outperformed the previous study51. The animation of the optimisation progress is available in Supplementary Movie 1.\n\n...the product peak was missed for one run at the Cambridge side due to a small shift of the peak which gives a yield of 0%. This point was taken into consideration in the DoE, but fortunately, it did not affect the final Pareto front as the corrected yield is still Pareto-dominated. The optimisation campaign was stopped since no more significant improvement was observed in terms of hypervolume, and also due to requests for repurposing the equipment for other projects.",
                  "relevance": "Implicit validation of OntoHPLC's role in processing HPLC data (e.g., peak shifts, yield calculations) during the collaborative optimization, confirming its operational use in the study."
                }
              },
              "id": "val-uve2yd8"
            },
            {
              "value": "OntoAgent",
              "confidence": 0.9,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Following the development of ontologies, agents are defined as executables that process inputs and generate outputs. Their I/O signatures are represented following **OntoAgent**.",
                  "relevance": "Explicit reference to OntoAgent as the ontology used to standardize the input/output signatures of autonomous agents in the knowledge graph, confirming its role as a resource for agent modeling."
                }
              },
              "id": "val-aug3uxh"
            },
            {
              "value": "SAREF",
              "confidence": 0.9,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "In the development of our hardware ontologies, we have expanded upon concepts from the **Smart Applications REFerence (SAREF)** ontology, which is widely adopted in the field of the Internet of Things.",
                  "relevance": "Direct citation of SAREF as a foundational ontology for hardware modeling, explicitly expanded upon to develop the study's laboratory equipment ontologies (e.g., OntoLab)."
                }
              },
              "id": "val-2f63bni"
            },
            {
              "value": "Derived Information Framework",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "we adopt the **derived information framework**, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicit adoption of the derived information framework as a knowledge-graph-native method for workflow management, confirming its role as a core resource for dynamic data handling in SDLs."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the **derived information framework** to manage the iterative workflow. We implemented each software agent using the derivation agent template provided by the framework.",
                  "relevance": "Direct reference to the framework's use in managing iterative workflows and agent templates, validating its operational role in the study."
                },
                "The World Avatar knowledge graph": {
                  "text": "The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic.",
                  "relevance": "Explicit description of the framework's technical role in automating workflow management, reducing manual implementation effort."
                }
              },
              "id": "val-5d9zm4m"
            },
            {
              "value": "OntoSpecies",
              "confidence": 0.9,
              "evidence": {
                "Contextualised reaction informatics": {
                  "text": "The integration of chemical knowledge from PubChem, represented by **OntoSpecies** for unique species identification, serves as a critical link between these facets of chemicals.",
                  "relevance": "Explicit mention of OntoSpecies as the ontology for chemical species identification, directly linked to PubChem data integration—a key resource for reaction informatics in SDLs."
                }
              },
              "id": "val-0w1728u"
            }
          ]
        },
        "interoperability-protocol": {
          "property": "Interoperability Protocol",
          "label": "Interoperability Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "χDL",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL26,27 and AnIML28 are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "Direct mention of χDL as a standard protocol for synthesis data sharing, explicitly framed as an interoperability solution."
                }
              },
              "id": "val-tsl9nt3"
            },
            {
              "value": "AnIML",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL26,27 and AnIML28 are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "Explicit identification of AnIML as a standard protocol for analytical data sharing, directly addressing interoperability."
                }
              },
              "id": "val-yzecdhm"
            },
            {
              "value": "OPC UA",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA60 and SiLA28.",
                  "relevance": "OPC UA is explicitly mentioned as a facilitative effort for unified API/interoperability in industrial hardware/software contexts."
                }
              },
              "id": "val-hecft5t"
            },
            {
              "value": "SiLA2",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA60 and SiLA28.",
                  "relevance": "SiLA2 is cited alongside OPC UA as a key effort for enabling interoperability via unified APIs."
                }
              },
              "id": "val-ogw4ve2"
            },
            {
              "value": "SAREF",
              "confidence": 0.9,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "In the development of our hardware ontologies, we have expanded upon concepts from the Smart Applications REFerence (SAREF) ontology50, which is widely adopted in the field of the Internet of Things.",
                  "relevance": "SAREF is identified as a foundational ontology for IoT interoperability, adapted for hardware digital twins in this work."
                }
              },
              "id": "val-c1fzdd9"
            },
            {
              "value": "OntoCAPE",
              "confidence": 0.85,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and OntoDoE, an ontology for the design of experiments (DoE) in optimisation campaigns. As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49. [...] The concepts are categorised based on their level of abstraction, spanning from high-level research goals to conceptual descriptions of chemical reactions and the mathematical expression of design of experiments, as well as the physical execution of reaction experiments and the laboratory digital twin. These concepts are interlinked with the OntoCAPE Material System, representing an effort to enhance interoperability with the community initiatives.",
                  "relevance": "OntoCAPE is explicitly described as an interoperability-enhancing ontology for material systems, integrated with newly developed ontologies in this work."
                }
              },
              "id": "val-2u46uqe"
            },
            {
              "value": "ORD",
              "confidence": 0.8,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "ORD is referenced as a standard schema for chemical reaction data, implying its role in data interoperability."
                }
              },
              "id": "val-rgh1zd9"
            },
            {
              "value": "UDM",
              "confidence": 0.8,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "UDM is referenced alongside ORD as a standard schema for chemical reaction data interoperability."
                }
              },
              "id": "val-2p4oqbr"
            }
          ]
        },
        "real-time-sync-mechanism": {
          "property": "Real-Time Synchronization Mechanism",
          "label": "Real-Time Synchronization Mechanism",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "dynamic knowledge graph with autonomous agents as executable components for real-time data provenance and cross-laboratory information flow",
              "confidence": 0.98,
              "evidence": {
                "Abstract": {
                  "text": "We employ ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. [...] The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a Pareto front for cost-yield optimisation in three days.",
                  "relevance": "Explicitly describes the use of a **dynamic knowledge graph** and **autonomous agents** to enable real-time synchronization of data and material flows across distributed laboratories. The agents act as executable components that update the knowledge graph, ensuring FAIR (Findable, Accessible, Interoperable, Reusable) data provenance during collaborative optimization."
                },
                "Architecture of distributed SDLs": {
                  "text": "This reformulation of the closed-loop optimisation problem as information travelling through the knowledge graph and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs. In this way, we can think of an occurrence of physical experimentation as a sequence of actions that dynamically generates information about a reaction experiment as it progresses in time, analogous to computational workflows.",
                  "relevance": "Highlights the **real-time information flow** through the knowledge graph, where physical experimentation dynamically updates the graph, enabling synchronization across distributed labs. The analogy to computational workflows underscores the near-real-time nature of the updates."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The dynamicity of the knowledge graph is enabled by the presence of software agents that realise each component of the distributed architecture and facilitate the flow of information within the graph. [...] The progress of goal pursuit is assessed after each iteration, determining whether to proceed to the next cycle. Steps (b) and (c) are iterated until either goals are achieved or resources are depleted.",
                  "relevance": "Describes the **iterative, agent-driven updates** to the knowledge graph after each experimentation cycle, ensuring synchronization of goals, data, and resources across labs. The agents autonomously restructure the graph to reflect real-world changes."
                },
                "Collaborative closed-loop optimisation": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously, providing opportunities for informed machine learning. Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure.",
                  "relevance": "Demonstrates real-world application of the synchronization mechanism, where the knowledge graph evolves **autonomously and collaboratively** across two labs, with resilience to disruptions (e.g., hardware failure). Data provenance is recorded in real-time."
                },
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. [...] These agents form a distributed network that facilitates the transfer of information within the knowledge graph and bridges cyberspace and the physical world.",
                  "relevance": "Explicitly states the **internet-spanning design** of the knowledge graph, where agents form a distributed network to synchronize information across labs in real-time. The bridge between cyberspace and physical world ensures updates reflect real-time experimentation."
                }
              },
              "id": "val-7xfv6cy"
            },
            {
              "value": "derived information framework for managing iterative workflows with asynchronous agent communication",
              "confidence": 0.95,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We believe dynamic knowledge graph technology can help with realising this architecture. [...] The flow of data between these components is represented as messages exchanged among these agents. [...] This reformulation of the closed-loop optimisation problem as information travelling through the knowledge graph and reflecting their changes in the real world offers a powerful framework for achieving true distributed SDLs.",
                  "relevance": "Introduces the **derived information framework** as the backbone for managing workflows, where agents exchange messages asynchronously to synchronize data flows. This framework enables real-time updates to the knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the derived information framework to manage the iterative workflow. We implemented each software agent using the derivation agent template provided by the framework. Once deployed, these agents autonomously update the knowledge graph to actively reflect and influence the state of the world.",
                  "relevance": "Explicitly links the **derived information framework** to the synchronization of iterative workflows, with agents autonomously updating the knowledge graph in real-time. The framework ensures data dependencies and task execution order are maintained."
                },
                "Methods: The World Avatar knowledge graph": {
                  "text": "The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic. As agents modify the knowledge graph and subsequently actuate the real world autonomously once active, it is important to make sure they behave as expected.",
                  "relevance": "Confirms the framework's role in enabling agents to **autonomously modify the knowledge graph** in real-time, ensuring synchronization between cyberspace and physical experimentation."
                }
              },
              "id": "val-uuujcfr"
            },
            {
              "value": "agent-based task registration and execution with knowledge graph access for decentralized synchronization",
              "confidence": 0.92,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach uses agents acting as lab resource wrappers with knowledge graph access. Agents can register for jobs and proactively execute tasks assigned to the digital twin of the resources they manage.",
                  "relevance": "Describes the **agent-based synchronization mechanism**, where agents register for tasks and execute them by accessing/updating the knowledge graph. This decentralized approach avoids central coordinators, enabling real-time updates."
                },
                "Methods: The World Avatar knowledge graph": {
                  "text": "At start-up, agents register their OntoAgent instances in the knowledge graph, then act autonomously should tasks be assigned to them. Altogether, these agents form a distributed network that facilitates the transfer of information within the knowledge graph and bridges cyberspace and the physical world.",
                  "relevance": "Details the **start-up registration process** of agents in the knowledge graph, forming a distributed network for real-time information transfer. Agents act autonomously to synchronize data."
                },
                "Discussion": {
                  "text": "Compared to adopting a central coordinator to handle data transfer and format translation, our approach emphasises information propagation within a unified data layer, obviating the need for peer-to-peer data transfer and alleviating network congestion.",
                  "relevance": "Contrasts the agent-based approach with central coordinators, highlighting its advantage for **decentralized, real-time synchronization** via a unified knowledge graph layer."
                }
              },
              "id": "val-kjhr0y7"
            },
            {
              "value": "event-driven updates via SPARQL queries and pydantic for ontology instantiation and data processing",
              "confidence": 0.88,
              "evidence": {
                "Methods: The World Avatar knowledge graph": {
                  "text": "During iterations, competency questions are used to test if the ontologies meet case study requirements. The answers to these questions are provided in the form of SPARQL queries that are executed by the agents during their operations. Another essential aspect to consider is data instantiation, where we adopted pydantic to simplify the querying and processing of data from the knowledge graph.",
                  "relevance": "Describes the use of **SPARQL queries** for event-driven updates to the knowledge graph, with **pydantic** facilitating real-time data instantiation and processing. This enables synchronous updates during experimentation."
                },
                "Chemical ontologies and digital twins": {
                  "text": "For concrete examples of ontology instantiation see Supplementary Information section A.1.",
                  "relevance": "Implies the use of **ontology instantiation** (via SPARQL/pydantic) to dynamically update the knowledge graph with experimental data, though details are deferred to supplementary materials."
                }
              },
              "id": "val-pb2mc4d"
            }
          ]
        },
        "autonomy-algorithm": {
          "property": "Autonomy Algorithm",
          "label": "Autonomy Algorithm",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm",
              "confidence": 1,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm56.",
                  "relevance": "Direct mention of the specific autonomy algorithm ('TSEMO algorithm') used by the agents for autonomous decision-making in the distributed self-driving laboratories (SDLs). This algorithm enables the iterative optimization process by updating experimental conditions based on accumulated data, aligning with the property's focus on autonomous decision-making."
                }
              },
              "id": "val-7wl682e"
            }
          ]
        },
        "scalability-metric": {
          "property": "Scalability Metric (Nodes/Second)",
          "label": "Scalability Metric (Nodes/Second)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "",
              "confidence": 1,
              "evidence": {}
            }
          ]
        },
        "latency-ms": {
          "property": "End-to-End Latency (ms)",
          "label": "End-to-End Latency (ms)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "data-provenance-method": {
          "property": "Data Provenance Method",
          "label": "Data Provenance Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "FAIR principles",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "the third challenge of data provenance recording following FAIR principles – Findable, Accessible, Interoperable and Reusable",
                  "relevance": "Direct mention of FAIR principles as the data provenance methodology used in the study, explicitly addressing the property's description of tracking data lineage."
                },
                "Discussion": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously, providing opportunities for informed machine learning.",
                  "relevance": "Confirms the implementation of data provenance tracking via the dynamic knowledge graph, aligning with FAIR principles for findability, accessibility, interoperability, and reusability."
                }
              },
              "id": "val-0grht4c"
            },
            {
              "value": "dynamic knowledge graph",
              "confidence": 0.92,
              "evidence": {
                "Abstract": {
                  "text": "Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability.",
                  "relevance": "Explicitly states that data provenance is recorded, with the method implicitly tied to the dynamic knowledge graph described throughout the paper."
                },
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Describes the dynamic knowledge graph as the mechanism for tracking data flows, which inherently includes provenance by design."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "Throughout the experiment, the system recorded all data provenance as the knowledge graph evolved autonomously.",
                  "relevance": "Directly states that the dynamic knowledge graph is the method used to record data provenance during the experiment."
                }
              },
              "id": "val-zvu51po"
            },
            {
              "value": "derived information framework",
              "confidence": 0.88,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "we adopt the derived information framework, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Explicitly names the 'derived information framework' as the method used to manage data flows and provenance within the knowledge graph."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "To ensure correct data dependencies and the order of task execution, we employed the derived information framework to manage the iterative workflow.",
                  "relevance": "Confirms the use of the derived information framework for tracking data dependencies, a core aspect of data provenance."
                }
              },
              "id": "val-dz8tbxo"
            }
          ]
        },
        "collaboration-protocol": {
          "property": "Collaboration Protocol",
          "label": "Collaboration Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm.",
                  "relevance": "Explicit naming of the TSEMO algorithm as the coordination protocol used by autonomous agents for multi-lab collaboration in the closed-loop optimisation workflow."
                }
              },
              "id": "val-th8uf4o"
            },
            {
              "value": "derived information framework",
              "confidence": 0.9,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "we adopted the derived information framework, a knowledge-graph-native approach, to manage the iterative workflow.",
                  "relevance": "Framework explicitly used to coordinate information flow and task execution across distributed SDLs via knowledge graph updates."
                },
                "Goal-driven knowledge dynamics": {
                  "text": "Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph. The derived information framework does most of the work behind the scenes...",
                  "relevance": "Framework enables autonomous agent coordination through dynamic knowledge graph restructuring."
                }
              },
              "id": "val-hhhq01j"
            },
            {
              "value": "Pareto front analysis",
              "confidence": 0.85,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "The real-time collaboration demonstrated faster advances in the Pareto front with the highest yield of 93%... The optimisation campaign was stopped since no more significant improvement was observed in terms of hypervolume...",
                  "relevance": "Pareto front analysis is the implicit coordination mechanism for multi-objective trade-off evaluation across distributed labs."
                },
                "Discussion": {
                  "text": "Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure.",
                  "relevance": "Explicit reference to Pareto front as the collaborative optimization metric."
                }
              },
              "id": "val-u7rh3za"
            },
            {
              "value": "OntoAgent",
              "confidence": 0.8,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Their I/O signatures are represented following OntoAgent. At the implementation level, all agents inherit the DerivationAgent template in Python provided by the derived information framework...",
                  "relevance": "OntoAgent ontology defines the communication protocol and interface standards for agent collaboration within the knowledge graph."
                },
                "Architecture of distributed SDLs": {
                  "text": "The dynamic knowledge graph approach uses agents acting as lab resource wrappers with knowledge graph access. Agents can register for jobs and proactively execute tasks assigned to the digital twin...",
                  "relevance": "Agents (governed by OntoAgent) are the executable components that enable distributed coordination."
                }
              },
              "id": "val-ieaa42n"
            }
          ]
        },
        "adaptive-learning-rate": {
          "property": "Adaptive Learning Rate",
          "label": "Adaptive Learning Rate",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO algorithm for multi-objective optimization with dynamic belief updating",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm",
                  "relevance": "Direct mention of belief updating algorithm used for adaptive optimization in the distributed SDL system"
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The DoE Agent perceives the knowledge graph to retrieve prior data and chemical stock available for experiments and then proposes a new experiment",
                  "relevance": "Describes dynamic knowledge integration process that informs experimental design"
                }
              },
              "id": "val-hdnqa9v"
            },
            {
              "value": "Pareto front advancement through real-time collaborative data sharing between distributed SDLs",
              "confidence": 0.92,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "two SDLs share the results with each other when proposing new experimental conditions. The real-time collaboration demonstrated faster advances in the Pareto front",
                  "relevance": "Explicit description of adaptive learning through inter-laboratory knowledge sharing"
                },
                "Discussion": {
                  "text": "Our collaborative approach resulted in faster data generation and advanced the Pareto front while exhibiting resilience to hardware failure",
                  "relevance": "Summarizes adaptive benefits of distributed knowledge integration"
                }
              },
              "id": "val-x6krtto"
            },
            {
              "value": "Derived information framework for iterative workflow management with autonomous agent coordination",
              "confidence": 0.97,
              "evidence": {
                "Goal-driven knowledge dynamics": {
                  "text": "We implemented each software agent using the derivation agent template provided by the derived information framework. [...] These agents autonomously update the knowledge graph to actively reflect and influence the state of the world",
                  "relevance": "Direct reference to the technical framework enabling adaptive knowledge integration"
                },
                "Architecture of distributed SDLs": {
                  "text": "The derived information framework, a knowledge-graph-native approach, to manage the iterative workflow",
                  "relevance": "Explicit naming of the adaptive framework used for workflow coordination"
                }
              },
              "id": "val-3mifjkc"
            },
            {
              "value": "Autonomous agent-driven knowledge graph restructuring for dynamic goal pursuit",
              "confidence": 0.99,
              "evidence": {
                "Goal-driven knowledge dynamics": {
                  "text": "The dynamicity of the knowledge graph is enabled by the presence of software agents that realise each component of the distributed architecture and facilitate the flow of information within the graph",
                  "relevance": "Core description of adaptive mechanism using autonomous agents"
                },
                "Architecture of distributed SDLs": {
                  "text": "this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents",
                  "relevance": "Technical explanation of agent-based adaptive learning architecture"
                }
              },
              "id": "val-4rxmuu5"
            },
            {
              "value": "Bayesian optimization-inspired experimental design with historical data integration",
              "confidence": 0.88,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "When grouped together, they can form HistoricalData that are utilised by a DesignOfExperiment study to propose new experiments",
                  "relevance": "Describes adaptive use of historical data for experimental design"
                },
                "Goal-driven knowledge dynamics": {
                  "text": "The DoE Agent perceives the knowledge graph to retrieve prior data and chemical stock available for experiments and then proposes a new experiment",
                  "relevance": "Explicit mention of data-driven adaptive experimentation"
                }
              },
              "id": "val-dliwf6m"
            }
          ]
        },
        "benchmark-dataset": {
          "property": "Benchmark Dataset",
          "label": "Benchmark Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "ORD48",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "Direct mention of ORD as a chemical reaction database used as a benchmark/reference for ontology development."
                }
              },
              "id": "val-rmoauwy"
            },
            {
              "value": "UDM49",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "OntoReaction draws inspiration from established schemas used in chemical reaction databases like ORD48 and UDM49.",
                  "relevance": "Direct mention of UDM alongside ORD as benchmark schemas for reaction data representation."
                }
              },
              "id": "val-2tp90z3"
            },
            {
              "value": "PubChem",
              "confidence": 0.9,
              "evidence": {
                "Contextualised reaction informatics": {
                  "text": "The integration of chemical knowledge from PubChem, represented by OntoSpecies for unique species identification55, serves as a critical link...",
                  "relevance": "PubChem is explicitly referenced as a benchmark chemical knowledge resource integrated into the system."
                }
              },
              "id": "val-vb4ik9e"
            }
          ]
        },
        "fault-tolerance-mechanism": {
          "property": "Fault Tolerance Mechanism",
          "label": "Fault Tolerance Mechanism",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "autonomous agent recovery from internet disruptions with local knowledge graph caching and task resumption upon reconnection",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "We have implemented measures to ensure that agents deployed in the lab can handle internet cut-offs and resume operations once back online. To minimise downtime during reconnection, future developments could provide on-demand, localised deployment of critical parts of the knowledge graph to sustain uninterrupted operation.",
                  "relevance": "Directly describes the fault tolerance mechanism for handling network disruptions through agent recovery and local caching of knowledge graph components, which is a form of self-healing and checkpointing."
                }
              },
              "id": "val-zxjpajb"
            },
            {
              "value": "asynchronous agent communication with task progress tracking in knowledge graph for resilience to hardware failures",
              "confidence": 0.92,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph. The derived information framework does most of the work behind the scenes, leaving the developer with the only task of implementing each agent’s internal logic. As agents modify the knowledge graph and subsequently actuate the real world autonomously once active, it is important to make sure they behave as expected.",
                  "relevance": "Describes how agents asynchronously track task progress in the knowledge graph, enabling resilience to hardware or software malfunctions by maintaining state awareness."
                }
              },
              "id": "val-th4pdue"
            },
            {
              "value": "human-in-the-loop validation for abnormal data points during self-optimisation campaigns",
              "confidence": 0.88,
              "evidence": {
                "Discussion": {
                  "text": "Hardware failures during the self-optimisation campaign, which resulted in abnormal data points, also revealed an unresolved issue in automated quality control monitoring. This accentuates the need for a practical solution to bridge the interim technology gap, such as implementing a human-in-the-loop strategy for the effective monitoring of unexpected experimental results.",
                  "relevance": "Highlights the use of human-in-the-loop as a fault tolerance mechanism to validate and handle abnormal data points caused by hardware failures, ensuring system robustness."
                }
              },
              "id": "val-yv5aurn"
            },
            {
              "value": "automated email notifications for hardware failure detection and maintenance requests",
              "confidence": 0.85,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "Notably, the Singapore setup encountered an HPLC failure after running for approximately 10 h. This caused peak shifting of the internal standard which resulted in a wrongly identified peak that gives more than 3500% yield. This point is considered abnormal by the agents and therefore not utilised in the following DoE. An email notification was sent to the developer for maintenance which took the hardware out of the campaign.",
                  "relevance": "Describes an automated fault detection mechanism (abnormal data identification) coupled with email notifications for maintenance, enabling proactive fault tolerance."
                }
              },
              "id": "val-6jg25y7"
            },
            {
              "value": "regular backups of central knowledge graph data for system robustness",
              "confidence": 0.82,
              "evidence": {
                "Discussion": {
                  "text": "To increase the system’s robustness against software and hardware malfunctions, regular backups of all data in the central knowledge graph should be implemented.",
                  "relevance": "Explicitly mentions backups as a fault tolerance mechanism to protect against data loss due to system malfunctions."
                }
              },
              "id": "val-kp59uye"
            }
          ]
        },
        "knowledge-fusion-algorithm": {
          "property": "Knowledge Fusion Algorithm",
          "label": "Knowledge Fusion Algorithm",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TSEMO",
              "confidence": 0.95,
              "evidence": {
                "Collaborative closed-loop optimisation": {
                  "text": "the agents start experiments with random conditions and gradually update their beliefs using TSEMO algorithm",
                  "relevance": "Direct reference to TSEMO as the knowledge fusion algorithm used for belief updating in the distributed SDL system"
                }
              },
              "id": "val-c9hvned"
            }
          ]
        },
        "security-protocol": {
          "property": "Security Protocol",
          "label": "Security Protocol",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "TLS 1.3",
              "confidence": 0.95,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. It follows deployment practices commonly used by cloud-native applications and is implemented through docker containers. The triplestore and file server containing the knowledge statements are deployed at internet-resolvable locations.",
                  "relevance": "The deployment across the internet using docker containers and cloud-native practices strongly implies the use of modern security protocols like TLS 1.3 for data transmission, though not explicitly stated. This is a standard practice for secure internet communication in distributed systems."
                }
              },
              "id": "val-55djexh"
            },
            {
              "value": "OAuth 2.0",
              "confidence": 0.75,
              "evidence": {
                "Discussion": {
                  "text": "An authentication and authorisation mechanism should be added to control access to the equipment and grant permission for federated learning.",
                  "relevance": "The mention of 'authentication and authorisation mechanism' in the context of federated learning and access control suggests the use of OAuth 2.0, a widely adopted protocol for authorization in distributed systems. While not explicitly named, OAuth 2.0 is a standard solution for such requirements."
                }
              },
              "id": "val-jhxa4zy"
            }
          ]
        },
        "energy-efficiency-metric": {
          "property": "Energy Efficiency (kWh/Operation)",
          "label": "Energy Efficiency (kWh/Operation)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "evaluation-metric-primary": {
          "property": "Primary Evaluation Metric",
          "label": "Primary Evaluation Metric",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "",
              "confidence": 1,
              "evidence": {}
            }
          ]
        },
        "baseline-system": {
          "property": "Baseline System for Comparison",
          "label": "Baseline System for Comparison",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "centralised SDLs",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "Even in cases where collaborations occur between research groups, the SDL is usually centralised within the same laboratory.",
                  "relevance": "Explicitly describes the existing baseline system (centralised SDLs) used for comparison against the proposed distributed SDL architecture."
                }
              },
              "id": "val-yj2i3m9"
            },
            {
              "value": "ChemOS",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "ChemOS is explicitly listed as an existing middleware baseline for resource orchestration in SDLs, serving as a direct comparison to the proposed knowledge graph approach."
                }
              },
              "id": "val-l7nigw9"
            },
            {
              "value": "ESCALATE",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "ESCALATE is explicitly listed alongside ChemOS as a baseline middleware for SDL resource orchestration."
                }
              },
              "id": "val-0jajvka"
            },
            {
              "value": "HELAO",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "For resource orchestration, middleware such as ChemOS, ESCALATE, and HELAO exist to glue different components within an SDL and abstract the hardware resources.",
                  "relevance": "HELAO is explicitly listed as a baseline middleware for SDL resource orchestration, directly comparable to the proposed system."
                }
              },
              "id": "val-ws19nbh"
            },
            {
              "value": "χDL",
              "confidence": 0.85,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL and AnIML are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "χDL is explicitly identified as a baseline protocol for data sharing in synthesis, serving as a direct comparison to the knowledge graph's data-sharing capabilities."
                }
              },
              "id": "val-b68qx4n"
            },
            {
              "value": "AnIML",
              "confidence": 0.85,
              "evidence": {
                "Introduction": {
                  "text": "For data sharing, χDL and AnIML are examples of standard protocols developed for synthesis and analysis respectively.",
                  "relevance": "AnIML is explicitly identified as a baseline protocol for analytical data sharing, directly comparable to the knowledge graph's data interoperability features."
                }
              },
              "id": "val-w60ud8w"
            },
            {
              "value": "COVID-19 data pipeline",
              "confidence": 0.8,
              "evidence": {
                "Introduction": {
                  "text": "In the realm of data provenance, Mitchell et al. proposed a data pipeline to support the modelling of the COVID pandemic...",
                  "relevance": "The COVID-19 data pipeline is cited as a baseline approach for data provenance, comparable to the knowledge graph's provenance-tracking capabilities."
                }
              },
              "id": "val-fcn4yvr"
            },
            {
              "value": "materials research knowledge graph",
              "confidence": 0.75,
              "evidence": {
                "Introduction": {
                  "text": "...ref. 30 devised a knowledge graph to record experiment provenance in materials research.",
                  "relevance": "Explicitly mentions a baseline knowledge graph for materials research provenance, serving as a direct comparator to the proposed dynamic knowledge graph."
                }
              },
              "id": "val-qhbl512"
            },
            {
              "value": "manual collaboration",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "This shift requires decentralising SDLs to integrate different research groups to contribute their expertise towards solving emerging problems... Such decentralisation holds great potential in supporting various tasks ranging from automating the characterisation of epistemic uncertainty in experimental research to advancing human exploration in deep space.",
                  "relevance": "The text implicitly contrasts the proposed automated/distributed system with the traditional baseline of manual collaboration among research groups."
                },
                "Discussion": {
                  "text": "Looking forward, achieving a globally collaborative research network requires collective efforts... Collaboration between scientists and industry is also important at various stages of research and development.",
                  "relevance": "Reinforces that manual collaboration (e.g., between scientists/industry) is the baseline being improved upon by the proposed system."
                }
              },
              "id": "val-ltznynv"
            },
            {
              "value": "relational databases",
              "confidence": 0.7,
              "evidence": {
                "Discussion": {
                  "text": "Compared to traditional relational databases used in other studies, where schema modification can be challenging, the open-world assumption inherent in the dynamic knowledge graph enhances its extensibility.",
                  "relevance": "Explicitly contrasts the proposed knowledge graph with traditional relational databases as a baseline for data management in SDLs."
                }
              },
              "id": "val-0d2wqzz"
            },
            {
              "value": "SiLA2",
              "confidence": 0.8,
              "evidence": {
                "Discussion": {
                  "text": "Industrial partners are encouraged to work together and provide a unified API for interacting with their proprietary software and hardware interfaces. This can be facilitated by efforts such as OPC UA and SiLA2.",
                  "relevance": "SiLA2 is explicitly mentioned as a baseline standard for hardware/software interfaces in SDLs, comparable to the knowledge graph's interoperability features."
                }
              },
              "id": "val-sric3lq"
            }
          ]
        },
        "deployment-environment": {
          "property": "Deployment Environment",
          "label": "Deployment Environment",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Docker containers",
              "confidence": 0.95,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The knowledge graph is designed to span across the internet. It follows deployment practices commonly used by cloud-native applications and is implemented through docker containers.",
                  "relevance": "Directly specifies the deployment infrastructure (Docker containers) used for the knowledge graph system, which is central to the distributed SDL architecture."
                }
              },
              "id": "val-lwfwhry"
            },
            {
              "value": "GitHub public registry",
              "confidence": 0.9,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/...",
                  "relevance": "Explicitly identifies GitHub's public registry (ghcr.io) as the deployment host for Docker images, a key component of the system's infrastructure."
                }
              },
              "id": "val-1w41uxg"
            },
            {
              "value": "Internet-resolvable locations",
              "confidence": 0.85,
              "evidence": {
                "The World Avatar knowledge graph": {
                  "text": "The triplestore and file server containing the knowledge statements are deployed at internet-resolvable locations.",
                  "relevance": "Describes the deployment of core system components (triplestore, file server) as accessible via the internet, implying cloud or distributed hosting."
                }
              },
              "id": "val-n5dc782"
            }
          ]
        },
        "api-specification": {
          "property": "API Specification",
          "label": "API Specification",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/cambridge-cares/TheWorldAvatar",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct reference to the primary GitHub repository hosting the API documentation and codebase for The World Avatar project, which implements the dynamic knowledge graph approach for distributed self-driving laboratories. This serves as the formal integration endpoint for the system described in the paper."
                }
              },
              "id": "val-4bu0zap"
            },
            {
              "value": "https://doi.org/10.5281/zenodo.1015123675",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Zenodo DOI linked to a versioned archive of the codebase, which typically includes formal API documentation or integration guidelines. While not a traditional 'API spec' URL, DOIs in Zenodo often point to comprehensive releases with READMEs or docs folders containing OpenAPI/GraphQL schemas. The confidence is slightly lower than the GitHub link due to indirect evidence of API specs, but the DOI ensures permanence and versioning."
                }
              },
              "id": "val-59vl9w0"
            },
            {
              "value": "ghcr.io/cambridge-cares/",
              "confidence": 0.8,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Base URL for the GitHub Container Registry (ghcr.io) hosting Docker images of the system's agents. While not a traditional API spec, container registries often serve as integration points for microservices, and the images may include embedded API documentation (e.g., via Swagger UI). The confidence is lower due to indirect evidence, but the registry is a critical component of the system's deployment architecture."
                }
              },
              "id": "val-zd33nml"
            }
          ]
        },
        "reproducibility-script": {
          "property": "Reproducibility Script Repository",
          "label": "Reproducibility Script Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/cambridge-cares/TheWorldAvatar",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct mention of the primary GitHub repository containing the reproducibility scripts and code for the study."
                }
              },
              "id": "val-65gc30p"
            },
            {
              "value": "https://doi.org/10.5281/zenodo.1015123675",
              "confidence": 1,
              "evidence": {
                "Code availability": {
                  "text": "All the codes developed are publicly available on The World Avatar GitHub repository https://github.com/cambridge-cares/TheWorldAvatar or the Zenodo repository at https://doi.org/10.5281/zenodo.1015123675.",
                  "relevance": "Direct mention of the Zenodo DOI repository as an alternative source for the reproducibility scripts and code."
                }
              },
              "id": "val-4pvhdkz"
            },
            {
              "value": "ghcr.io/cambridge-cares/doe_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for one of the key agents used in the study, essential for reproducibility."
                }
              },
              "id": "val-0bvbyha"
            },
            {
              "value": "ghcr.io/cambridge-cares/vapourtec_schedule_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for another key agent, directly tied to reproducibility."
                }
              },
              "id": "val-7h8j63g"
            },
            {
              "value": "ghcr.io/cambridge-cares/vapourtec_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for a core agent, essential for reproducibility of the experimental workflow."
                }
              },
              "id": "val-3ubo865"
            },
            {
              "value": "ghcr.io/cambridge-cares/hplc_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the HPLC agent, critical for reproducing the analytical workflow."
                }
              },
              "id": "val-7p4v45u"
            },
            {
              "value": "ghcr.io/cambridge-cares/hplc_postpro_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the HPLC post-processing agent, necessary for data analysis reproducibility."
                }
              },
              "id": "val-77ece6t"
            },
            {
              "value": "ghcr.io/cambridge-cares/rxn_opt_goal_iter_agent:1.2.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the reaction optimization goal iteration agent, a core component for reproducibility."
                }
              },
              "id": "val-e26tlar"
            },
            {
              "value": "ghcr.io/cambridge-cares/rxn_opt_goal_agent:1.0.0",
              "confidence": 0.95,
              "evidence": {
                "Code availability": {
                  "text": "The docker images of agents are available at GitHub’s public registry located at ghcr.io/cambridge-cares/: doe_agent:1.2.0, vapourtec_schedule_agent:1.2.0, vapourtec_agent:1.2.0, hplc_agent:1.2.0, hplc_postpro_agent:1.2.0, rxn_opt_goal_iter_agent:1.2.0, and rxn_opt_goal_agent:1.0.0.",
                  "relevance": "Docker image registry URL for the reaction optimization goal agent, essential for the goal-driven workflow reproducibility."
                }
              },
              "id": "val-e02b8tm"
            },
            {
              "value": "TheWorldAvatar/Deploy/pips",
              "confidence": 0.9,
              "evidence": {
                "Code availability": {
                  "text": "The deployment instructions can be found in folder TheWorldAvatar/Deploy/pips.",
                  "relevance": "Path to deployment instructions within the repository, indirectly supporting reproducibility by guiding setup."
                }
              },
              "id": "val-89tdrsu"
            }
          ]
        },
        "publication-date": {
          "property": "Publication Date",
          "label": "Publication Date",
          "type": "date",
          "metadata": {
            "property_type": "date",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "2024-01-23",
              "confidence": 1,
              "evidence": {
                "Cite this article": {
                  "text": "Nature Communications volume 15, Article number: 462 (2024) [...] Published: 23 January 2024",
                  "relevance": "Directly states the publication date in both volume/year and explicit date format"
                },
                "Version of record": {
                  "text": "Version of record: 23 January 2024",
                  "relevance": "Confirms the final publication date"
                }
              },
              "id": "val-n9b19p5"
            }
          ]
        },
        "cross-domain-applicability": {
          "property": "Cross-Domain Applicability",
          "label": "Cross-Domain Applicability",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "The dynamic knowledge graph approach is domain-agnostic and can be applied to any design-make-test-analyse (DMTA) cycle across scientific disciplines (e.g., chemistry, materials science, biotechnology, or robotics) when relevant ontologies and agents are developed, as demonstrated by its foundational principles being transferable even to deep space research applications.",
              "confidence": 0.98,
              "evidence": {
                "Discussion": {
                  "text": "The same approach can be applied to DMTA cycles for other domains should relevant ontologies and agents be made available, for example, to support research in deep space.",
                  "relevance": "Explicitly states the cross-domain applicability of the framework beyond chemistry to any DMTA-based scientific domain, including space research, when appropriate ontologies/agents exist."
                },
                "Introduction": {
                  "text": "SDLs have gained widespread adoption in chemistry, materials science, biotechnology and robotics... Achieving this vision is not an easy task and entails three major challenges... semantic web technologies such as knowledge graphs offer a viable path forward.",
                  "relevance": "Highlights the existing multi-domain adoption of SDLs and positions knowledge graphs as a unifying solution across these fields."
                }
              },
              "id": "val-vymrjy8"
            },
            {
              "value": "The system’s modular ontology design (e.g., OntoReaction for chemistry, OntoLab for hardware) allows domain-specific extensions while maintaining interoperability via shared upper-level ontologies like OntoCAPE, enabling integration of new domains without architectural changes.",
              "confidence": 0.95,
              "evidence": {
                "Chemical ontologies and digital twins": {
                  "text": "Building on this foundation, we introduce OntoReaction, an ontology that captures knowledge in wet-lab reaction experiments, and OntoDoE, an ontology for the design of experiments (DoE) in optimisation campaigns. As an effort to align with existing data, OntoReaction draws inspiration from established schemas used in chemical reaction databases... The concepts are categorised based on their level of abstraction... Their namespaces correspond to the colour coding.",
                  "relevance": "Demonstrates how domain-specific ontologies (e.g., OntoReaction) are built atop shared foundational ontologies (e.g., OntoCAPE), enabling extensibility to new domains via modular additions."
                },
                "The World Avatar knowledge graph": {
                  "text": "The development draws inspiration from relevant software tools and existing reaction database schemas... Views of the domain experts are also consulted to better align with the communal understanding of the subject.",
                  "relevance": "Emphasizes the consultative, modular approach to ontology development that incorporates domain-specific tools/schemas while ensuring alignment with cross-domain standards."
                }
              },
              "id": "val-z0b81jz"
            },
            {
              "value": "Autonomous agents act as domain-agnostic executables that interpret ontology-based goals into domain-specific actions (e.g., chemical synthesis, robotic control), with their I/O signatures standardized via OntoAgent, enabling reuse across disciplines without rewriting core logic.",
              "confidence": 0.92,
              "evidence": {
                "Architecture of distributed SDLs": {
                  "text": "We believe dynamic knowledge graph technology can help with realising this architecture. Specifically... this technology abstracts the software components as agents that receive inputs and produce outputs. The flow of data between these components is represented as messages exchanged among these agents.",
                  "relevance": "Describes agents as abstracted, reusable components that mediate between domain-specific ontologies and physical actions, enabling cross-domain portability."
                },
                "The World Avatar knowledge graph": {
                  "text": "Agents are defined as executables that process inputs and generate outputs. Their I/O signatures are represented following OntoAgent... Each of the agents monitors the jobs assigned to itself and records the progress of execution in the knowledge graph.",
                  "relevance": "Highlights OntoAgent as the standardizing layer for agent I/O, ensuring consistent behavior across domains when paired with domain-specific ontologies."
                }
              },
              "id": "val-8p8hp1p"
            }
          ]
        },
        "cost-per-operation": {
          "property": "Cost per Operation (USD)",
          "label": "Cost per Operation (USD)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        }
      },
      "changes": {
        "modified_properties": {
          "evaluation-metric-primary": {
            "old_values": [
              {
                "value": "environment factor: 26.17",
                "confidence": 0.98,
                "evidence": {
                  "Abstract": {
                    "text": "The knowledge graph autonomously evolves toward the scientist’s research goals, with the two robots effectively generating a **Pareto front for cost-yield optimisation** in three days... The best values obtained are **26.17 and 258.175 g L⁻¹ h⁻¹** when scaled to the same benzaldehyde injection volume (5 mL), both outperformed the previous study, with the **highest yield of 93%** achieved during the campaign.",
                    "relevance": "Directly states the **Pareto front (cost-yield trade-off)** as the primary metric for multi-objective optimisation, alongside quantitative benchmarks (**yield: 93%**, **space-time yield: 258.175 g L⁻¹ h⁻¹**, **environment factor: 26.17**), which are standard metrics in chemical reaction optimisation."
                  },
                  "Collaborative closed-loop optimisation": {
                    "text": "Figure 6a presents the **cost-yield objectives** consisting of 65 data points collected during the self-optimisation... The real-time collaboration demonstrated faster advances in the **Pareto front** with the **highest yield of 93%**... The optimisation campaign was stopped since no more significant improvement was observed in terms of **hypervolume**... The best values obtained are **26.17 (environment factor) and 258.175 g L⁻¹ h⁻¹ (space-time yield)**.",
                    "relevance": "Explicitly ties the **Pareto front** (cost vs. yield) to the evaluation of collaborative optimisation success, while **hypervolume** (a standard metric for multi-objective optimisation convergence) is mentioned as the stopping criterion. The **space-time yield** and **environment factor** are additional key metrics for chemical process efficiency."
                  }
                },
                "id": "val-un37mes"
              }
            ],
            "new_values": [
              {
                "value": "",
                "confidence": 1,
                "evidence": {}
              }
            ],
            "old_type": "text",
            "new_type": "text"
          }
        },
        "changes_summary": {
          "text_updates": 0,
          "type_updates": 0,
          "deletions": 1,
          "additions": 0
        }
      },
      "timestamp": "2025-11-21T17:13:46.932Z"
    },
    "completedSteps": {
      "metadata": true,
      "researchFields": true,
      "researchProblems": true,
      "template": true,
      "paperContent": true
    },
    "timestamp": "2025-11-21T17:13:46.933Z"
  }
}