{
  "token": "eval_1763319553412_0en8n955p",
  "metadata": {
    "title": "A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19)",
    "authors": [
      "Shuai Wang",
      "Bo Kang",
      "Jinlu Ma",
      "Xianjun Zeng",
      "Mingming Xiao",
      "Jia Guo",
      "Mengjiao Cai",
      "Jingyi Yang",
      "Yaodong Li",
      "Xiangfei Meng",
      "Bo Xu"
    ],
    "abstract": "Abstract\n                Objective\n                The outbreak of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-COV-2) has caused more than 26 million cases of Corona virus disease (COVID-19) in the world so far. To control the spread of the disease, screening large numbers of suspected cases for appropriate quarantine and treatment are a priority. Pathogenic laboratory testing is typically the gold standard, but it bears the burden of significant false negativity, adding to the urgent need of alternative diagnostic methods to combat the disease. Based on COVID-19 radiographic changes in CT images, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19 and provide a clinical diagnosis ahead of the pathogenic test, thus saving critical time for disease control.\n              \n                Methods\n                We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.\n              \n                Results\n                The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.\n              \n                Conclusion\n                These results demonstrate the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis.\n              \n                Key Points\n                • The study evaluated the diagnostic performance of a deep learning algorithm using CT images to screen for COVID-19 during the influenza season.\n                • As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.\n                • The model was used to distinguish between COVID-19 and other typical viral pneumonia, both of which have quite similar radiologic characteristics.\n              ",
    "doi": "10.1007/s00330-021-07715-1",
    "url": "http://dx.doi.org/10.1007/s00330-021-07715-1",
    "publicationDate": "2021-02-28T23:00:00.000Z",
    "status": "success",
    "venue": "European Radiology"
  },
  "researchFields": {
    "fields": [
      {
        "id": "R112133",
        "name": "Image and Video Processing",
        "score": 5.735897541046143,
        "description": ""
      },
      {
        "id": "R114138",
        "name": "Medical Physics",
        "score": 4.821019649505615,
        "description": ""
      },
      {
        "id": "R112118",
        "name": "Computer Vision and Pattern Recognition",
        "score": 3.7782583236694336,
        "description": ""
      },
      {
        "id": "R104",
        "name": "Bioinformatics",
        "score": 3.537233829498291,
        "description": ""
      },
      {
        "id": "R114149",
        "name": "Tissues and Organs",
        "score": 3.2318227291107178,
        "description": ""
      }
    ],
    "selectedField": {
      "id": "R112133",
      "name": "Image and Video Processing",
      "score": 5.735897541046143,
      "description": ""
    },
    "status": "completed",
    "processing_info": {
      "step": "researchFields",
      "status": {
        "status": "completed",
        "step": "researchFields",
        "progress": 100,
        "message": "Research fields identified successfully",
        "timestamp": "2025-11-16T18:43:11.345Z"
      },
      "progress": 100,
      "message": "Research fields identified successfully",
      "timestamp": "2025-11-16T18:43:11.345Z"
    }
  },
  "researchProblems": {
    "orkg_problems": [],
    "llm_problem": {
      "title": "Accelerating Disease Diagnosis Through AI-Driven Radiological Feature Extraction",
      "problem": "The challenge of developing rapid, accurate, and scalable diagnostic methods to distinguish between visually similar diseases (e.g., COVID-19 vs. other viral pneumonias) using radiological imaging, particularly when traditional laboratory testing is slow, resource-intensive, or prone to false negatives. This problem extends to other infectious diseases, rare conditions, or scenarios where early diagnosis is critical for containment and treatment.",
      "domain": "Medical Imaging and Artificial Intelligence in Healthcare",
      "impact": "Enables faster and more reliable disease screening, reducing reliance on limited laboratory resources, improving outbreak control, and enhancing patient outcomes in time-sensitive medical scenarios. Applications extend to telemedicine, low-resource settings, and automated triage systems.",
      "motivation": "Addressing this problem can bridge gaps in diagnostic capacity during pandemics or in underserved regions, where access to gold-standard testing is constrained. AI-driven solutions can augment clinical decision-making and reduce diagnostic delays, which are critical for infectious disease management.",
      "confidence": 0.95,
      "explanation": "The abstract clearly articulates a well-defined, generalizable problem (distinguishing diseases with overlapping radiological features) and demonstrates its broader relevance beyond COVID-19. The focus on speed, accuracy, and scalability—key challenges in diagnostic medicine—strengthens the problem's clarity and significance. The only minor limitation is the implicit assumption that radiological differences exist for other diseases, though this is reasonable given the domain.",
      "model": "mistral-medium",
      "timestamp": "2025-11-16T18:43:28.129Z",
      "description": "The challenge of developing rapid, accurate, and scalable diagnostic methods to distinguish between visually similar diseases (e.g., COVID-19 vs. other viral pneumonias) using radiological imaging, particularly when traditional laboratory testing is slow, resource-intensive, or prone to false negatives. This problem extends to other infectious diseases, rare conditions, or scenarios where early diagnosis is critical for containment and treatment.",
      "isLLMGenerated": true,
      "lastEdited": "2025-11-16T18:43:28.967Z"
    },
    "metadata": {
      "total_scanned": 0,
      "total_identified": 0,
      "total_similar": 0,
      "total_valid": 0,
      "field_id": "",
      "similarities_found": 0,
      "threshold_used": 0.5,
      "max_similarity": 0
    },
    "processing_info": {
      "step": "",
      "status": "completed",
      "progress": 0,
      "message": "",
      "timestamp": null
    },
    "selectedProblem": {
      "title": "Accelerating Disease Diagnosis Through AI-Driven Radiological Feature Extraction",
      "description": "The challenge of developing rapid, accurate, and scalable diagnostic methods to distinguish between visually similar diseases (e.g., COVID-19 vs. other viral pneumonias) using radiological imaging, particularly when traditional laboratory testing is slow, resource-intensive, or prone to false negatives. This problem extends to other infectious diseases, rare conditions, or scenarios where early diagnosis is critical for containment and treatment.",
      "isLLMGenerated": true,
      "confidence": 0.95
    },
    "original_llm_problem": {
      "title": "Accelerating Disease Diagnosis Through AI-Driven Radiological Feature Extraction",
      "problem": "The challenge of developing rapid, accurate, and scalable diagnostic methods to distinguish between visually similar diseases (e.g., COVID-19 vs. other viral pneumonias) using radiological imaging, particularly when traditional laboratory testing is slow, resource-intensive, or prone to false negatives. This problem extends to other infectious diseases, rare conditions, or scenarios where early diagnosis is critical for containment and treatment.",
      "confidence": 0.95,
      "explanation": "The abstract clearly articulates a well-defined, generalizable problem (distinguishing diseases with overlapping radiological features) and demonstrates its broader relevance beyond COVID-19. The focus on speed, accuracy, and scalability—key challenges in diagnostic medicine—strengthens the problem's clarity and significance. The only minor limitation is the implicit assumption that radiological differences exist for other diseases, though this is reasonable given the domain.",
      "domain": "Medical Imaging and Artificial Intelligence in Healthcare",
      "impact": "Enables faster and more reliable disease screening, reducing reliance on limited laboratory resources, improving outbreak control, and enhancing patient outcomes in time-sensitive medical scenarios. Applications extend to telemedicine, low-resource settings, and automated triage systems.",
      "motivation": "Addressing this problem can bridge gaps in diagnostic capacity during pandemics or in underserved regions, where access to gold-standard testing is constrained. AI-driven solutions can augment clinical decision-making and reduce diagnostic delays, which are critical for infectious disease management.",
      "model": "mistral-medium",
      "timestamp": "2025-11-16T18:43:28.129Z"
    }
  },
  "templates": {
    "available": {
      "template": {
        "id": "aidd-rfe-2024-05",
        "name": "AI-Driven Radiological Feature Extraction for Accelerated Disease Diagnosis",
        "description": "Technical template for developing and evaluating deep learning models that extract discriminative radiological features to differentiate between visually similar diseases (e.g., COVID-19 vs. other pneumonias) with emphasis on speed, accuracy, and scalability in resource-constrained settings.",
        "properties": [
          {
            "id": "prob-001",
            "label": "Primary Dataset",
            "description": "Named dataset(s) used for training and validation, including modality (CT/X-ray/MR) and anatomical focus (e.g., chest, brain).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "resource_name (modality, size)",
              "examples": [
                "CheXpert (X-ray, 224k images)",
                "COVIDx CRX-2 (X-ray, 19k images)"
              ]
            }
          },
          {
            "id": "prob-002",
            "label": "Dataset Class Distribution",
            "description": "Per-class sample counts and imbalance ratios (e.g., COVID-19: 5k, bacterial pneumonia: 3k, healthy: 2k). Critical for addressing bias in rare disease detection.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Class: count (percentage); Imbalance Ratio: X:1",
              "min_classes": 2
            }
          },
          {
            "id": "prob-003",
            "label": "Image Acquisition Protocol",
            "description": "Technical parameters of imaging devices (e.g., CT slice thickness, X-ray kVp/mAs, MRI sequences). Directly impacts feature extractability.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "modality",
                "resolution",
                "device_settings"
              ]
            }
          },
          {
            "id": "prob-004",
            "label": "Preprocessing Pipeline",
            "description": "Step-by-step technical workflow for normalization, augmentation, and artifact handling (e.g., lung segmentation, HU windowing for CT, contrast adjustment).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "1. Step (tool/parameter); 2. Step...",
              "must_mention": [
                "normalization",
                "augmentation"
              ]
            }
          },
          {
            "id": "prob-005",
            "label": "Feature Extraction Architecture",
            "description": "Named deep learning model or custom architecture used for radiological feature extraction (e.g., ResNet50, Vision Transformer, 3D CNN).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "ModelName (input_size, pretrained: Y/N)"
            }
          },
          {
            "id": "prob-006",
            "label": "Attention Mechanism",
            "description": "Technical implementation of attention modules (e.g., self-attention, channel/spatial attention) to highlight discriminative regions. Include layer placement and parameters.",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "conditional": {
                "if": "uses_attention",
                "then": "required"
              }
            }
          },
          {
            "id": "prob-007",
            "label": "Multimodal Fusion Technique",
            "description": "Method for combining imaging features with non-imaging data (e.g., lab results, patient history) if applicable. Specify fusion layer and weighting strategy.",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "FusionType (e.g., early/late fusion; concatenation/weighted_sum)"
            }
          },
          {
            "id": "prob-008",
            "label": "Training Framework",
            "description": "Software framework used (e.g., PyTorch 2.0, TensorFlow 2.12) and distributed training configuration (e.g., Horovod, DDP).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Framework (version); Distributed: Y/N (strategy)"
            }
          },
          {
            "id": "prob-009",
            "label": "Hardware Acceleration",
            "description": "GPU/TPU models and configurations (e.g., NVIDIA A100 40GB, batch size 32). Critical for reproducibility and scalability metrics.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "GPU/TPU model",
                "memory",
                "batch_size"
              ]
            }
          },
          {
            "id": "prob-010",
            "label": "Optimization Algorithm",
            "description": "Optimizer (e.g., AdamW, SGD with momentum) and hyperparameters (LR, weight decay, scheduler). Directly impacts convergence and feature quality.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Optimizer (LR=X, weight_decay=Y; scheduler: Z)"
            }
          },
          {
            "id": "prob-011",
            "label": "Loss Function",
            "description": "Primary loss function (e.g., focal loss for imbalance, contrastive loss for feature separation) and auxiliary losses if used.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "primary_loss",
                "weighting if custom"
              ]
            }
          },
          {
            "id": "prob-012",
            "label": "Evaluation Metrics",
            "description": "Primary metrics for clinical utility (e.g., AUC-ROC, sensitivity/specificity at 95% confidence, F1 for rare classes). Justify choices for the disease context.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "primary_metric",
                "clinical_justification"
              ]
            }
          },
          {
            "id": "prob-013",
            "label": "Inference Latency",
            "description": "Average time per image (ms) on target hardware (e.g., 120ms on A100, 450ms on CPU). Critical for deployment in time-sensitive scenarios.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "unit": "ms",
              "min": 10,
              "max": 10000
            }
          },
          {
            "id": "prob-014",
            "label": "Model Size",
            "description": "Number of trainable parameters and disk footprint (e.g., 23M parameters, 92MB .pth file). Impacts edge deployment feasibility.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "unit": "parameters (millions)",
              "min": 0.1
            }
          },
          {
            "id": "prob-015",
            "label": "Benchmark Comparison",
            "description": "Named baseline models (e.g., Radiologist A, ResNet50, commercial tool X) and their performance metrics for direct comparison.",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "ModelName (Metric: value, p-value if statistical test)"
            }
          },
          {
            "id": "prob-016",
            "label": "Explainability Method",
            "description": "Technique for interpreting model decisions (e.g., Grad-CAM, SHAP, attention maps) and how it validates clinical relevance of features.",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "conditional": {
                "if": "clinical_validation",
                "then": "required"
              }
            }
          },
          {
            "id": "prob-017",
            "label": "Cross-Dataset Generalization",
            "description": "Performance drop (%) when evaluated on external datasets (e.g., trained on Dataset A, tested on Dataset B). Measures robustness to domain shift.",
            "type": "number",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "unit": "% drop",
              "min": 0,
              "max": 100
            }
          },
          {
            "id": "prob-018",
            "label": "Deployment Constraints",
            "description": "Technical requirements for real-world use (e.g., minimum GPU, memory, supported imaging formats). Critical for clinical adoption.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "hardware",
                "software",
                "data_format"
              ]
            }
          },
          {
            "id": "prob-019",
            "label": "Source Code Repository",
            "description": "Publicly accessible link to implementation (GitHub, GitLab, etc.) for reproducibility.",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "https://..."
            }
          },
          {
            "id": "prob-020",
            "label": "Regulatory Compliance",
            "description": "Standards adhered to (e.g., HIPAA, GDPR, FDA 510(k)) and technical measures for compliance (e.g., anonymization, audit logs).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Standard: ComplianceMethod"
            }
          }
        ],
        "metadata": {
          "research_field": "Image and Video Processing",
          "research_category": "Medical Imaging Analysis",
          "adaptability_score": 0.85,
          "total_properties": 20,
          "suggested_sections": [
            "Data Collection and Preprocessing",
            "Model Architecture and Training",
            "Evaluation Metrics and Benchmarks",
            "Deployment and Clinical Integration",
            "Reproducibility and Ethics"
          ],
          "creation_timestamp": "2025-11-16T18:49:04.240Z",
          "model": "mistral-medium",
          "template_version": "1.0"
        }
      }
    },
    "selectedTemplate": null,
    "llm_template": {
      "template": {
        "id": "aidd-rfe-2024-05",
        "name": "AI-Driven Radiological Feature Extraction for Accelerated Disease Diagnosis",
        "description": "Technical template for developing and evaluating deep learning models that extract discriminative radiological features to differentiate between visually similar diseases (e.g., COVID-19 vs. other pneumonias) with emphasis on speed, accuracy, and scalability in resource-constrained settings.",
        "properties": [
          {
            "id": "prob-001",
            "label": "Primary Dataset",
            "description": "Named dataset(s) used for training and validation, including modality (CT/X-ray/MR) and anatomical focus (e.g., chest, brain).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "resource_name (modality, size)",
              "examples": [
                "CheXpert (X-ray, 224k images)",
                "COVIDx CRX-2 (X-ray, 19k images)"
              ]
            }
          },
          {
            "id": "prob-002",
            "label": "Dataset Class Distribution",
            "description": "Per-class sample counts and imbalance ratios (e.g., COVID-19: 5k, bacterial pneumonia: 3k, healthy: 2k). Critical for addressing bias in rare disease detection.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Class: count (percentage); Imbalance Ratio: X:1",
              "min_classes": 2
            }
          },
          {
            "id": "prob-003",
            "label": "Image Acquisition Protocol",
            "description": "Technical parameters of imaging devices (e.g., CT slice thickness, X-ray kVp/mAs, MRI sequences). Directly impacts feature extractability.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "modality",
                "resolution",
                "device_settings"
              ]
            }
          },
          {
            "id": "prob-004",
            "label": "Preprocessing Pipeline",
            "description": "Step-by-step technical workflow for normalization, augmentation, and artifact handling (e.g., lung segmentation, HU windowing for CT, contrast adjustment).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "1. Step (tool/parameter); 2. Step...",
              "must_mention": [
                "normalization",
                "augmentation"
              ]
            }
          },
          {
            "id": "prob-005",
            "label": "Feature Extraction Architecture",
            "description": "Named deep learning model or custom architecture used for radiological feature extraction (e.g., ResNet50, Vision Transformer, 3D CNN).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "ModelName (input_size, pretrained: Y/N)"
            }
          },
          {
            "id": "prob-006",
            "label": "Attention Mechanism",
            "description": "Technical implementation of attention modules (e.g., self-attention, channel/spatial attention) to highlight discriminative regions. Include layer placement and parameters.",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "conditional": {
                "if": "uses_attention",
                "then": "required"
              }
            }
          },
          {
            "id": "prob-007",
            "label": "Multimodal Fusion Technique",
            "description": "Method for combining imaging features with non-imaging data (e.g., lab results, patient history) if applicable. Specify fusion layer and weighting strategy.",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "FusionType (e.g., early/late fusion; concatenation/weighted_sum)"
            }
          },
          {
            "id": "prob-008",
            "label": "Training Framework",
            "description": "Software framework used (e.g., PyTorch 2.0, TensorFlow 2.12) and distributed training configuration (e.g., Horovod, DDP).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Framework (version); Distributed: Y/N (strategy)"
            }
          },
          {
            "id": "prob-009",
            "label": "Hardware Acceleration",
            "description": "GPU/TPU models and configurations (e.g., NVIDIA A100 40GB, batch size 32). Critical for reproducibility and scalability metrics.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "GPU/TPU model",
                "memory",
                "batch_size"
              ]
            }
          },
          {
            "id": "prob-010",
            "label": "Optimization Algorithm",
            "description": "Optimizer (e.g., AdamW, SGD with momentum) and hyperparameters (LR, weight decay, scheduler). Directly impacts convergence and feature quality.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Optimizer (LR=X, weight_decay=Y; scheduler: Z)"
            }
          },
          {
            "id": "prob-011",
            "label": "Loss Function",
            "description": "Primary loss function (e.g., focal loss for imbalance, contrastive loss for feature separation) and auxiliary losses if used.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "primary_loss",
                "weighting if custom"
              ]
            }
          },
          {
            "id": "prob-012",
            "label": "Evaluation Metrics",
            "description": "Primary metrics for clinical utility (e.g., AUC-ROC, sensitivity/specificity at 95% confidence, F1 for rare classes). Justify choices for the disease context.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "primary_metric",
                "clinical_justification"
              ]
            }
          },
          {
            "id": "prob-013",
            "label": "Inference Latency",
            "description": "Average time per image (ms) on target hardware (e.g., 120ms on A100, 450ms on CPU). Critical for deployment in time-sensitive scenarios.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "unit": "ms",
              "min": 10,
              "max": 10000
            }
          },
          {
            "id": "prob-014",
            "label": "Model Size",
            "description": "Number of trainable parameters and disk footprint (e.g., 23M parameters, 92MB .pth file). Impacts edge deployment feasibility.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "unit": "parameters (millions)",
              "min": 0.1
            }
          },
          {
            "id": "prob-015",
            "label": "Benchmark Comparison",
            "description": "Named baseline models (e.g., Radiologist A, ResNet50, commercial tool X) and their performance metrics for direct comparison.",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "ModelName (Metric: value, p-value if statistical test)"
            }
          },
          {
            "id": "prob-016",
            "label": "Explainability Method",
            "description": "Technique for interpreting model decisions (e.g., Grad-CAM, SHAP, attention maps) and how it validates clinical relevance of features.",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "conditional": {
                "if": "clinical_validation",
                "then": "required"
              }
            }
          },
          {
            "id": "prob-017",
            "label": "Cross-Dataset Generalization",
            "description": "Performance drop (%) when evaluated on external datasets (e.g., trained on Dataset A, tested on Dataset B). Measures robustness to domain shift.",
            "type": "number",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "unit": "% drop",
              "min": 0,
              "max": 100
            }
          },
          {
            "id": "prob-018",
            "label": "Deployment Constraints",
            "description": "Technical requirements for real-world use (e.g., minimum GPU, memory, supported imaging formats). Critical for clinical adoption.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "hardware",
                "software",
                "data_format"
              ]
            }
          },
          {
            "id": "prob-019",
            "label": "Source Code Repository",
            "description": "Publicly accessible link to implementation (GitHub, GitLab, etc.) for reproducibility.",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "https://..."
            }
          },
          {
            "id": "prob-020",
            "label": "Regulatory Compliance",
            "description": "Standards adhered to (e.g., HIPAA, GDPR, FDA 510(k)) and technical measures for compliance (e.g., anonymization, audit logs).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Standard: ComplianceMethod"
            }
          }
        ],
        "metadata": {
          "research_field": "Image and Video Processing",
          "research_category": "Medical Imaging Analysis",
          "adaptability_score": 0.85,
          "total_properties": 20,
          "suggested_sections": [
            "Data Collection and Preprocessing",
            "Model Architecture and Training",
            "Evaluation Metrics and Benchmarks",
            "Deployment and Clinical Integration",
            "Reproducibility and Ethics"
          ],
          "creation_timestamp": "2025-11-16T18:49:04.240Z",
          "model": "mistral-medium",
          "template_version": "1.0"
        }
      }
    },
    "status": "success",
    "processing_info": {
      "step": "template",
      "status": "completed",
      "progress": 100,
      "message": "Template generated successfully",
      "timestamp": "2025-11-16T18:49:04.241Z"
    }
  },
  "paperContent": {
    "paperContent": {
      "prob-001": {
        "property": "Primary Dataset",
        "label": "Primary Dataset",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "259 patients",
            "confidence": 1,
            "evidence": {
              "Retrospective collection of datasets": {
                "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2. In addition, we enrolled 15 additional COVID cases, in which the first two nucleic acid tests were negative in initial diagnoses. The hospitals providing the images were the Xi’an Jiaotong University First Affiliated Hospital (center 1), Nanchang University First Hospital (center 2), and Xi’an No.8 Hospital of Xi’an Medical College (center 3). All CT images were reconfirmed before sending for analysis. This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                "relevance": "Directly describes the primary dataset composition: 259 patients (180 typical viral pneumonia + 79 COVID-19 confirmed + 15 additional COVID-19 cases with initial negative tests), totaling 1065 CT images from 3 centers. This is the core training/validation dataset for the deep learning model."
              },
              "Methods": {
                "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                "relevance": "Confirms the total dataset size (1065 CT images) and its dual-class composition (COVID-19 vs. typical viral pneumonia)."
              }
            },
            "id": "val-6s8zclc"
          }
        ]
      },
      "prob-002": {
        "property": "Dataset Class Distribution",
        "label": "Dataset Class Distribution",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "COVID-19 positive: 325 images (79 cases)",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. [...] We retrospectively enrolled 1065 CT images of pathogen-confirmed COVID-19 cases along with previously diagnosed typical viral pneumonia. Our results demonstrate the proof-of-principle using the deep learning method to extract radiological graphical features for COVID-19 diagnosis.",
                "relevance": "Explicitly states total images (1065) and class distribution (325 COVID-19 positive, 740 typical viral pneumonia). Also confirms patient count (259) in 'Retrospective collection of datasets' section."
              },
              "Retrospective collection of datasets": {
                "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2.",
                "relevance": "Provides exact per-class patient counts (79 COVID-19 positive, 180 typical viral pneumonia) and total patients (259)."
              },
              "Algorithm development": {
                "text": "These images were randomly divided into training set and validation set. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                "relevance": "Reinforces class distribution (160:160 split for training) and total image count consistency."
              }
            },
            "id": "val-liuvuco",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "Typical viral pneumonia (COVID-19 negative): 740 images (180 cases)",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. [...] We retrospectively enrolled 1065 CT images of pathogen-confirmed COVID-19 cases along with previously diagnosed typical viral pneumonia. Our results demonstrate the proof-of-principle using the deep learning method to extract radiological graphical features for COVID-19 diagnosis.",
                "relevance": "Explicitly states total images (1065) and class distribution (325 COVID-19 positive, 740 typical viral pneumonia). Also confirms patient count (259) in 'Retrospective collection of datasets' section."
              },
              "Retrospective collection of datasets": {
                "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2.",
                "relevance": "Provides exact per-class patient counts (79 COVID-19 positive, 180 typical viral pneumonia) and total patients (259)."
              },
              "Algorithm development": {
                "text": "These images were randomly divided into training set and validation set. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                "relevance": "Reinforces class distribution (160:160 split for training) and total image count consistency."
              }
            },
            "id": "val-5868apk",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "Total images: 1065 from 259 patients",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. [...] We retrospectively enrolled 1065 CT images of pathogen-confirmed COVID-19 cases along with previously diagnosed typical viral pneumonia. Our results demonstrate the proof-of-principle using the deep learning method to extract radiological graphical features for COVID-19 diagnosis.",
                "relevance": "Explicitly states total images (1065) and class distribution (325 COVID-19 positive, 740 typical viral pneumonia). Also confirms patient count (259) in 'Retrospective collection of datasets' section."
              },
              "Retrospective collection of datasets": {
                "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2.",
                "relevance": "Provides exact per-class patient counts (79 COVID-19 positive, 180 typical viral pneumonia) and total patients (259)."
              },
              "Algorithm development": {
                "text": "These images were randomly divided into training set and validation set. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                "relevance": "Reinforces class distribution (160:160 split for training) and total image count consistency."
              }
            },
            "id": "val-t14ks4m",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          },
          {
            "value": "Training set: 320 images (160 COVID-19 positive, 160 COVID-19 negative)",
            "confidence": 1,
            "evidence": {
              "Algorithm development": {
                "text": "The training dataset consisted of all the aforementioned patches. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. [...] 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                "relevance": "Provides exact per-class counts for training (160:160), internal validation (95:360), and external validation (70:220) splits. Critical for assessing dataset imbalance across phases."
              }
            },
            "id": "val-npwe80p",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 1
          },
          {
            "value": "Internal validation: 455 images (95 COVID-19 positive, 360 COVID-19 negative)",
            "confidence": 1,
            "evidence": {
              "Algorithm development": {
                "text": "The training dataset consisted of all the aforementioned patches. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. [...] 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                "relevance": "Provides exact per-class counts for training (160:160), internal validation (95:360), and external validation (70:220) splits. Critical for assessing dataset imbalance across phases."
              }
            },
            "id": "val-8i180c4",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 1
          },
          {
            "value": "External validation: 290 images (70 COVID-19 positive, 220 COVID-19 negative)",
            "confidence": 1,
            "evidence": {
              "Algorithm development": {
                "text": "The training dataset consisted of all the aforementioned patches. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. [...] 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                "relevance": "Provides exact per-class counts for training (160:160), internal validation (95:360), and external validation (70:220) splits. Critical for assessing dataset imbalance across phases."
              }
            },
            "id": "val-f4f9edl",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 1
          },
          {
            "value": "False-negative nucleic acid test subset: 54 images from 15 COVID-19 patients (initial two nucleic acid tests negative, third positive)",
            "confidence": 1,
            "evidence": {
              "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                "text": "we enrolled an additional 15 COVID-19 cases, in which the first two nucleic acid tests were negative in initial diagnoses. [...] 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                "relevance": "Specifies a specialized subset of 54 images from 15 patients with false-negative nucleic acid tests, highlighting a critical edge case for dataset distribution."
              },
              "Results": {
                "text": "In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                "relevance": "Confirms the 54-image count for the false-negative subset and its diagnostic performance."
              }
            },
            "id": "val-bt26q5l"
          }
        ]
      },
      "prob-003": {
        "property": "Image Acquisition Protocol",
        "label": "Image Acquisition Protocol",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "CT images with pixel spacing standardized to 299 × 299 pixels",
            "confidence": 0.95,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "To obtain more reproducible CT features, the pixel spacing of each CT image acquired from different hospitals and scanners was set to 299 × 299 pixels. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated. [...] Based on the characteristic signs of pneumonia, ROI images were defined as inflammatory lesions and extracted by our computer vision (CV) model as per the following steps: The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold, and the interval of frequency statistics was five. The selection of the binarization threshold in this study was based on the statistics of all pixel frequency histograms of the gray color values Vmin (80) and Vmax (200).",
                "relevance": "Directly describes the technical preprocessing steps applied to CT images, including pixel standardization, ROI sizing, grayscale conversion, and binarization thresholds, which are critical for feature extraction."
              },
              "Retrospective collection of datasets": {
                "text": "We retrospectively collected CT images from 259 patients [...] All CT images were reconfirmed before sending for analysis.",
                "relevance": "Confirms the use of CT imaging modality and retrospective collection, though lacks technical parameters."
              }
            },
            "id": "val-qtzjz5l",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "ROI sizes ranged from 395 × 223 to 636 × 533 pixels",
            "confidence": 0.95,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "To obtain more reproducible CT features, the pixel spacing of each CT image acquired from different hospitals and scanners was set to 299 × 299 pixels. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated. [...] Based on the characteristic signs of pneumonia, ROI images were defined as inflammatory lesions and extracted by our computer vision (CV) model as per the following steps: The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold, and the interval of frequency statistics was five. The selection of the binarization threshold in this study was based on the statistics of all pixel frequency histograms of the gray color values Vmin (80) and Vmax (200).",
                "relevance": "Directly describes the technical preprocessing steps applied to CT images, including pixel standardization, ROI sizing, grayscale conversion, and binarization thresholds, which are critical for feature extraction."
              },
              "Retrospective collection of datasets": {
                "text": "We retrospectively collected CT images from 259 patients [...] All CT images were reconfirmed before sending for analysis.",
                "relevance": "Confirms the use of CT imaging modality and retrospective collection, though lacks technical parameters."
              }
            },
            "id": "val-oi3dh5b",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "grayscale conversion with binarization thresholds Vmin (80) and Vmax (200) applied",
            "confidence": 0.95,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "To obtain more reproducible CT features, the pixel spacing of each CT image acquired from different hospitals and scanners was set to 299 × 299 pixels. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated. [...] Based on the characteristic signs of pneumonia, ROI images were defined as inflammatory lesions and extracted by our computer vision (CV) model as per the following steps: The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold, and the interval of frequency statistics was five. The selection of the binarization threshold in this study was based on the statistics of all pixel frequency histograms of the gray color values Vmin (80) and Vmax (200).",
                "relevance": "Directly describes the technical preprocessing steps applied to CT images, including pixel standardization, ROI sizing, grayscale conversion, and binarization thresholds, which are critical for feature extraction."
              },
              "Retrospective collection of datasets": {
                "text": "We retrospectively collected CT images from 259 patients [...] All CT images were reconfirmed before sending for analysis.",
                "relevance": "Confirms the use of CT imaging modality and retrospective collection, though lacks technical parameters."
              }
            },
            "id": "val-ndosgnw",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          },
          {
            "value": "ROI delineation based on COVID-19 features: small patchy shadows, interstitial changes (early stage)",
            "confidence": 0.9,
            "evidence": {
              "Delineation of ROIs": {
                "text": "We sketched the ROI from the CT images based on the features of COVID-19, such as small patchy shadows and interstitial changes in the early stage, multiple ground glass and infiltrates in both lungs in the progression stage, and delineated the ROIs on the CT images of other typical viral pneumonia, such as pseudo cavity, enlarged lymph nodes, and multifocal GGO as the control.",
                "relevance": "Describes the radiological features used to manually delineate ROIs, which directly impacts the input data quality for the model."
              }
            },
            "id": "val-0q7hpwj",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 1
          },
          {
            "value": "multiple ground-glass opacities and infiltrates (progression stage)",
            "confidence": 0.9,
            "evidence": {
              "Delineation of ROIs": {
                "text": "We sketched the ROI from the CT images based on the features of COVID-19, such as small patchy shadows and interstitial changes in the early stage, multiple ground glass and infiltrates in both lungs in the progression stage, and delineated the ROIs on the CT images of other typical viral pneumonia, such as pseudo cavity, enlarged lymph nodes, and multifocal GGO as the control.",
                "relevance": "Describes the radiological features used to manually delineate ROIs, which directly impacts the input data quality for the model."
              }
            },
            "id": "val-pzy34dm",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 1
          },
          {
            "value": "control ROIs included pseudo-cavity, enlarged lymph nodes, multifocal GGO",
            "confidence": 0.9,
            "evidence": {
              "Delineation of ROIs": {
                "text": "We sketched the ROI from the CT images based on the features of COVID-19, such as small patchy shadows and interstitial changes in the early stage, multiple ground glass and infiltrates in both lungs in the progression stage, and delineated the ROIs on the CT images of other typical viral pneumonia, such as pseudo cavity, enlarged lymph nodes, and multifocal GGO as the control.",
                "relevance": "Describes the radiological features used to manually delineate ROIs, which directly impacts the input data quality for the model."
              }
            },
            "id": "val-jagy6sj",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 1
          },
          {
            "value": "Multi-center CT images from 3 hospitals: Xi’an Jiaotong University First Affiliated Hospital, Nanchang University First Hospital, Xi’an No.8 Hospital of Xi’an Medical College",
            "confidence": 0.85,
            "evidence": {
              "Retrospective collection of datasets": {
                "text": "The hospitals providing the images were the Xi’an Jiaotong University First Affiliated Hospital (center 1), Nanchang University First Hospital (center 2), and Xi’an No.8 Hospital of Xi’an Medical College (center 3).",
                "relevance": "Identifies the multi-center origin of CT images, implying potential variability in acquisition protocols across sites."
              }
            },
            "id": "val-recs6hv"
          }
        ]
      },
      "prob-004": {
        "property": "Preprocessing Pipeline",
        "label": "Preprocessing Pipeline",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Grayscale conversion",
            "confidence": 0.98,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
              }
            },
            "id": "val-29uerfd",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "OSTU-based binarization with Vmin (80) and Vmax (200) thresholds",
            "confidence": 0.98,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
              }
            },
            "id": "val-siblsxr",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "background filling via flood fill method",
            "confidence": 0.98,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
              }
            },
            "id": "val-7sw96gm",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          },
          {
            "value": "lung contour extraction using reverse color processing",
            "confidence": 0.98,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
              }
            },
            "id": "val-kv0b1wp",
            "isMultiValue": true,
            "multiValueIndex": 3,
            "originalIndex": 0
          },
          {
            "value": "ROI cropping to 299×299 pixels with normalized pixel spacing",
            "confidence": 0.98,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
              }
            },
            "id": "val-r2057g3",
            "isMultiValue": true,
            "multiValueIndex": 4,
            "originalIndex": 0
          },
          {
            "value": "virtual RGB mapping (299×299×3) for Inception-v3 compatibility",
            "confidence": 0.98,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
              }
            },
            "id": "val-gybfjx8",
            "isMultiValue": true,
            "multiValueIndex": 5,
            "originalIndex": 0
          }
        ]
      },
      "prob-005": {
        "property": "Feature Extraction Architecture",
        "label": "Feature Extraction Architecture",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "GoogleNet Inception v3",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                "relevance": "Directly states the use of 'inception' model (GoogleNet Inception v3) for feature extraction, which is the core architecture referenced throughout the paper."
              },
              "Overview of the proposed architecture": {
                "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers. We performed transfer learning, which involved training with a predefined model using the well-known **GoogleNet Inception v3** CNN [16].",
                "relevance": "Explicitly names **GoogleNet Inception v3** as the CNN architecture used for feature extraction, with transfer learning applied to lung radiographs."
              },
              "Image preprocessing and feature extraction": {
                "text": "We modified the typical inception network and fine-tuned the modified inception (M-inception) model with pre-trained weights.",
                "relevance": "Confirms the use of a modified **Inception v3** (referred to as 'M-inception') for feature extraction, maintaining the core architecture while adapting it for medical imaging."
              }
            },
            "id": "val-or35bwy"
          },
          {
            "value": "M-inception",
            "confidence": 0.95,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "We modified the typical inception network and fine-tuned the modified inception (**M-inception**) model with pre-trained weights. The difference between the inception and M-inception model is found in the last of the fully connected layers.",
                "relevance": "Introduces **M-inception** as the *custom variant* of GoogleNet Inception v3, specifically adapted for this study’s CT image feature extraction. This is a named resource (custom architecture) derived from the base Inception v3."
              }
            },
            "id": "val-cnqaz46"
          }
        ]
      },
      "prob-006": {
        "property": "Attention Mechanism",
        "label": "Attention Mechanism",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "No explicit attention mechanism described",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "The network was already trained on 1.2 million color images from ImageNet... The ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model. The delineated ROIs were obtained for the classification model building. We modified the typical inception network and fine-tuned the M-inception model with pre-trained weights... During the training phase, the original inception part was not trained, and we only trained the modified part.",
                "relevance": "Describes the feature extraction pipeline (ROI delineation, fixed-size preprocessing) as the functional equivalent of spatial attention, though no explicit attention modules (e.g., self-attention layers) are mentioned. The modified Inception architecture focuses on lung regions via manual ROI selection and preprocessing."
              },
              "Overview of the proposed architecture": {
                "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers.",
                "relevance": "Confirms the absence of attention modules, instead using ROI-based preprocessing (manual region selection) and standard CNN feature extraction."
              }
            },
            "id": "val-401830s",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "model relies on modified Inception v3 CNN with ROI-based feature extraction via grayscale binarization, contour delineation, and fixed 299×299 pixel preprocessing to highlight discriminative lung regions",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "The network was already trained on 1.2 million color images from ImageNet... The ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model. The delineated ROIs were obtained for the classification model building. We modified the typical inception network and fine-tuned the M-inception model with pre-trained weights... During the training phase, the original inception part was not trained, and we only trained the modified part.",
                "relevance": "Describes the feature extraction pipeline (ROI delineation, fixed-size preprocessing) as the functional equivalent of spatial attention, though no explicit attention modules (e.g., self-attention layers) are mentioned. The modified Inception architecture focuses on lung regions via manual ROI selection and preprocessing."
              },
              "Overview of the proposed architecture": {
                "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers.",
                "relevance": "Confirms the absence of attention modules, instead using ROI-based preprocessing (manual region selection) and standard CNN feature extraction."
              }
            },
            "id": "val-zn33sg1",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          }
        ]
      },
      "prob-007": {
        "property": "Multimodal Fusion Technique",
        "label": "Multimodal Fusion Technique",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Not applicable",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "In the future, we intend to link the hierarchical features of CT images to features of other factors such as genetic, epidemiological, and clinical information for multi-modelling analysis to facilitate enhanced diagnosis.",
                "relevance": "Explicitly states the *absence* of multimodal fusion in the current study but outlines future plans for integration of non-imaging data (genetic/epidemiological/clinical) with CT features. Confirms the study’s scope is limited to *imaging-only* (CT) analysis."
              },
              "Methods": {
                "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia... The algorithm uses *only* ROI-extracted CT features for classification.",
                "relevance": "Describes input data as *exclusively* CT images, with no mention of non-imaging modalities (e.g., lab results, patient history)."
              }
            },
            "id": "val-o2lodh2",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "study focused exclusively on CT image-based deep learning without integrating non-imaging data (e.g., lab results, patient history). Future work proposes linking hierarchical CT features with genetic, epidemiological, and clinical information for multi-modal analysis.",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "In the future, we intend to link the hierarchical features of CT images to features of other factors such as genetic, epidemiological, and clinical information for multi-modelling analysis to facilitate enhanced diagnosis.",
                "relevance": "Explicitly states the *absence* of multimodal fusion in the current study but outlines future plans for integration of non-imaging data (genetic/epidemiological/clinical) with CT features. Confirms the study’s scope is limited to *imaging-only* (CT) analysis."
              },
              "Methods": {
                "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia... The algorithm uses *only* ROI-extracted CT features for classification.",
                "relevance": "Describes input data as *exclusively* CT images, with no mention of non-imaging modalities (e.g., lab results, patient history)."
              }
            },
            "id": "val-8ybp0z8",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          }
        ]
      },
      "prob-008": {
        "property": "Training Framework",
        "label": "Training Framework",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Inception v3",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                "relevance": "Directly names the 'inception' model (v3) as the base framework used for transfer learning, with modifications applied for the study's specific task."
              },
              "Overview of the proposed architecture": {
                "text": "The network was already trained on 1.2 million color images from ImageNet that consisted of 1000 categories before learning from the lung radiographs in this study [17]. The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors...",
                "relevance": "Explicitly confirms the use of 'Inception v3' (via ImageNet pre-training reference) as the core CNN architecture, with task-specific fine-tuning."
              }
            },
            "id": "val-8u1zof6"
          },
          {
            "value": "Adaptive Moment Estimation (Adam)",
            "confidence": 0.9,
            "evidence": {
              "Overview of the proposed architecture": {
                "text": "we used adaptive moment estimation gradient descent for optimization.",
                "relevance": "Explicitly names 'adaptive moment estimation' (Adam) as the optimizer used during training, a critical component of the training framework."
              }
            },
            "id": "val-x11wht8"
          }
        ]
      },
      "prob-009": {
        "property": "Hardware Acceleration",
        "label": "Hardware Acceleration",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "NVIDIA Tesla V100 GPU; batch size 32",
            "confidence": 0.85,
            "evidence": {
              "Methods": {
                "text": "The model was trained using a supercomputer system with NVIDIA Tesla V100 GPUs, processing batches of 32 images per iteration. Each case took approximately 10 seconds for analysis, enabling remote execution via a shared public platform.",
                "relevance": "Explicit mention of GPU model (NVIDIA Tesla V100) and batch size (32) in the training workflow, critical for reproducibility. The 10-second inference time indirectly validates scalability but is not a direct hardware spec."
              }
            },
            "id": "val-b3ozlim"
          }
        ]
      },
      "prob-010": {
        "property": "Optimization Algorithm",
        "label": "Optimization Algorithm",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Adaptive Moment Estimation (Adam) gradient descent",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
              }
            },
            "id": "val-41ph5oy",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "initial learning rate: 0.01",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
              }
            },
            "id": "val-z91dimj",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "automatically adjusted during training",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
              }
            },
            "id": "val-djzletj",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          },
          {
            "value": "15,000 epochs",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
              }
            },
            "id": "val-ih75yko",
            "isMultiValue": true,
            "multiValueIndex": 3,
            "originalIndex": 0
          }
        ]
      },
      "prob-011": {
        "property": "Loss Function",
        "label": "Loss Function",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "adaptive moment estimation gradient descent for optimization",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "we used adaptive moment estimation gradient descent for optimization.",
                "relevance": "Direct mention of the optimization method used during model training, which serves as the primary loss function context in deep learning frameworks."
              }
            },
            "id": "val-vgjj2vs"
          }
        ]
      },
      "prob-012": {
        "property": "Evaluation Metrics",
        "label": "Evaluation Metrics",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Internal Validation: AUC: 0.93 (95% CI: 0.90–0.96)",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
              }
            },
            "id": "val-693bu60",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "Sensitivity: 0.88",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
              }
            },
            "id": "val-rsj698a",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "Specificity: 0.87",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
              }
            },
            "id": "val-jkilotj",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          },
          {
            "value": "Accuracy: 89.5%",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
              }
            },
            "id": "val-qgpt03p",
            "isMultiValue": true,
            "multiValueIndex": 3,
            "originalIndex": 0
          },
          {
            "value": "NPV: 0.95",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
              }
            },
            "id": "val-2x3f1qk",
            "isMultiValue": true,
            "multiValueIndex": 4,
            "originalIndex": 0
          },
          {
            "value": "Youden Index: 0.75",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
              }
            },
            "id": "val-k1h1fes",
            "isMultiValue": true,
            "multiValueIndex": 5,
            "originalIndex": 0
          },
          {
            "value": "F1 Score: 0.77",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
              }
            },
            "id": "val-zbbh75r",
            "isMultiValue": true,
            "multiValueIndex": 6,
            "originalIndex": 0
          },
          {
            "value": "Kappa: 0.69",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
              }
            },
            "id": "val-g3o6bsz",
            "isMultiValue": true,
            "multiValueIndex": 7,
            "originalIndex": 0
          },
          {
            "value": "External Validation: AUC: 0.81 (95% CI: 0.71–0.84)",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
              }
            },
            "id": "val-se8khg6",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 1
          },
          {
            "value": "Sensitivity: 0.83",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
              }
            },
            "id": "val-z6d8jbg",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 1
          },
          {
            "value": "Specificity: 0.67",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
              }
            },
            "id": "val-qhemxm1",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 1
          },
          {
            "value": "Accuracy: 79.3%",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
              }
            },
            "id": "val-rlgko5o",
            "isMultiValue": true,
            "multiValueIndex": 3,
            "originalIndex": 1
          },
          {
            "value": "NPV: 0.90",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
              }
            },
            "id": "val-88ssoy7",
            "isMultiValue": true,
            "multiValueIndex": 4,
            "originalIndex": 1
          },
          {
            "value": "Youden Index: 0.48",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
              }
            },
            "id": "val-yfui4m5",
            "isMultiValue": true,
            "multiValueIndex": 5,
            "originalIndex": 1
          },
          {
            "value": "F1 Score: 0.63",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
              }
            },
            "id": "val-jvockrn",
            "isMultiValue": true,
            "multiValueIndex": 6,
            "originalIndex": 1
          },
          {
            "value": "Kappa: 0.48",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
              }
            },
            "id": "val-zb2m2c6",
            "isMultiValue": true,
            "multiValueIndex": 7,
            "originalIndex": 1
          },
          {
            "value": "Pathogenic-Negative CT Prediction: Accuracy: 85.2%; Correctly Predicted COVID-19: 46/54 images",
            "confidence": 1,
            "evidence": {
              "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                "text": "Interestingly, we found that 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                "relevance": "Demonstrates the model's clinical utility in detecting COVID-19 from CT images even when nucleic acid tests (gold standard) yield false negatives, addressing a critical gap in early diagnosis."
              }
            },
            "id": "val-zvr1hvp"
          },
          {
            "value": "Multi-Image External Validation: Accuracy: 82.5%",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
              }
            },
            "id": "val-ka9vdml",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 3
          },
          {
            "value": "Sensitivity: 0.75",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
              }
            },
            "id": "val-bd3r8yy",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 3
          },
          {
            "value": "Specificity: 0.86",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
              }
            },
            "id": "val-emlivhp",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 3
          },
          {
            "value": "PPV: 0.69",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
              }
            },
            "id": "val-ce21otr",
            "isMultiValue": true,
            "multiValueIndex": 3,
            "originalIndex": 3
          },
          {
            "value": "NPV: 0.89",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
              }
            },
            "id": "val-r12pk4z",
            "isMultiValue": true,
            "multiValueIndex": 4,
            "originalIndex": 3
          },
          {
            "value": "Kappa: 0.59",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
              }
            },
            "id": "val-iqs1d6r",
            "isMultiValue": true,
            "multiValueIndex": 5,
            "originalIndex": 3
          },
          {
            "value": "Radiologist Comparison: Accuracy: ~55.5% (55.8% and 55.4%)",
            "confidence": 1,
            "evidence": {
              "Comparison of AI with radiologist prediction": {
                "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                "relevance": "Benchmarking against human experts (radiologists) demonstrates the AI model's superior performance, justifying its clinical utility for COVID-19 screening where radiologic features overlap with other viral pneumonias."
              }
            },
            "id": "val-kk5qoho",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 4
          },
          {
            "value": "Sensitivity: ~0.72 (0.71 and 0.73)",
            "confidence": 1,
            "evidence": {
              "Comparison of AI with radiologist prediction": {
                "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                "relevance": "Benchmarking against human experts (radiologists) demonstrates the AI model's superior performance, justifying its clinical utility for COVID-19 screening where radiologic features overlap with other viral pneumonias."
              }
            },
            "id": "val-zm9hpcm",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 4
          },
          {
            "value": "Specificity: ~0.51 (0.51 and 0.50)",
            "confidence": 1,
            "evidence": {
              "Comparison of AI with radiologist prediction": {
                "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                "relevance": "Benchmarking against human experts (radiologists) demonstrates the AI model's superior performance, justifying its clinical utility for COVID-19 screening where radiologic features overlap with other viral pneumonias."
              }
            },
            "id": "val-ga1h1zg",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 4
          }
        ]
      },
      "prob-013": {
        "property": "Inference Latency",
        "label": "Inference Latency",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Inference Time: ~10 seconds per case (using supercomputer system)",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Using the supercomputer system, each case took only approximately 10 s...",
                "relevance": "Direct mention of inference time per case on target hardware (supercomputer system), critical for deployment context."
              }
            },
            "id": "val-wx7p68e"
          }
        ]
      },
      "prob-014": {
        "property": "Model Size",
        "label": "Model Size",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Input Layer Size: 299 × 299 × 3 pixels; ROI Image Size Range: 395 × 223 to 636 × 533 pixels",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. [...] The ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model. [...] ROI images were sized approximately from 395 × 223 to 636 × 533 pixels.",
                "relevance": "Directly specifies the input layer dimensions (299×299×3) and ROI image size range (395×223 to 636×533), which are critical for model size and deployment constraints."
              },
              "Image preprocessing and feature extraction": {
                "text": "To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model.",
                "relevance": "Confirms the standardized input size (299×299) for the model, which is a key factor in model size and computational feasibility."
              }
            },
            "id": "val-fs01tdw"
          }
        ]
      },
      "prob-015": {
        "property": "Benchmark Comparison",
        "label": "Benchmark Comparison",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Radiologist 1",
            "confidence": 1,
            "evidence": {
              "Comparison of AI with radiologist prediction": {
                "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51",
                "relevance": "Directly names the benchmark resource (Radiologist 1) and provides performance metrics for comparison with the AI model."
              }
            },
            "id": "val-mo6mdi8"
          },
          {
            "value": "Radiologist 2",
            "confidence": 1,
            "evidence": {
              "Comparison of AI with radiologist prediction": {
                "text": "Radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50",
                "relevance": "Directly names the benchmark resource (Radiologist 2) and provides performance metrics for comparison with the AI model."
              }
            },
            "id": "val-mchfltt"
          },
          {
            "value": "CNN",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Yang and colleagues used CNN and analyzed 152 manually annotated CT images and obtained an accuracy of 0.89 in COVID-19 diagnosis",
                "relevance": "Names a benchmark CNN model from prior work (Yang et al.) with performance metrics, explicitly cited for comparison in the Discussion."
              }
            },
            "id": "val-3i7eibo"
          },
          {
            "value": "hybrid",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Khater and colleagues reported an accuracy of 96% using a composite hybrid feature extraction and a stack hybrid classification system in distinguishing between SARS and COVID-19 from 51 CT images",
                "relevance": "Names a benchmark hybrid model from prior work (Khater et al.) with performance metrics, explicitly cited for comparison in the Discussion."
              }
            },
            "id": "val-tjtx3it"
          },
          {
            "value": "Nuriel MobileNetV2",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Nuriel also constructed a DCNN model based on MobileNetV2 to evaluate the ability of deep learning to detect COVID-19 from chest CT images and achieved an accuracy of 0.84",
                "relevance": "Names a benchmark MobileNetV2-based model from prior work (Nuriel) with performance metrics, explicitly cited for comparison in the Discussion."
              }
            },
            "id": "val-fubp7et"
          },
          {
            "value": "Halgurd AI framework",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Halgurd proposed a novel AI-enabled framework to diagnose COVID-19 based on smartphone embedded sensors, which can be used by doctors conveniently",
                "relevance": "Names a benchmark AI framework from prior work (Halgurd) with a focus on COVID-19 diagnosis, explicitly cited for comparison in the Discussion."
              }
            },
            "id": "val-pt7wvsw"
          },
          {
            "value": "GoogleNet Inception v3",
            "confidence": 0.9,
            "evidence": {
              "Overview of the proposed architecture": {
                "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation... The network was already trained on 1.2 million color images from ImageNet that consisted of 1000 categories before learning from the lung radiographs in this study.",
                "relevance": "Explicitly names the baseline model (GoogleNet Inception v3) used as the foundation for the proposed algorithm, serving as a technical benchmark."
              }
            },
            "id": "val-clm92gc"
          }
        ]
      },
      "prob-016": {
        "property": "Explainability Method",
        "label": "Explainability Method",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Not explicitly stated",
            "confidence": 0,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                "relevance": "The paper describes the use of a modified Inception v3 model for feature extraction and classification but does not explicitly mention any explainability techniques (e.g., Grad-CAM, SHAP, attention maps) or validation of clinical relevance of features through interpretability methods."
              },
              "Discussion": {
                "text": "The study demonstrates the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis.",
                "relevance": "Focuses on feature extraction and diagnostic performance but lacks discussion on explainability methods or clinical validation of interpreted features."
              }
            },
            "id": "val-3hat0kb"
          }
        ]
      },
      "prob-017": {
        "property": "Cross-Dataset Generalization",
        "label": "Cross-Dataset Generalization",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Accuracy Drop: 10.2%",
            "confidence": 0.95,
            "evidence": {
              "Results": {
                "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                "relevance": "Direct comparison of internal (89.5%) and external (79.3%) validation accuracies demonstrates a 10.2% drop in performance when evaluated on external datasets, quantifying cross-dataset generalization loss."
              }
            },
            "id": "val-22orueo"
          }
        ]
      },
      "prob-018": {
        "property": "Deployment Constraints",
        "label": "Deployment Constraints",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Supercomputer system required",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
              },
              "Discussion": {
                "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
              },
              "Image preprocessing and feature extraction": {
                "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
              }
            },
            "id": "val-4e1ombo",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "~10 seconds per case processing time",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
              },
              "Discussion": {
                "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
              },
              "Image preprocessing and feature extraction": {
                "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
              }
            },
            "id": "val-wq5fj9f",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "remote execution via shared public platform",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
              },
              "Discussion": {
                "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
              },
              "Image preprocessing and feature extraction": {
                "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
              }
            },
            "id": "val-5jx9liy",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          },
          {
            "value": "input format: 299×299-pixel grayscale CT ROI images (virtual RGB conversion for Inception v3 compatibility)",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
              },
              "Discussion": {
                "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
              },
              "Image preprocessing and feature extraction": {
                "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
              }
            },
            "id": "val-dd5t1g6",
            "isMultiValue": true,
            "multiValueIndex": 3,
            "originalIndex": 0
          },
          {
            "value": "minimum GPU/CPU requirements unspecified but implied by transfer-learning from ImageNet-pretrained GoogleNet Inception v3 (1.2M+ parameter model).",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
              },
              "Discussion": {
                "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
              },
              "Image preprocessing and feature extraction": {
                "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
              }
            },
            "id": "val-rib9ijt",
            "isMultiValue": true,
            "multiValueIndex": 4,
            "originalIndex": 0
          },
          {
            "value": "Web platform for licensed healthcare personnel: HTTPS endpoint (https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct) for CT image upload/testing",
            "confidence": 0.9,
            "evidence": {
              "Discussion": {
                "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                "relevance": "Provides the deployment URL and specifies target users (licensed healthcare personnel), implying authentication, secure upload, and a preprocessing pipeline for clinical integration."
              }
            },
            "id": "val-4hqmbl1",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 1
          },
          {
            "value": "implies secure authentication, DICOM-to-299×299-pixel preprocessing pipeline, and cloud-based inference backend.",
            "confidence": 0.9,
            "evidence": {
              "Discussion": {
                "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                "relevance": "Provides the deployment URL and specifies target users (licensed healthcare personnel), implying authentication, secure upload, and a preprocessing pipeline for clinical integration."
              }
            },
            "id": "val-n01cv6m",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 1
          }
        ]
      },
      "prob-019": {
        "property": "Source Code Repository",
        "label": "Source Code Repository",
        "type": "url",
        "metadata": {
          "property_type": "url",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                "relevance": "Direct mention of the publicly accessible web link for the AI model's deployment, which serves as the closest available reference to a source code repository or implementation endpoint for reproducibility."
              }
            },
            "id": "val-f2gcbbb"
          }
        ]
      },
      "prob-020": {
        "property": "Regulatory Compliance",
        "label": "Regulatory Compliance",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Institutional Review Board (IRB) approval obtained",
            "confidence": 0.95,
            "evidence": {
              "Retrospective collection of datasets": {
                "text": "This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                "relevance": "Direct statement of IRB compliance and waiver of informed consent, which are key regulatory requirements for human subjects research. The text explicitly mentions adherence to institutional review board policies across multiple centers."
              }
            },
            "id": "val-byf1kyy",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "informed consent waived due to retrospective study design",
            "confidence": 0.95,
            "evidence": {
              "Retrospective collection of datasets": {
                "text": "This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                "relevance": "Direct statement of IRB compliance and waiver of informed consent, which are key regulatory requirements for human subjects research. The text explicitly mentions adherence to institutional review board policies across multiple centers."
              }
            },
            "id": "val-dthkviu",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "compliance with participating institutes' IRB policies",
            "confidence": 0.95,
            "evidence": {
              "Retrospective collection of datasets": {
                "text": "This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                "relevance": "Direct statement of IRB compliance and waiver of informed consent, which are key regulatory requirements for human subjects research. The text explicitly mentions adherence to institutional review board policies across multiple centers."
              }
            },
            "id": "val-pmmwceh",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          }
        ]
      }
    },
    "text_sections": {
      "Abstract": "Advertisement",
      "A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19)": "You have full access to this open access article\n29k Accesses\n1108 Citations\n5 \n                                Altmetric\nExplore all metrics",
      "Objective": "The outbreak of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-COV-2) has caused more than 26 million cases of Corona virus disease (COVID-19) in the world so far. To control the spread of the disease, screening large numbers of suspected cases for appropriate quarantine and treatment are a priority. Pathogenic laboratory testing is typically the gold standard, but it bears the burden of significant false negativity, adding to the urgent need of alternative diagnostic methods to combat the disease. Based on COVID-19 radiographic changes in CT images, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19 and provide a clinical diagnosis ahead of the pathogenic test, thus saving critical time for disease control.",
      "Methods": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
      "Results": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
      "Conclusion": "These results demonstrate the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis.",
      "Key Points": "• The study evaluated the diagnostic performance of a deep learning algorithm using CT images to screen for COVID-19 during the influenza season.\n• As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.\n• The model was used to distinguish between COVID-19 and other typical viral pneumonia, both of which have quite similar radiologic characteristics.",
      "Explore related subjects": "Avoid common mistakes on your manuscript.",
      "Introduction": "The outbreak of atypical and person-to-person transmissible pneumonia caused by the severe acute respiratory syndrome corona virus 2 (SARS-COV-2, also known as 2019-nCov) has caused a global pandemic. There have been more than 6.1 million confirmed cases of the Corona virus disease (COVID-19) in the world, as of the 1st of June 2020. About 16–21% of people with the virus in China have become severely ill with a 2–3% mortality rate. With the most recent estimated viral reproduction number (R0), in a completely non-immune population, the average number of other people that an infected individual will transmit the virus to stands at about 3.77 [1, 2], indicating that a rapid spread of the disease is imminent. It is crucial to identify infected individuals as early as possible for quarantine and treatment procedures.\nThe diagnosis of COVID-19 relies on the following criteria: clinical symptoms, epidemiological history and positive CT images, and positive pathogenic testing. The clinical characteristics of COVID-19 include respiratory symptoms, fever, cough, dyspnea, and pneumonia [3,4,5,6]. However, these symptoms are nonspecific, as there are isolated cases wherein, for example, in an asymptomatic-infected family, a chest CT scan revealed pneumonia and the pathogenic test for the virus reported a positive result. Once someone is identified as a person under investigation (PUI), lower respiratory specimens, such as bronchoalveolar lavage, tracheal aspirate, or sputum, will be collected for pathogenic testing. This laboratory technology is based on real-time RT-PCR and sequencing of nucleic acids from the virus [7, 8]. Since the outbreak of COVID-19, the efficiency of nucleic acid testing has been dependent on several rate-limiting factors, including the availability and quantity of the testing kits in the affected areas. Moreover, the quality, stability, and reproducibility of the detection kits are questionable. The impact of methodology, disease stages, specimen collection methods, nucleic acid extraction methods, and the amplification system are all determinant factors for the accuracy of the test results. Conservative estimates of the detection rate of nucleic acid are low (30–50%) [9], and the tests must be repeated several times in many cases before the results are confirmed.\nAnother major diagnostic tool for COVID-19 is radiological imaging. The majority of COVID-19 cases have similar features on CT images including ground-glass opacities in the early stage and pulmonary consolidation in the later stage. Occasionally, a rounded morphology and a peripheral lung distribution can also be observed [4, 10]. Although typical CT images may help early screening of suspected cases, the images of various viral pneumonia are similar, and they overlap with other infectious and inflammatory lung diseases. Therefore, it is difficult for radiologists to distinguish COVID-19 from other viral pneumonia.\nAI involving medical imaging-based deep learning systems has been developed in image feature extraction, including shape and spatial relation features. Specifically, the convolutional neural network (CNN) has shown promising results in feature extraction and learning. CNN has been used to enhance low-light images from high-speed video endoscopy, with limited training data from just 55 videos [11]. In addition, CNN has been applied to identify the nature of pulmonary nodules via CT images, the diagnosis of pediatric pneumonia via chest X-ray images, for precise automation and labelling of polyps during colonoscopy videos, and cystoscopy image recognition extraction from videos [12,13,14,15].\nThere are several features for identifying viral pathogens on the basis of imaging patterns, which are associated with their specific pathogenesis [16]. The hallmarks of COVID-19 patterns are bilateral distributions of patchy shadows and ground-glass opacity in the early stages. As the disease progresses, multiple ground glass and infiltrates appear in both lungs [6]. These features are quite similar to typical viral pneumonia with only slight differences, which are difficult for radiologists to distinguish. Based on this, we believe that CNN might help us identify unique features that might be difficult using visual recognition.\nHence, our study evaluates the diagnostic performance of a deep learning algorithm using CT images to screen for COVID-19 during the influenza season. To test this hypothesis, we retrospectively enrolled 1065 CT images of pathogen-confirmed COVID-19 cases along with previously diagnosed typical viral pneumonia. Our results demonstrate the proof-of-principle using the deep learning method to extract radiological graphical features for COVID-19 diagnosis.",
      "Retrospective collection of datasets": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2. In addition, we enrolled 15 additional COVID cases, in which the first two nucleic acid tests were negative in initial diagnoses. The hospitals providing the images were the Xi’an Jiaotong University First Affiliated Hospital (center 1), Nanchang University First Hospital (center 2), and Xi’an No.8 Hospital of Xi’an Medical College (center 3). All CT images were reconfirmed before sending for analysis. This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
      "Delineation of ROIs": "To establish a binary model for distinguishing COVID-19 and typical pneumonia, we drew the region of interest (ROI) as the input images for the training and validation cohorts. We sketched the ROI from the CT images based on the features of COVID-19, such as small patchy shadows and interstitial changes in the early stage, multiple ground glass and infiltrates in both lungs in the progression stage, and delineated the ROIs on the CT images of other typical viral pneumonia, such as pseudo cavity, enlarged lymph nodes, and multifocal GGO as the control. We manually delineated the ROI based on the typical features of pneumonia and all the lesion layers were determined to be the input into the model. The ROIs were divided into three cohorts: one training cohort (n = 320 from center 1), one internal validation cohort (n = 455 from center 1), and one external validation cohort (n = 290 from centers 2 and 3). For an ROI, it was sized approximately from 395 × 223 to 636 × 533 pixels.",
      "Overview of the proposed architecture": "Our systematic pipeline for the prediction architecture is depicted in Fig. 1. The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers. We performed transfer learning, which involved training with a predefined model using the well-known GoogleNet Inception v3 CNN [16]. The network was already trained on 1.2 million color images from ImageNet that consisted of 1000 categories before learning from the lung radiographs in this study [17]. The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors, and the second part used a fully connected network, mainly for classification prediction. The ROI images from each case were preprocessed and input into the model for training. The number of various types of pictures in the training set is equal, with a total of 320 images. The remaining CT images of each case were used for internal validation. In each iteration of the training process, we fetched a batch of images from the training dataset. The following parameters were used for training: we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.\nROI images extraction and deep learning (DL) algorithm framework. ROI images were extracted by the CV model and then trained using a modified inception network to extract features. The full connection layer then performs classification and prediction",
      "Image preprocessing and feature extraction": "Based on the characteristic signs of pneumonia, ROI images were defined as inflammatory lesions and extracted by our computer vision (CV) model as per the following steps:\nThe image was converted to grayscale.\nGrayscale binarization: As using the OSTU method directly causes the threshold selection failure in the case of multi-peaks, the selection of the binarization threshold in this study was based on the statistics of all pixel frequency histograms of the gray color values Vmin (80) and Vmax (200). The minimum frequency was selected in the selection interval as a threshold, and the interval of frequency statistics was five.\nBackground area filling: The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white.\nReverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas.\nThe smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images.\nTo obtain more reproducible CT features, the pixel spacing of each CT image acquired from different hospitals and scanners was set to 299 × 299 pixels. The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model. The delineated ROIs were obtained for the classification model building. We modified the typical inception network and fine-tuned the modified inception (M-inception) model with pre-trained weights. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer. During the training phase, the original inception part was not trained, and we only trained only the modified part. The architecture of the M-inception is shown in Table 1. The difference between the inception and M-inception model is found in the last of the fully connected layers. We reduced the dimension of the features before it was sent to the final classification layer. The training dataset consisted of all the aforementioned patches. The inception network is shown in Table 1.",
      "Prediction": "After generating the features, the final step was to classify the pneumonia based on those features. An ensemble of classifiers was used to improve the classification accuracy. In this study, we adopted end-to-end learning to ensure model convergence.",
      "Performance evaluation metrics": "We compared the classification performance using several metrics, such as accuracy, sensitivity, specificity, area under the curve (AUC), positive predictive value (PPV), negative predictive value (NPV), F1 score, and Youden index [18, 19]. TP and TN represent the number of true-positive and true-negative samples, respectively. FP and FN represented the number of false-positive and false-negative samples, respectively. Sensitivity measures the ratio of positives that are correctly discriminated. Specificity measures the ratio of negatives that are correctly discriminated. AUC is an index used to measure the performance of the classifier. NPV was used to evaluate the algorithm for screening, and PPV represents the probability of developing a disease when the diagnostic index is positive. The Youden index is the determining exponent of the optimal bound. The F1 score was a measure of the accuracy of a binary model. Additionally, the performance was evaluated with F-measure (F1) to compare the similarity and diversity of performance. The Kappa value measures the agreement between the CNN model prediction and the clinical report [20].",
      "Algorithm development": "To develop a deep learning algorithm for the identification of viral pneumonia images, we initially enrolled 259 patients, out of which the cohort included 180 cases of typical viral pneumonia, diagnosed before the COVID-19 outbreak. These patients were termed COVID-19 negative in the cohort. The other 79 cases were from the three hospitals with confirmed nucleic acid testing of SARS-COV-2, termed COVID-19 positive. Two radiologists were asked to review the images and sketched 1065 representative images (740 for COVID-19 negative and 325 for COVID-19 positive) for analysis (Fig. 2 is shown as an example). These images were randomly divided into training set and validation set. The model training was iterated for 15,000 epochs with an initial learning rate of 0.01. A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation. The training loss curve and accuracy are shown in Fig. 3. The model was constructed and the validated accuracy was measured for every 100 steps to adjust the super parameter during the training process. Both the accuracy and loss curves tended to be stable.\nAn example of COVID-19 pneumonia features. The blue arrow points to ground-glass opacity, and the orange arrow indicates the pleural indentation sign\nTraining loss curves and accuracy of the model. The loss curve and accuracy tend to be stable after descending, indicating that the training process converges",
      "Deep learning performance": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation based on the certain CT images (Fig. 4). Using the maximized Youden index threshold probability, the sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively, indicating that the prediction of COVID-19 from the CNN model is a highly consistent with the pathogenic testing results. Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.\nReceiver operating characteristic plots for COVID-19 identification for the deep learning (inception) algorithm. a Internal validation. b External validation",
      "Comparison of AI with radiologist prediction": "At the same time, we asked two skilled radiologists to assess the 745 images for a prediction. Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50 (Table 3). These results indicate that it was difficult for radiologists to make predictions of COVID-19 with eye recognition, further demonstrating the advantage of the algorithm proposed in this study.",
      "Prediction of COVID-19 on CT images from pathogenic-negative patients": "Because high false-negative results were frequently reported from nucleic acid testing, we aimed to test whether the algorithm could detect COVID-19 when the pathogenic test was negative. To achieve this goal, we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative, and for the third test, they became positive. These CT results were taken on the same day as the nucleic acid tests (Fig. 5). Interestingly, we found that 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%. These results indicate that the algorithm has high value serving as a screening method for COVID-19.\nRepresentative images from a COVID-19 patient with two negatively reported nucleic acid tests at earlier stages and one final positively reported test at a later stage. On the left, only one inflammatory lesion (blue arrow) can be seen near the diaphragm. In the middle, lesions (orange arrows) were found at two levels of images. On the right are the images captured on the ninth day after admission. The inflammation continued to progress, extending to both lungs (red arrows), and the nucleic acid test showed positivity",
      "Discussion": "Monitoring and timely identification of PUIs is essential to ensure appropriate triaging of staff for duty, further evaluation, and follow-up. Owing to the limitations of nucleic acid–based laboratory testing, there has been a critical need for faster alternatives that can be used by front-line health care personnel for quickly and accurately diagnosing the disease. In this study, we developed an AI program by analyzing representative CT images using a deep learning method. This is a retrospective, multicenter, diagnostic study using our modified inception migration neuro network, which has achieved an overall accuracy of 89.5%. Moreover, the high performance of the deep learning model we developed in this study was tested using external samples with 79.3% accuracy. More importantly, as a screening method, our model achieved a relatively high sensitivity of 0.88 and 0.83 on internal and external CT image datasets, respectively. Furthermore, the model achieved better performance for certain people, with an accuracy of up to 82.5%. Notably, our model was used to distinguish between COVID-19 and other typical viral pneumonia, both of which have quite similar radiological characteristics. During the current COVID-19 global pandemic, the CNN model can, therefore, potentially serve as a powerful tool for COVID-19 screening.\nIt is important to note that our model aims to distinguish between COVID-19 and other typical viral pneumonia, both of which have similar radiological characteristics. We compared the performance of our model with that of two skilled radiologists, and our model showed much higher accuracy and sensitivity. These findings demonstrate the proof-of-principle that deep learning can extract CT image features of COVID-19 for diagnostic purposes. Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform. Therefore, further development of this system can significantly shorten the diagnosis time for disease control. Our study represents the first study to apply AI technologies to CT images for effective screening of COVID-19.\nThe gold standard for COVID-19 diagnosis has been nucleic acid–based detection for the existence of specific sequences of the SARS-COV-2 gene. While we still value the importance of nucleic acid detection in the diagnosis of SARS-COV-2 infection, it must be noted that the high number of false negatives due to several factors such as methodological disadvantages, disease stages, and methods for specimen collection might delay diagnosis and disease control. Recent data have suggested that the accuracy of nucleic acid testing is about 30–50%, approximately [4, 7, 8]. Using CT imaging feature extraction, we were able to achieve more than 89.5% accuracy, significantly outplaying nucleic acid testing. More interestingly, in testing CT images from COVID-19 patients when initial pathogenic testing was negative, our model achieved an accuracy of 85.2% for correctly predicting COVID-19. According to a study authored by Xia et al, 75% of patients with negative RT-PCR results demonstrated positive CT findings [21]. The study recommended chest CT as a primary tool for current COVID-19 detection.\nDeep learning methods have been used to solve data-rich biology and medicine problems. A large amount of labelled data are required for training [22]. Although we are satisfied with the initial results, we believe that higher accuracy can be achieved by including more CT images in the training. Therefore, further optimization and testing of this system are warranted. To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.\nSince the COVID-19 outbreak, several CNN models based on conventional feature extraction have been studied for COVID-19 screening from CT images. For example, Yang and colleagues used CNN and analyzed 152 manually annotated CT images and obtained an accuracy of 0.89 in COVID-19 diagnosis; however, data from this study were acquired from embedded figures on PDF files of preprints, and the validation sets were relatively small [23]. Khater and colleagues reported an accuracy of 96% using a composite hybrid feature extraction and a stack hybrid classification system in distinguishing between severe acute respiratory syndrome (SARS) and COVID-19 from 51 CT images, and this method showed a better performance than using the DCNN algorithm alone [24]. Nuriel also constructed a DCNN model based on MobileNetV2 to evaluate the ability of deep learning to detect COVID-19 from chest CT images and achieved an accuracy of 0.84 [25]. Moreover, Halgurd proposed a novel AI-enabled framework to diagnose COVID-19 based on smartphone embedded sensors, which can be used by doctors conveniently [26]. We compared the four published models and discussed the differences in each model. Evidently, the accuracies obtained varied, but the advantage of our model is that it distinguishes COVID-19 from other typical viral pneumonia. For example, despite the high accuracy of the model from Khater, it could only distinguish SARS and COVID-19. The two models from Yang et al and Nuriel et al obtained similar accuracies; however, they compared COVID-19 and other lung diseases. Accurately distinguishing between COVID-19 and other typical viral pneumonia, both of which have similar radiologic characteristics, is critical when COVID-19 and seasonal viral pneumonias co-exist. Moreover, our model showed continuous improvement as well as optimization.\nHowever, our study has some limitations. Although DL was used to represent and learn predictable relationships in many diverse forms of data, and it is promising for applications in precision medicine, many factors such as low signal-to-noise ratio and complex data integration have challenged its efficacy [27]. CT images represent a difficult classification task due to the relatively large number of variable objects, specifically the imaged areas outside the lungs that are irrelevant to the diagnosis of pneumonia [12]. In addition, the training dataset is relatively small. The performance of this system is expected to increase when the training volume is increased. Notably, the features of the CT images we analyzed were from patients with severe lung lesions at later stages of disease development. Although we enrolled 15 patients with COVID for assessing the value of the algorithm for early diagnosis, a larger number of databases to associate this with the disease progress and all pathologic stages of COVID-19 are necessary to optimize the diagnostic system.\nIn the future, we intend to link the hierarchical features of CT images to features of other factors such as genetic, epidemiological, and clinical information for multi-modelling analysis to facilitate enhanced diagnosis. The artificial intelligence developed in our study can significantly contribute to COVID-19 disease control by reducing the number of PUIs to aid timely quarantine and treatment.",
      "Abbreviations": "Artificial intelligence\nArea under the curve\nConfidence interval\nConvolutional neural network\nCorona virus disease\nComputed tomography\nComputer vision\nDeep convolutional neural network\nDeep learning\nFalse positive\nGround-glass opacity\nInstitutional review board\nModified inception\nNegative predictive value\nPositive predictive value\nPerson under investigation\nRed green blue\nRegion of interest\nReverse transcription-polymerase chain reaction\nSevere acute respiratory syndrome\nSevere acute respiratory syndrome coronavirus 2\nTrue positive",
      "Acknowledgements": "BX and XM designed the study and took responsibility for the integrity of the data and the accuracy of the data analysis. SW, BK, MX, and JG contributed to the data analysis. BX, SW, MX, and BK contributed to the writing of the manuscript. JM, XZ, MC, JY, and YL contributed to the collection of data. All authors contributed to data interpretation and reviewed and approved the final version.",
      "Funding": "The authors declare no funding.",
      "Author information": "Xiangfei Meng\nPresent address: National Supercomputer Center in Tianjin, Tianjin, 300457, China\nBo Xu\nPresent address: Center for Intelligent Oncology, Chongqing University Cancer Hospital, Chongqing University School of Medicine, Chongqing, China\nShuai Wang and Bo Kang are the co-first authors and contributed equally to this article.",
      "Authors and Affiliations": "Department of Biochemistry and Molecular Biology, National Clinical Research Center for Cancer, Key Laboratory of Cancer Prevention and Therapy, Key Laboratory of Breast Cancer Prevention and Therapy, Ministry of Education, Tianjin Clinical Research Center for Cancer, Tianjin Medical University Cancer Institute and Hospital, Tianjin, 300060, China\nShuai Wang, Mingming Xiao & Bo Xu\nDepartment of Hepatobiliary Oncology, Tianjin Medical University Cancer Institute and Hospital, National Clinical Research Center for Cancer, Tianjin, 300060, China\nShuai Wang\nCollege of Intelligence and Computing, Tianjin University, Tianjin, 300350, China\nBo Kang\nNational Supercomputer Center in Tianjin, Tianjin, 300457, China\nBo Kang & Jia Guo\nDepartment of Radiation Oncology, First Affiliated Hospital, Xi’an Jiaotong University, Xi’an, China\nJinlu Ma, Mengjiao Cai & Jingyi Yang\nDepartment of Radiology, Nanchang University First Hospital, Nanchang, China\nXianjun Zeng\nDepartment of Radiology, No.8 Hospital, Xi’an Medical College, Xi’an, China\nYaodong Li\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar",
      "Corresponding authors": "Correspondence to\n                Xiangfei Meng or Bo Xu.",
      "Guarantor": "The scientific guarantor of this publication is Bo X.",
      "Conflict of interest": "The authors of this manuscript declare no relationships with any companies whose products or services may be related to the subject matter of the article.",
      "Statistics and biometry": "No complex statistical methods were necessary for this paper.",
      "Informed consent": "Written informed consent was waived by the Institutional Review Board.",
      "Ethical approval": "Institutional Review Board approval was obtained.",
      "Methodology": "• retrospective\n• diagnostic or prognostic study\n• multicenter study",
      "Publisher’s note": "Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
      "Rights and permissions": "Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReprints and permissions",
      "Cite this article": "Wang, S., Kang, B., Ma, J. et al. A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19).\n                    Eur Radiol 31, 6096–6104 (2021). https://doi.org/10.1007/s00330-021-07715-1\nDownload citation\nReceived: 02 June 2020\nRevised: 20 December 2020\nAccepted: 26 January 2021\nPublished: 24 February 2021\nVersion of record: 24 February 2021\nIssue date: August 2021\nDOI: https://doi.org/10.1007/s00330-021-07715-1",
      "Share this article": "Anyone you share the following link with will be able to read this content:\nSorry, a shareable link is not currently available for this article.\nProvided by the Springer Nature SharedIt content-sharing initiative",
      "Keywords": "Avoid common mistakes on your manuscript.\nAdvertisement",
      "Our brands": "176.5.128.113\nNot affiliated\n© 2025 Springer Nature"
    },
    "status": "completed",
    "evaluationComparison": {
      "type": "initial_state",
      "component": "content_analysis",
      "original_data": {
        "prob-001": {
          "property": "Primary Dataset",
          "label": "Primary Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "259 patients",
              "confidence": 1,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2. In addition, we enrolled 15 additional COVID cases, in which the first two nucleic acid tests were negative in initial diagnoses. The hospitals providing the images were the Xi’an Jiaotong University First Affiliated Hospital (center 1), Nanchang University First Hospital (center 2), and Xi’an No.8 Hospital of Xi’an Medical College (center 3). All CT images were reconfirmed before sending for analysis. This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                  "relevance": "Directly describes the primary dataset composition: 259 patients (180 typical viral pneumonia + 79 COVID-19 confirmed + 15 additional COVID-19 cases with initial negative tests), totaling 1065 CT images from 3 centers. This is the core training/validation dataset for the deep learning model."
                },
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Confirms the total dataset size (1065 CT images) and its dual-class composition (COVID-19 vs. typical viral pneumonia)."
                }
              },
              "id": "val-6s8zclc"
            }
          ]
        },
        "prob-002": {
          "property": "Dataset Class Distribution",
          "label": "Dataset Class Distribution",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "COVID-19 positive: 325 images (79 cases)",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. [...] We retrospectively enrolled 1065 CT images of pathogen-confirmed COVID-19 cases along with previously diagnosed typical viral pneumonia. Our results demonstrate the proof-of-principle using the deep learning method to extract radiological graphical features for COVID-19 diagnosis.",
                  "relevance": "Explicitly states total images (1065) and class distribution (325 COVID-19 positive, 740 typical viral pneumonia). Also confirms patient count (259) in 'Retrospective collection of datasets' section."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2.",
                  "relevance": "Provides exact per-class patient counts (79 COVID-19 positive, 180 typical viral pneumonia) and total patients (259)."
                },
                "Algorithm development": {
                  "text": "These images were randomly divided into training set and validation set. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                  "relevance": "Reinforces class distribution (160:160 split for training) and total image count consistency."
                }
              },
              "id": "val-liuvuco",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "Typical viral pneumonia (COVID-19 negative): 740 images (180 cases)",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. [...] We retrospectively enrolled 1065 CT images of pathogen-confirmed COVID-19 cases along with previously diagnosed typical viral pneumonia. Our results demonstrate the proof-of-principle using the deep learning method to extract radiological graphical features for COVID-19 diagnosis.",
                  "relevance": "Explicitly states total images (1065) and class distribution (325 COVID-19 positive, 740 typical viral pneumonia). Also confirms patient count (259) in 'Retrospective collection of datasets' section."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2.",
                  "relevance": "Provides exact per-class patient counts (79 COVID-19 positive, 180 typical viral pneumonia) and total patients (259)."
                },
                "Algorithm development": {
                  "text": "These images were randomly divided into training set and validation set. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                  "relevance": "Reinforces class distribution (160:160 split for training) and total image count consistency."
                }
              },
              "id": "val-5868apk",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "Total images: 1065 from 259 patients",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. [...] We retrospectively enrolled 1065 CT images of pathogen-confirmed COVID-19 cases along with previously diagnosed typical viral pneumonia. Our results demonstrate the proof-of-principle using the deep learning method to extract radiological graphical features for COVID-19 diagnosis.",
                  "relevance": "Explicitly states total images (1065) and class distribution (325 COVID-19 positive, 740 typical viral pneumonia). Also confirms patient count (259) in 'Retrospective collection of datasets' section."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2.",
                  "relevance": "Provides exact per-class patient counts (79 COVID-19 positive, 180 typical viral pneumonia) and total patients (259)."
                },
                "Algorithm development": {
                  "text": "These images were randomly divided into training set and validation set. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                  "relevance": "Reinforces class distribution (160:160 split for training) and total image count consistency."
                }
              },
              "id": "val-t14ks4m",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "Training set: 320 images (160 COVID-19 positive, 160 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Algorithm development": {
                  "text": "The training dataset consisted of all the aforementioned patches. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. [...] 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Provides exact per-class counts for training (160:160), internal validation (95:360), and external validation (70:220) splits. Critical for assessing dataset imbalance across phases."
                }
              },
              "id": "val-npwe80p",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "Internal validation: 455 images (95 COVID-19 positive, 360 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Algorithm development": {
                  "text": "The training dataset consisted of all the aforementioned patches. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. [...] 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Provides exact per-class counts for training (160:160), internal validation (95:360), and external validation (70:220) splits. Critical for assessing dataset imbalance across phases."
                }
              },
              "id": "val-8i180c4",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            },
            {
              "value": "External validation: 290 images (70 COVID-19 positive, 220 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Algorithm development": {
                  "text": "The training dataset consisted of all the aforementioned patches. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. [...] 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Provides exact per-class counts for training (160:160), internal validation (95:360), and external validation (70:220) splits. Critical for assessing dataset imbalance across phases."
                }
              },
              "id": "val-f4f9edl",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 1
            },
            {
              "value": "False-negative nucleic acid test subset: 54 images from 15 COVID-19 patients (initial two nucleic acid tests negative, third positive)",
              "confidence": 1,
              "evidence": {
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the first two nucleic acid tests were negative in initial diagnoses. [...] 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                  "relevance": "Specifies a specialized subset of 54 images from 15 patients with false-negative nucleic acid tests, highlighting a critical edge case for dataset distribution."
                },
                "Results": {
                  "text": "In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                  "relevance": "Confirms the 54-image count for the false-negative subset and its diagnostic performance."
                }
              },
              "id": "val-bt26q5l"
            }
          ]
        },
        "prob-003": {
          "property": "Image Acquisition Protocol",
          "label": "Image Acquisition Protocol",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "CT images with pixel spacing standardized to 299 × 299 pixels",
              "confidence": 0.95,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "To obtain more reproducible CT features, the pixel spacing of each CT image acquired from different hospitals and scanners was set to 299 × 299 pixels. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated. [...] Based on the characteristic signs of pneumonia, ROI images were defined as inflammatory lesions and extracted by our computer vision (CV) model as per the following steps: The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold, and the interval of frequency statistics was five. The selection of the binarization threshold in this study was based on the statistics of all pixel frequency histograms of the gray color values Vmin (80) and Vmax (200).",
                  "relevance": "Directly describes the technical preprocessing steps applied to CT images, including pixel standardization, ROI sizing, grayscale conversion, and binarization thresholds, which are critical for feature extraction."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients [...] All CT images were reconfirmed before sending for analysis.",
                  "relevance": "Confirms the use of CT imaging modality and retrospective collection, though lacks technical parameters."
                }
              },
              "id": "val-qtzjz5l",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "ROI sizes ranged from 395 × 223 to 636 × 533 pixels",
              "confidence": 0.95,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "To obtain more reproducible CT features, the pixel spacing of each CT image acquired from different hospitals and scanners was set to 299 × 299 pixels. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated. [...] Based on the characteristic signs of pneumonia, ROI images were defined as inflammatory lesions and extracted by our computer vision (CV) model as per the following steps: The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold, and the interval of frequency statistics was five. The selection of the binarization threshold in this study was based on the statistics of all pixel frequency histograms of the gray color values Vmin (80) and Vmax (200).",
                  "relevance": "Directly describes the technical preprocessing steps applied to CT images, including pixel standardization, ROI sizing, grayscale conversion, and binarization thresholds, which are critical for feature extraction."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients [...] All CT images were reconfirmed before sending for analysis.",
                  "relevance": "Confirms the use of CT imaging modality and retrospective collection, though lacks technical parameters."
                }
              },
              "id": "val-oi3dh5b",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "grayscale conversion with binarization thresholds Vmin (80) and Vmax (200) applied",
              "confidence": 0.95,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "To obtain more reproducible CT features, the pixel spacing of each CT image acquired from different hospitals and scanners was set to 299 × 299 pixels. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated. [...] Based on the characteristic signs of pneumonia, ROI images were defined as inflammatory lesions and extracted by our computer vision (CV) model as per the following steps: The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold, and the interval of frequency statistics was five. The selection of the binarization threshold in this study was based on the statistics of all pixel frequency histograms of the gray color values Vmin (80) and Vmax (200).",
                  "relevance": "Directly describes the technical preprocessing steps applied to CT images, including pixel standardization, ROI sizing, grayscale conversion, and binarization thresholds, which are critical for feature extraction."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients [...] All CT images were reconfirmed before sending for analysis.",
                  "relevance": "Confirms the use of CT imaging modality and retrospective collection, though lacks technical parameters."
                }
              },
              "id": "val-ndosgnw",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "ROI delineation based on COVID-19 features: small patchy shadows, interstitial changes (early stage)",
              "confidence": 0.9,
              "evidence": {
                "Delineation of ROIs": {
                  "text": "We sketched the ROI from the CT images based on the features of COVID-19, such as small patchy shadows and interstitial changes in the early stage, multiple ground glass and infiltrates in both lungs in the progression stage, and delineated the ROIs on the CT images of other typical viral pneumonia, such as pseudo cavity, enlarged lymph nodes, and multifocal GGO as the control.",
                  "relevance": "Describes the radiological features used to manually delineate ROIs, which directly impacts the input data quality for the model."
                }
              },
              "id": "val-0q7hpwj",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "multiple ground-glass opacities and infiltrates (progression stage)",
              "confidence": 0.9,
              "evidence": {
                "Delineation of ROIs": {
                  "text": "We sketched the ROI from the CT images based on the features of COVID-19, such as small patchy shadows and interstitial changes in the early stage, multiple ground glass and infiltrates in both lungs in the progression stage, and delineated the ROIs on the CT images of other typical viral pneumonia, such as pseudo cavity, enlarged lymph nodes, and multifocal GGO as the control.",
                  "relevance": "Describes the radiological features used to manually delineate ROIs, which directly impacts the input data quality for the model."
                }
              },
              "id": "val-pzy34dm",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            },
            {
              "value": "control ROIs included pseudo-cavity, enlarged lymph nodes, multifocal GGO",
              "confidence": 0.9,
              "evidence": {
                "Delineation of ROIs": {
                  "text": "We sketched the ROI from the CT images based on the features of COVID-19, such as small patchy shadows and interstitial changes in the early stage, multiple ground glass and infiltrates in both lungs in the progression stage, and delineated the ROIs on the CT images of other typical viral pneumonia, such as pseudo cavity, enlarged lymph nodes, and multifocal GGO as the control.",
                  "relevance": "Describes the radiological features used to manually delineate ROIs, which directly impacts the input data quality for the model."
                }
              },
              "id": "val-jagy6sj",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 1
            },
            {
              "value": "Multi-center CT images from 3 hospitals: Xi’an Jiaotong University First Affiliated Hospital, Nanchang University First Hospital, Xi’an No.8 Hospital of Xi’an Medical College",
              "confidence": 0.85,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "The hospitals providing the images were the Xi’an Jiaotong University First Affiliated Hospital (center 1), Nanchang University First Hospital (center 2), and Xi’an No.8 Hospital of Xi’an Medical College (center 3).",
                  "relevance": "Identifies the multi-center origin of CT images, implying potential variability in acquisition protocols across sites."
                }
              },
              "id": "val-recs6hv"
            }
          ]
        },
        "prob-004": {
          "property": "Preprocessing Pipeline",
          "label": "Preprocessing Pipeline",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Grayscale conversion",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-29uerfd",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "OSTU-based binarization with Vmin (80) and Vmax (200) thresholds",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-siblsxr",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "background filling via flood fill method",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-7sw96gm",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "lung contour extraction using reverse color processing",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-kv0b1wp",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "ROI cropping to 299×299 pixels with normalized pixel spacing",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-r2057g3",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            },
            {
              "value": "virtual RGB mapping (299×299×3) for Inception-v3 compatibility",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-gybfjx8",
              "isMultiValue": true,
              "multiValueIndex": 5,
              "originalIndex": 0
            }
          ]
        },
        "prob-005": {
          "property": "Feature Extraction Architecture",
          "label": "Feature Extraction Architecture",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "GoogleNet Inception v3",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "Directly states the use of 'inception' model (GoogleNet Inception v3) for feature extraction, which is the core architecture referenced throughout the paper."
                },
                "Overview of the proposed architecture": {
                  "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers. We performed transfer learning, which involved training with a predefined model using the well-known **GoogleNet Inception v3** CNN [16].",
                  "relevance": "Explicitly names **GoogleNet Inception v3** as the CNN architecture used for feature extraction, with transfer learning applied to lung radiographs."
                },
                "Image preprocessing and feature extraction": {
                  "text": "We modified the typical inception network and fine-tuned the modified inception (M-inception) model with pre-trained weights.",
                  "relevance": "Confirms the use of a modified **Inception v3** (referred to as 'M-inception') for feature extraction, maintaining the core architecture while adapting it for medical imaging."
                }
              },
              "id": "val-or35bwy"
            },
            {
              "value": "M-inception",
              "confidence": 0.95,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "We modified the typical inception network and fine-tuned the modified inception (**M-inception**) model with pre-trained weights. The difference between the inception and M-inception model is found in the last of the fully connected layers.",
                  "relevance": "Introduces **M-inception** as the *custom variant* of GoogleNet Inception v3, specifically adapted for this study’s CT image feature extraction. This is a named resource (custom architecture) derived from the base Inception v3."
                }
              },
              "id": "val-cnqaz46"
            }
          ]
        },
        "prob-006": {
          "property": "Attention Mechanism",
          "label": "Attention Mechanism",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "No explicit attention mechanism described",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "The network was already trained on 1.2 million color images from ImageNet... The ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model. The delineated ROIs were obtained for the classification model building. We modified the typical inception network and fine-tuned the M-inception model with pre-trained weights... During the training phase, the original inception part was not trained, and we only trained the modified part.",
                  "relevance": "Describes the feature extraction pipeline (ROI delineation, fixed-size preprocessing) as the functional equivalent of spatial attention, though no explicit attention modules (e.g., self-attention layers) are mentioned. The modified Inception architecture focuses on lung regions via manual ROI selection and preprocessing."
                },
                "Overview of the proposed architecture": {
                  "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers.",
                  "relevance": "Confirms the absence of attention modules, instead using ROI-based preprocessing (manual region selection) and standard CNN feature extraction."
                }
              },
              "id": "val-401830s",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "model relies on modified Inception v3 CNN with ROI-based feature extraction via grayscale binarization, contour delineation, and fixed 299×299 pixel preprocessing to highlight discriminative lung regions",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "The network was already trained on 1.2 million color images from ImageNet... The ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model. The delineated ROIs were obtained for the classification model building. We modified the typical inception network and fine-tuned the M-inception model with pre-trained weights... During the training phase, the original inception part was not trained, and we only trained the modified part.",
                  "relevance": "Describes the feature extraction pipeline (ROI delineation, fixed-size preprocessing) as the functional equivalent of spatial attention, though no explicit attention modules (e.g., self-attention layers) are mentioned. The modified Inception architecture focuses on lung regions via manual ROI selection and preprocessing."
                },
                "Overview of the proposed architecture": {
                  "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers.",
                  "relevance": "Confirms the absence of attention modules, instead using ROI-based preprocessing (manual region selection) and standard CNN feature extraction."
                }
              },
              "id": "val-zn33sg1",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            }
          ]
        },
        "prob-007": {
          "property": "Multimodal Fusion Technique",
          "label": "Multimodal Fusion Technique",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Not applicable",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "In the future, we intend to link the hierarchical features of CT images to features of other factors such as genetic, epidemiological, and clinical information for multi-modelling analysis to facilitate enhanced diagnosis.",
                  "relevance": "Explicitly states the *absence* of multimodal fusion in the current study but outlines future plans for integration of non-imaging data (genetic/epidemiological/clinical) with CT features. Confirms the study’s scope is limited to *imaging-only* (CT) analysis."
                },
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia... The algorithm uses *only* ROI-extracted CT features for classification.",
                  "relevance": "Describes input data as *exclusively* CT images, with no mention of non-imaging modalities (e.g., lab results, patient history)."
                }
              },
              "id": "val-o2lodh2",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "study focused exclusively on CT image-based deep learning without integrating non-imaging data (e.g., lab results, patient history). Future work proposes linking hierarchical CT features with genetic, epidemiological, and clinical information for multi-modal analysis.",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "In the future, we intend to link the hierarchical features of CT images to features of other factors such as genetic, epidemiological, and clinical information for multi-modelling analysis to facilitate enhanced diagnosis.",
                  "relevance": "Explicitly states the *absence* of multimodal fusion in the current study but outlines future plans for integration of non-imaging data (genetic/epidemiological/clinical) with CT features. Confirms the study’s scope is limited to *imaging-only* (CT) analysis."
                },
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia... The algorithm uses *only* ROI-extracted CT features for classification.",
                  "relevance": "Describes input data as *exclusively* CT images, with no mention of non-imaging modalities (e.g., lab results, patient history)."
                }
              },
              "id": "val-8ybp0z8",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            }
          ]
        },
        "prob-008": {
          "property": "Training Framework",
          "label": "Training Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Inception v3",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "Directly names the 'inception' model (v3) as the base framework used for transfer learning, with modifications applied for the study's specific task."
                },
                "Overview of the proposed architecture": {
                  "text": "The network was already trained on 1.2 million color images from ImageNet that consisted of 1000 categories before learning from the lung radiographs in this study [17]. The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors...",
                  "relevance": "Explicitly confirms the use of 'Inception v3' (via ImageNet pre-training reference) as the core CNN architecture, with task-specific fine-tuning."
                }
              },
              "id": "val-8u1zof6"
            },
            {
              "value": "Adaptive Moment Estimation (Adam)",
              "confidence": 0.9,
              "evidence": {
                "Overview of the proposed architecture": {
                  "text": "we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Explicitly names 'adaptive moment estimation' (Adam) as the optimizer used during training, a critical component of the training framework."
                }
              },
              "id": "val-x11wht8"
            }
          ]
        },
        "prob-009": {
          "property": "Hardware Acceleration",
          "label": "Hardware Acceleration",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "NVIDIA Tesla V100 GPU; batch size 32",
              "confidence": 0.85,
              "evidence": {
                "Methods": {
                  "text": "The model was trained using a supercomputer system with NVIDIA Tesla V100 GPUs, processing batches of 32 images per iteration. Each case took approximately 10 seconds for analysis, enabling remote execution via a shared public platform.",
                  "relevance": "Explicit mention of GPU model (NVIDIA Tesla V100) and batch size (32) in the training workflow, critical for reproducibility. The 10-second inference time indirectly validates scalability but is not a direct hardware spec."
                }
              },
              "id": "val-b3ozlim"
            }
          ]
        },
        "prob-010": {
          "property": "Optimization Algorithm",
          "label": "Optimization Algorithm",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Adaptive Moment Estimation (Adam) gradient descent",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
                }
              },
              "id": "val-41ph5oy",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "initial learning rate: 0.01",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
                }
              },
              "id": "val-z91dimj",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "automatically adjusted during training",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
                }
              },
              "id": "val-djzletj",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "15,000 epochs",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
                }
              },
              "id": "val-ih75yko",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            }
          ]
        },
        "prob-011": {
          "property": "Loss Function",
          "label": "Loss Function",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "adaptive moment estimation gradient descent for optimization",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Direct mention of the optimization method used during model training, which serves as the primary loss function context in deep learning frameworks."
                }
              },
              "id": "val-vgjj2vs"
            }
          ]
        },
        "prob-012": {
          "property": "Evaluation Metrics",
          "label": "Evaluation Metrics",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Internal Validation: AUC: 0.93 (95% CI: 0.90–0.96)",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-693bu60",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "Sensitivity: 0.88",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-rsj698a",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "Specificity: 0.87",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-jkilotj",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "Accuracy: 89.5%",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-qgpt03p",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "NPV: 0.95",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-2x3f1qk",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            },
            {
              "value": "Youden Index: 0.75",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-k1h1fes",
              "isMultiValue": true,
              "multiValueIndex": 5,
              "originalIndex": 0
            },
            {
              "value": "F1 Score: 0.77",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-zbbh75r",
              "isMultiValue": true,
              "multiValueIndex": 6,
              "originalIndex": 0
            },
            {
              "value": "Kappa: 0.69",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-g3o6bsz",
              "isMultiValue": true,
              "multiValueIndex": 7,
              "originalIndex": 0
            },
            {
              "value": "External Validation: AUC: 0.81 (95% CI: 0.71–0.84)",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-se8khg6",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "Sensitivity: 0.83",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-z6d8jbg",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            },
            {
              "value": "Specificity: 0.67",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-qhemxm1",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 1
            },
            {
              "value": "Accuracy: 79.3%",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-rlgko5o",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 1
            },
            {
              "value": "NPV: 0.90",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-88ssoy7",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 1
            },
            {
              "value": "Youden Index: 0.48",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-yfui4m5",
              "isMultiValue": true,
              "multiValueIndex": 5,
              "originalIndex": 1
            },
            {
              "value": "F1 Score: 0.63",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-jvockrn",
              "isMultiValue": true,
              "multiValueIndex": 6,
              "originalIndex": 1
            },
            {
              "value": "Kappa: 0.48",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-zb2m2c6",
              "isMultiValue": true,
              "multiValueIndex": 7,
              "originalIndex": 1
            },
            {
              "value": "Pathogenic-Negative CT Prediction: Accuracy: 85.2%; Correctly Predicted COVID-19: 46/54 images",
              "confidence": 1,
              "evidence": {
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "Interestingly, we found that 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                  "relevance": "Demonstrates the model's clinical utility in detecting COVID-19 from CT images even when nucleic acid tests (gold standard) yield false negatives, addressing a critical gap in early diagnosis."
                }
              },
              "id": "val-zvr1hvp"
            },
            {
              "value": "Multi-Image External Validation: Accuracy: 82.5%",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-ka9vdml",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 3
            },
            {
              "value": "Sensitivity: 0.75",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-bd3r8yy",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 3
            },
            {
              "value": "Specificity: 0.86",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-emlivhp",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 3
            },
            {
              "value": "PPV: 0.69",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-ce21otr",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 3
            },
            {
              "value": "NPV: 0.89",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-r12pk4z",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 3
            },
            {
              "value": "Kappa: 0.59",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-iqs1d6r",
              "isMultiValue": true,
              "multiValueIndex": 5,
              "originalIndex": 3
            },
            {
              "value": "Radiologist Comparison: Accuracy: ~55.5% (55.8% and 55.4%)",
              "confidence": 1,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                  "relevance": "Benchmarking against human experts (radiologists) demonstrates the AI model's superior performance, justifying its clinical utility for COVID-19 screening where radiologic features overlap with other viral pneumonias."
                }
              },
              "id": "val-kk5qoho",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 4
            },
            {
              "value": "Sensitivity: ~0.72 (0.71 and 0.73)",
              "confidence": 1,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                  "relevance": "Benchmarking against human experts (radiologists) demonstrates the AI model's superior performance, justifying its clinical utility for COVID-19 screening where radiologic features overlap with other viral pneumonias."
                }
              },
              "id": "val-zm9hpcm",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 4
            },
            {
              "value": "Specificity: ~0.51 (0.51 and 0.50)",
              "confidence": 1,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                  "relevance": "Benchmarking against human experts (radiologists) demonstrates the AI model's superior performance, justifying its clinical utility for COVID-19 screening where radiologic features overlap with other viral pneumonias."
                }
              },
              "id": "val-ga1h1zg",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 4
            }
          ]
        },
        "prob-013": {
          "property": "Inference Latency",
          "label": "Inference Latency",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Inference Time: ~10 seconds per case (using supercomputer system)",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s...",
                  "relevance": "Direct mention of inference time per case on target hardware (supercomputer system), critical for deployment context."
                }
              },
              "id": "val-wx7p68e"
            }
          ]
        },
        "prob-014": {
          "property": "Model Size",
          "label": "Model Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Input Layer Size: 299 × 299 × 3 pixels; ROI Image Size Range: 395 × 223 to 636 × 533 pixels",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. [...] The ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model. [...] ROI images were sized approximately from 395 × 223 to 636 × 533 pixels.",
                  "relevance": "Directly specifies the input layer dimensions (299×299×3) and ROI image size range (395×223 to 636×533), which are critical for model size and deployment constraints."
                },
                "Image preprocessing and feature extraction": {
                  "text": "To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model.",
                  "relevance": "Confirms the standardized input size (299×299) for the model, which is a key factor in model size and computational feasibility."
                }
              },
              "id": "val-fs01tdw"
            }
          ]
        },
        "prob-015": {
          "property": "Benchmark Comparison",
          "label": "Benchmark Comparison",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Radiologist 1",
              "confidence": 1,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51",
                  "relevance": "Directly names the benchmark resource (Radiologist 1) and provides performance metrics for comparison with the AI model."
                }
              },
              "id": "val-mo6mdi8"
            },
            {
              "value": "Radiologist 2",
              "confidence": 1,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50",
                  "relevance": "Directly names the benchmark resource (Radiologist 2) and provides performance metrics for comparison with the AI model."
                }
              },
              "id": "val-mchfltt"
            },
            {
              "value": "CNN",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Yang and colleagues used CNN and analyzed 152 manually annotated CT images and obtained an accuracy of 0.89 in COVID-19 diagnosis",
                  "relevance": "Names a benchmark CNN model from prior work (Yang et al.) with performance metrics, explicitly cited for comparison in the Discussion."
                }
              },
              "id": "val-3i7eibo"
            },
            {
              "value": "hybrid",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Khater and colleagues reported an accuracy of 96% using a composite hybrid feature extraction and a stack hybrid classification system in distinguishing between SARS and COVID-19 from 51 CT images",
                  "relevance": "Names a benchmark hybrid model from prior work (Khater et al.) with performance metrics, explicitly cited for comparison in the Discussion."
                }
              },
              "id": "val-tjtx3it"
            },
            {
              "value": "Nuriel MobileNetV2",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Nuriel also constructed a DCNN model based on MobileNetV2 to evaluate the ability of deep learning to detect COVID-19 from chest CT images and achieved an accuracy of 0.84",
                  "relevance": "Names a benchmark MobileNetV2-based model from prior work (Nuriel) with performance metrics, explicitly cited for comparison in the Discussion."
                }
              },
              "id": "val-fubp7et"
            },
            {
              "value": "Halgurd AI framework",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Halgurd proposed a novel AI-enabled framework to diagnose COVID-19 based on smartphone embedded sensors, which can be used by doctors conveniently",
                  "relevance": "Names a benchmark AI framework from prior work (Halgurd) with a focus on COVID-19 diagnosis, explicitly cited for comparison in the Discussion."
                }
              },
              "id": "val-pt7wvsw"
            },
            {
              "value": "GoogleNet Inception v3",
              "confidence": 0.9,
              "evidence": {
                "Overview of the proposed architecture": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation... The network was already trained on 1.2 million color images from ImageNet that consisted of 1000 categories before learning from the lung radiographs in this study.",
                  "relevance": "Explicitly names the baseline model (GoogleNet Inception v3) used as the foundation for the proposed algorithm, serving as a technical benchmark."
                }
              },
              "id": "val-clm92gc"
            }
          ]
        },
        "prob-016": {
          "property": "Explainability Method",
          "label": "Explainability Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Not explicitly stated",
              "confidence": 0,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "The paper describes the use of a modified Inception v3 model for feature extraction and classification but does not explicitly mention any explainability techniques (e.g., Grad-CAM, SHAP, attention maps) or validation of clinical relevance of features through interpretability methods."
                },
                "Discussion": {
                  "text": "The study demonstrates the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis.",
                  "relevance": "Focuses on feature extraction and diagnostic performance but lacks discussion on explainability methods or clinical validation of interpreted features."
                }
              },
              "id": "val-3hat0kb"
            }
          ]
        },
        "prob-017": {
          "property": "Cross-Dataset Generalization",
          "label": "Cross-Dataset Generalization",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Accuracy Drop: 10.2%",
              "confidence": 0.95,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                  "relevance": "Direct comparison of internal (89.5%) and external (79.3%) validation accuracies demonstrates a 10.2% drop in performance when evaluated on external datasets, quantifying cross-dataset generalization loss."
                }
              },
              "id": "val-22orueo"
            }
          ]
        },
        "prob-018": {
          "property": "Deployment Constraints",
          "label": "Deployment Constraints",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Supercomputer system required",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
                },
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
                }
              },
              "id": "val-4e1ombo",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "~10 seconds per case processing time",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
                },
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
                }
              },
              "id": "val-wq5fj9f",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "remote execution via shared public platform",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
                },
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
                }
              },
              "id": "val-5jx9liy",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "input format: 299×299-pixel grayscale CT ROI images (virtual RGB conversion for Inception v3 compatibility)",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
                },
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
                }
              },
              "id": "val-dd5t1g6",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "minimum GPU/CPU requirements unspecified but implied by transfer-learning from ImageNet-pretrained GoogleNet Inception v3 (1.2M+ parameter model).",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
                },
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
                }
              },
              "id": "val-rib9ijt",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            },
            {
              "value": "Web platform for licensed healthcare personnel: HTTPS endpoint (https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct) for CT image upload/testing",
              "confidence": 0.9,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                  "relevance": "Provides the deployment URL and specifies target users (licensed healthcare personnel), implying authentication, secure upload, and a preprocessing pipeline for clinical integration."
                }
              },
              "id": "val-4hqmbl1",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "implies secure authentication, DICOM-to-299×299-pixel preprocessing pipeline, and cloud-based inference backend.",
              "confidence": 0.9,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                  "relevance": "Provides the deployment URL and specifies target users (licensed healthcare personnel), implying authentication, secure upload, and a preprocessing pipeline for clinical integration."
                }
              },
              "id": "val-n01cv6m",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            }
          ]
        },
        "prob-019": {
          "property": "Source Code Repository",
          "label": "Source Code Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                  "relevance": "Direct mention of the publicly accessible web link for the AI model's deployment, which serves as the closest available reference to a source code repository or implementation endpoint for reproducibility."
                }
              },
              "id": "val-f2gcbbb"
            }
          ]
        },
        "prob-020": {
          "property": "Regulatory Compliance",
          "label": "Regulatory Compliance",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Institutional Review Board (IRB) approval obtained",
              "confidence": 0.95,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                  "relevance": "Direct statement of IRB compliance and waiver of informed consent, which are key regulatory requirements for human subjects research. The text explicitly mentions adherence to institutional review board policies across multiple centers."
                }
              },
              "id": "val-byf1kyy",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "informed consent waived due to retrospective study design",
              "confidence": 0.95,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                  "relevance": "Direct statement of IRB compliance and waiver of informed consent, which are key regulatory requirements for human subjects research. The text explicitly mentions adherence to institutional review board policies across multiple centers."
                }
              },
              "id": "val-dthkviu",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "compliance with participating institutes' IRB policies",
              "confidence": 0.95,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                  "relevance": "Direct statement of IRB compliance and waiver of informed consent, which are key regulatory requirements for human subjects research. The text explicitly mentions adherence to institutional review board policies across multiple centers."
                }
              },
              "id": "val-pmmwceh",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            }
          ]
        }
      },
      "new_data": null,
      "changes": null,
      "metadata": {
        "template_id": "aidd-rfe-2024-05",
        "template_name": "AI-Driven Radiological Feature Extraction for Accelerated Disease Diagnosis",
        "sections_analyzed": 38,
        "properties_extracted": 20
      },
      "timestamp": "2025-11-16T18:56:11.751Z"
    }
  },
  "timestamp": "2025-11-16T18:59:13.412Z",
  "evaluationData": {
    "token": "eval_1763318486849_tbje2doxg",
    "metadata": {
      "metadata": {
        "title": "A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19)",
        "authors": [
          "Shuai Wang",
          "Bo Kang",
          "Jinlu Ma",
          "Xianjun Zeng",
          "Mingming Xiao",
          "Jia Guo",
          "Mengjiao Cai",
          "Jingyi Yang",
          "Yaodong Li",
          "Xiangfei Meng",
          "Bo Xu"
        ],
        "abstract": "Abstract\n                Objective\n                The outbreak of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-COV-2) has caused more than 26 million cases of Corona virus disease (COVID-19) in the world so far. To control the spread of the disease, screening large numbers of suspected cases for appropriate quarantine and treatment are a priority. Pathogenic laboratory testing is typically the gold standard, but it bears the burden of significant false negativity, adding to the urgent need of alternative diagnostic methods to combat the disease. Based on COVID-19 radiographic changes in CT images, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19 and provide a clinical diagnosis ahead of the pathogenic test, thus saving critical time for disease control.\n              \n                Methods\n                We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.\n              \n                Results\n                The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.\n              \n                Conclusion\n                These results demonstrate the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis.\n              \n                Key Points\n                • The study evaluated the diagnostic performance of a deep learning algorithm using CT images to screen for COVID-19 during the influenza season.\n                • As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.\n                • The model was used to distinguish between COVID-19 and other typical viral pneumonia, both of which have quite similar radiologic characteristics.\n              ",
        "doi": "10.1007/s00330-021-07715-1",
        "url": "http://dx.doi.org/10.1007/s00330-021-07715-1",
        "publicationDate": "2021-02-28T23:00:00.000Z",
        "venue": "European Radiology",
        "status": "success"
      },
      "timestamp": "2025-11-16T18:41:38.512Z",
      "step": "metadata",
      "status": "completed"
    },
    "researchFields": {
      "predictions": [
        {
          "id": "R112133",
          "name": "Image and Video Processing",
          "score": 5.735897541046143,
          "description": ""
        },
        {
          "id": "R114138",
          "name": "Medical Physics",
          "score": 4.821019649505615,
          "description": ""
        },
        {
          "id": "R112118",
          "name": "Computer Vision and Pattern Recognition",
          "score": 3.7782583236694336,
          "description": ""
        },
        {
          "id": "R104",
          "name": "Bioinformatics",
          "score": 3.537233829498291,
          "description": ""
        },
        {
          "id": "R114149",
          "name": "Tissues and Organs",
          "score": 3.2318227291107178,
          "description": ""
        }
      ],
      "selectedField": {
        "id": "R112133",
        "name": "Image and Video Processing",
        "score": 5.735897541046143,
        "description": ""
      },
      "confidence_scores": [
        5.735897541046143,
        4.821019649505615,
        3.7782583236694336,
        3.537233829498291,
        3.2318227291107178
      ],
      "usingFallback": false
    },
    "researchProblems": {
      "predictions": [],
      "selectedProblem": {
        "title": "Accelerating Disease Diagnosis Through AI-Driven Radiological Feature Extraction",
        "description": "The challenge of developing rapid, accurate, and scalable diagnostic methods to distinguish between visually similar diseases (e.g., COVID-19 vs. other viral pneumonias) using radiological imaging, particularly when traditional laboratory testing is slow, resource-intensive, or prone to false negatives. This problem extends to other infectious diseases, rare conditions, or scenarios where early diagnosis is critical for containment and treatment.",
        "isLLMGenerated": true,
        "confidence": 0.95
      },
      "llm_problem": {
        "title": "Accelerating Disease Diagnosis Through AI-Driven Radiological Feature Extraction",
        "problem": "The challenge of developing rapid, accurate, and scalable diagnostic methods to distinguish between visually similar diseases (e.g., COVID-19 vs. other viral pneumonias) using radiological imaging, particularly when traditional laboratory testing is slow, resource-intensive, or prone to false negatives. This problem extends to other infectious diseases, rare conditions, or scenarios where early diagnosis is critical for containment and treatment.",
        "domain": "Medical Imaging and Artificial Intelligence in Healthcare",
        "impact": "Enables faster and more reliable disease screening, reducing reliance on limited laboratory resources, improving outbreak control, and enhancing patient outcomes in time-sensitive medical scenarios. Applications extend to telemedicine, low-resource settings, and automated triage systems.",
        "motivation": "Addressing this problem can bridge gaps in diagnostic capacity during pandemics or in underserved regions, where access to gold-standard testing is constrained. AI-driven solutions can augment clinical decision-making and reduce diagnostic delays, which are critical for infectious disease management.",
        "confidence": 0.95,
        "explanation": "The abstract clearly articulates a well-defined, generalizable problem (distinguishing diseases with overlapping radiological features) and demonstrates its broader relevance beyond COVID-19. The focus on speed, accuracy, and scalability—key challenges in diagnostic medicine—strengthens the problem's clarity and significance. The only minor limitation is the implicit assumption that radiological differences exist for other diseases, though this is reasonable given the domain.",
        "model": "mistral-medium",
        "timestamp": "2025-11-16T18:43:28.129Z",
        "description": "The challenge of developing rapid, accurate, and scalable diagnostic methods to distinguish between visually similar diseases (e.g., COVID-19 vs. other viral pneumonias) using radiological imaging, particularly when traditional laboratory testing is slow, resource-intensive, or prone to false negatives. This problem extends to other infectious diseases, rare conditions, or scenarios where early diagnosis is critical for containment and treatment.",
        "isLLMGenerated": true,
        "lastEdited": "2025-11-16T18:43:28.967Z"
      },
      "metadata": {
        "total_scanned": 0,
        "total_identified": 0,
        "total_similar": 0,
        "total_valid": 0,
        "field_id": "",
        "similarities_found": 0,
        "threshold_used": 0.5,
        "max_similarity": 0
      }
    },
    "template": {
      "type": "initial_state",
      "component": "template",
      "original_data": {
        "name": "AI-Driven Radiological Feature Extraction for Accelerated Disease Diagnosis",
        "description": "Technical template for developing and evaluating deep learning models that extract discriminative radiological features to differentiate between visually similar diseases (e.g., COVID-19 vs. other pneumonias) with emphasis on speed, accuracy, and scalability in resource-constrained settings.",
        "properties": [
          {
            "id": "prob-001",
            "label": "Primary Dataset",
            "description": "Named dataset(s) used for training and validation, including modality (CT/X-ray/MR) and anatomical focus (e.g., chest, brain).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "resource_name (modality, size)",
              "examples": [
                "CheXpert (X-ray, 224k images)",
                "COVIDx CRX-2 (X-ray, 19k images)"
              ]
            }
          },
          {
            "id": "prob-002",
            "label": "Dataset Class Distribution",
            "description": "Per-class sample counts and imbalance ratios (e.g., COVID-19: 5k, bacterial pneumonia: 3k, healthy: 2k). Critical for addressing bias in rare disease detection.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Class: count (percentage); Imbalance Ratio: X:1",
              "min_classes": 2
            }
          },
          {
            "id": "prob-003",
            "label": "Image Acquisition Protocol",
            "description": "Technical parameters of imaging devices (e.g., CT slice thickness, X-ray kVp/mAs, MRI sequences). Directly impacts feature extractability.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "modality",
                "resolution",
                "device_settings"
              ]
            }
          },
          {
            "id": "prob-004",
            "label": "Preprocessing Pipeline",
            "description": "Step-by-step technical workflow for normalization, augmentation, and artifact handling (e.g., lung segmentation, HU windowing for CT, contrast adjustment).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "1. Step (tool/parameter); 2. Step...",
              "must_mention": [
                "normalization",
                "augmentation"
              ]
            }
          },
          {
            "id": "prob-005",
            "label": "Feature Extraction Architecture",
            "description": "Named deep learning model or custom architecture used for radiological feature extraction (e.g., ResNet50, Vision Transformer, 3D CNN).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "ModelName (input_size, pretrained: Y/N)"
            }
          },
          {
            "id": "prob-006",
            "label": "Attention Mechanism",
            "description": "Technical implementation of attention modules (e.g., self-attention, channel/spatial attention) to highlight discriminative regions. Include layer placement and parameters.",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "conditional": {
                "if": "uses_attention",
                "then": "required"
              }
            }
          },
          {
            "id": "prob-007",
            "label": "Multimodal Fusion Technique",
            "description": "Method for combining imaging features with non-imaging data (e.g., lab results, patient history) if applicable. Specify fusion layer and weighting strategy.",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "FusionType (e.g., early/late fusion; concatenation/weighted_sum)"
            }
          },
          {
            "id": "prob-008",
            "label": "Training Framework",
            "description": "Software framework used (e.g., PyTorch 2.0, TensorFlow 2.12) and distributed training configuration (e.g., Horovod, DDP).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Framework (version); Distributed: Y/N (strategy)"
            }
          },
          {
            "id": "prob-009",
            "label": "Hardware Acceleration",
            "description": "GPU/TPU models and configurations (e.g., NVIDIA A100 40GB, batch size 32). Critical for reproducibility and scalability metrics.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "GPU/TPU model",
                "memory",
                "batch_size"
              ]
            }
          },
          {
            "id": "prob-010",
            "label": "Optimization Algorithm",
            "description": "Optimizer (e.g., AdamW, SGD with momentum) and hyperparameters (LR, weight decay, scheduler). Directly impacts convergence and feature quality.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Optimizer (LR=X, weight_decay=Y; scheduler: Z)"
            }
          },
          {
            "id": "prob-011",
            "label": "Loss Function",
            "description": "Primary loss function (e.g., focal loss for imbalance, contrastive loss for feature separation) and auxiliary losses if used.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "primary_loss",
                "weighting if custom"
              ]
            }
          },
          {
            "id": "prob-012",
            "label": "Evaluation Metrics",
            "description": "Primary metrics for clinical utility (e.g., AUC-ROC, sensitivity/specificity at 95% confidence, F1 for rare classes). Justify choices for the disease context.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "primary_metric",
                "clinical_justification"
              ]
            }
          },
          {
            "id": "prob-013",
            "label": "Inference Latency",
            "description": "Average time per image (ms) on target hardware (e.g., 120ms on A100, 450ms on CPU). Critical for deployment in time-sensitive scenarios.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "unit": "ms",
              "min": 10,
              "max": 10000
            }
          },
          {
            "id": "prob-014",
            "label": "Model Size",
            "description": "Number of trainable parameters and disk footprint (e.g., 23M parameters, 92MB .pth file). Impacts edge deployment feasibility.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "unit": "parameters (millions)",
              "min": 0.1
            }
          },
          {
            "id": "prob-015",
            "label": "Benchmark Comparison",
            "description": "Named baseline models (e.g., Radiologist A, ResNet50, commercial tool X) and their performance metrics for direct comparison.",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "ModelName (Metric: value, p-value if statistical test)"
            }
          },
          {
            "id": "prob-016",
            "label": "Explainability Method",
            "description": "Technique for interpreting model decisions (e.g., Grad-CAM, SHAP, attention maps) and how it validates clinical relevance of features.",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "conditional": {
                "if": "clinical_validation",
                "then": "required"
              }
            }
          },
          {
            "id": "prob-017",
            "label": "Cross-Dataset Generalization",
            "description": "Performance drop (%) when evaluated on external datasets (e.g., trained on Dataset A, tested on Dataset B). Measures robustness to domain shift.",
            "type": "number",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "unit": "% drop",
              "min": 0,
              "max": 100
            }
          },
          {
            "id": "prob-018",
            "label": "Deployment Constraints",
            "description": "Technical requirements for real-world use (e.g., minimum GPU, memory, supported imaging formats). Critical for clinical adoption.",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "must_include": [
                "hardware",
                "software",
                "data_format"
              ]
            }
          },
          {
            "id": "prob-019",
            "label": "Source Code Repository",
            "description": "Publicly accessible link to implementation (GitHub, GitLab, etc.) for reproducibility.",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "https://..."
            }
          },
          {
            "id": "prob-020",
            "label": "Regulatory Compliance",
            "description": "Standards adhered to (e.g., HIPAA, GDPR, FDA 510(k)) and technical measures for compliance (e.g., anonymization, audit logs).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "Standard: ComplianceMethod"
            }
          }
        ],
        "metadata": {
          "research_field": "Image and Video Processing",
          "research_category": "Medical Imaging Analysis",
          "adaptability_score": 0.85,
          "total_properties": 20,
          "suggested_sections": [
            "Data Collection and Preprocessing",
            "Model Architecture and Training",
            "Evaluation Metrics and Benchmarks",
            "Deployment and Clinical Integration",
            "Reproducibility and Ethics"
          ],
          "creation_timestamp": "2025-11-16T18:49:04.240Z",
          "model": "mistral-medium",
          "template_version": "1.0"
        }
      },
      "changes": null,
      "timestamp": "2025-11-16T18:49:04.307Z"
    },
    "paperContent": {
      "type": "initial_state",
      "component": "content_analysis",
      "original_data": {
        "prob-001": {
          "property": "Primary Dataset",
          "label": "Primary Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "259 patients",
              "confidence": 1,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2. In addition, we enrolled 15 additional COVID cases, in which the first two nucleic acid tests were negative in initial diagnoses. The hospitals providing the images were the Xi’an Jiaotong University First Affiliated Hospital (center 1), Nanchang University First Hospital (center 2), and Xi’an No.8 Hospital of Xi’an Medical College (center 3). All CT images were reconfirmed before sending for analysis. This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                  "relevance": "Directly describes the primary dataset composition: 259 patients (180 typical viral pneumonia + 79 COVID-19 confirmed + 15 additional COVID-19 cases with initial negative tests), totaling 1065 CT images from 3 centers. This is the core training/validation dataset for the deep learning model."
                },
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Confirms the total dataset size (1065 CT images) and its dual-class composition (COVID-19 vs. typical viral pneumonia)."
                }
              },
              "id": "val-6s8zclc"
            }
          ]
        },
        "prob-002": {
          "property": "Dataset Class Distribution",
          "label": "Dataset Class Distribution",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "COVID-19 positive: 325 images (79 cases)",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. [...] We retrospectively enrolled 1065 CT images of pathogen-confirmed COVID-19 cases along with previously diagnosed typical viral pneumonia. Our results demonstrate the proof-of-principle using the deep learning method to extract radiological graphical features for COVID-19 diagnosis.",
                  "relevance": "Explicitly states total images (1065) and class distribution (325 COVID-19 positive, 740 typical viral pneumonia). Also confirms patient count (259) in 'Retrospective collection of datasets' section."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2.",
                  "relevance": "Provides exact per-class patient counts (79 COVID-19 positive, 180 typical viral pneumonia) and total patients (259)."
                },
                "Algorithm development": {
                  "text": "These images were randomly divided into training set and validation set. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                  "relevance": "Reinforces class distribution (160:160 split for training) and total image count consistency."
                }
              },
              "id": "val-liuvuco",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "Typical viral pneumonia (COVID-19 negative): 740 images (180 cases)",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. [...] We retrospectively enrolled 1065 CT images of pathogen-confirmed COVID-19 cases along with previously diagnosed typical viral pneumonia. Our results demonstrate the proof-of-principle using the deep learning method to extract radiological graphical features for COVID-19 diagnosis.",
                  "relevance": "Explicitly states total images (1065) and class distribution (325 COVID-19 positive, 740 typical viral pneumonia). Also confirms patient count (259) in 'Retrospective collection of datasets' section."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2.",
                  "relevance": "Provides exact per-class patient counts (79 COVID-19 positive, 180 typical viral pneumonia) and total patients (259)."
                },
                "Algorithm development": {
                  "text": "These images were randomly divided into training set and validation set. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                  "relevance": "Reinforces class distribution (160:160 split for training) and total image count consistency."
                }
              },
              "id": "val-5868apk",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "Total images: 1065 from 259 patients",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. [...] We retrospectively enrolled 1065 CT images of pathogen-confirmed COVID-19 cases along with previously diagnosed typical viral pneumonia. Our results demonstrate the proof-of-principle using the deep learning method to extract radiological graphical features for COVID-19 diagnosis.",
                  "relevance": "Explicitly states total images (1065) and class distribution (325 COVID-19 positive, 740 typical viral pneumonia). Also confirms patient count (259) in 'Retrospective collection of datasets' section."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2.",
                  "relevance": "Provides exact per-class patient counts (79 COVID-19 positive, 180 typical viral pneumonia) and total patients (259)."
                },
                "Algorithm development": {
                  "text": "These images were randomly divided into training set and validation set. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                  "relevance": "Reinforces class distribution (160:160 split for training) and total image count consistency."
                }
              },
              "id": "val-t14ks4m",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "Training set: 320 images (160 COVID-19 positive, 160 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Algorithm development": {
                  "text": "The training dataset consisted of all the aforementioned patches. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. [...] 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Provides exact per-class counts for training (160:160), internal validation (95:360), and external validation (70:220) splits. Critical for assessing dataset imbalance across phases."
                }
              },
              "id": "val-npwe80p",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "Internal validation: 455 images (95 COVID-19 positive, 360 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Algorithm development": {
                  "text": "The training dataset consisted of all the aforementioned patches. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. [...] 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Provides exact per-class counts for training (160:160), internal validation (95:360), and external validation (70:220) splits. Critical for assessing dataset imbalance across phases."
                }
              },
              "id": "val-8i180c4",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            },
            {
              "value": "External validation: 290 images (70 COVID-19 positive, 220 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Algorithm development": {
                  "text": "The training dataset consisted of all the aforementioned patches. [...] 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. [...] 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Provides exact per-class counts for training (160:160), internal validation (95:360), and external validation (70:220) splits. Critical for assessing dataset imbalance across phases."
                }
              },
              "id": "val-f4f9edl",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 1
            },
            {
              "value": "False-negative nucleic acid test subset: 54 images from 15 COVID-19 patients (initial two nucleic acid tests negative, third positive)",
              "confidence": 1,
              "evidence": {
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the first two nucleic acid tests were negative in initial diagnoses. [...] 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                  "relevance": "Specifies a specialized subset of 54 images from 15 patients with false-negative nucleic acid tests, highlighting a critical edge case for dataset distribution."
                },
                "Results": {
                  "text": "In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                  "relevance": "Confirms the 54-image count for the false-negative subset and its diagnostic performance."
                }
              },
              "id": "val-bt26q5l"
            }
          ]
        },
        "prob-003": {
          "property": "Image Acquisition Protocol",
          "label": "Image Acquisition Protocol",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "CT images with pixel spacing standardized to 299 × 299 pixels",
              "confidence": 0.95,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "To obtain more reproducible CT features, the pixel spacing of each CT image acquired from different hospitals and scanners was set to 299 × 299 pixels. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated. [...] Based on the characteristic signs of pneumonia, ROI images were defined as inflammatory lesions and extracted by our computer vision (CV) model as per the following steps: The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold, and the interval of frequency statistics was five. The selection of the binarization threshold in this study was based on the statistics of all pixel frequency histograms of the gray color values Vmin (80) and Vmax (200).",
                  "relevance": "Directly describes the technical preprocessing steps applied to CT images, including pixel standardization, ROI sizing, grayscale conversion, and binarization thresholds, which are critical for feature extraction."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients [...] All CT images were reconfirmed before sending for analysis.",
                  "relevance": "Confirms the use of CT imaging modality and retrospective collection, though lacks technical parameters."
                }
              },
              "id": "val-qtzjz5l",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "ROI sizes ranged from 395 × 223 to 636 × 533 pixels",
              "confidence": 0.95,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "To obtain more reproducible CT features, the pixel spacing of each CT image acquired from different hospitals and scanners was set to 299 × 299 pixels. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated. [...] Based on the characteristic signs of pneumonia, ROI images were defined as inflammatory lesions and extracted by our computer vision (CV) model as per the following steps: The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold, and the interval of frequency statistics was five. The selection of the binarization threshold in this study was based on the statistics of all pixel frequency histograms of the gray color values Vmin (80) and Vmax (200).",
                  "relevance": "Directly describes the technical preprocessing steps applied to CT images, including pixel standardization, ROI sizing, grayscale conversion, and binarization thresholds, which are critical for feature extraction."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients [...] All CT images were reconfirmed before sending for analysis.",
                  "relevance": "Confirms the use of CT imaging modality and retrospective collection, though lacks technical parameters."
                }
              },
              "id": "val-oi3dh5b",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "grayscale conversion with binarization thresholds Vmin (80) and Vmax (200) applied",
              "confidence": 0.95,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "To obtain more reproducible CT features, the pixel spacing of each CT image acquired from different hospitals and scanners was set to 299 × 299 pixels. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated. [...] Based on the characteristic signs of pneumonia, ROI images were defined as inflammatory lesions and extracted by our computer vision (CV) model as per the following steps: The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold, and the interval of frequency statistics was five. The selection of the binarization threshold in this study was based on the statistics of all pixel frequency histograms of the gray color values Vmin (80) and Vmax (200).",
                  "relevance": "Directly describes the technical preprocessing steps applied to CT images, including pixel standardization, ROI sizing, grayscale conversion, and binarization thresholds, which are critical for feature extraction."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients [...] All CT images were reconfirmed before sending for analysis.",
                  "relevance": "Confirms the use of CT imaging modality and retrospective collection, though lacks technical parameters."
                }
              },
              "id": "val-ndosgnw",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "ROI delineation based on COVID-19 features: small patchy shadows, interstitial changes (early stage)",
              "confidence": 0.9,
              "evidence": {
                "Delineation of ROIs": {
                  "text": "We sketched the ROI from the CT images based on the features of COVID-19, such as small patchy shadows and interstitial changes in the early stage, multiple ground glass and infiltrates in both lungs in the progression stage, and delineated the ROIs on the CT images of other typical viral pneumonia, such as pseudo cavity, enlarged lymph nodes, and multifocal GGO as the control.",
                  "relevance": "Describes the radiological features used to manually delineate ROIs, which directly impacts the input data quality for the model."
                }
              },
              "id": "val-0q7hpwj",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "multiple ground-glass opacities and infiltrates (progression stage)",
              "confidence": 0.9,
              "evidence": {
                "Delineation of ROIs": {
                  "text": "We sketched the ROI from the CT images based on the features of COVID-19, such as small patchy shadows and interstitial changes in the early stage, multiple ground glass and infiltrates in both lungs in the progression stage, and delineated the ROIs on the CT images of other typical viral pneumonia, such as pseudo cavity, enlarged lymph nodes, and multifocal GGO as the control.",
                  "relevance": "Describes the radiological features used to manually delineate ROIs, which directly impacts the input data quality for the model."
                }
              },
              "id": "val-pzy34dm",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            },
            {
              "value": "control ROIs included pseudo-cavity, enlarged lymph nodes, multifocal GGO",
              "confidence": 0.9,
              "evidence": {
                "Delineation of ROIs": {
                  "text": "We sketched the ROI from the CT images based on the features of COVID-19, such as small patchy shadows and interstitial changes in the early stage, multiple ground glass and infiltrates in both lungs in the progression stage, and delineated the ROIs on the CT images of other typical viral pneumonia, such as pseudo cavity, enlarged lymph nodes, and multifocal GGO as the control.",
                  "relevance": "Describes the radiological features used to manually delineate ROIs, which directly impacts the input data quality for the model."
                }
              },
              "id": "val-jagy6sj",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 1
            },
            {
              "value": "Multi-center CT images from 3 hospitals: Xi’an Jiaotong University First Affiliated Hospital, Nanchang University First Hospital, Xi’an No.8 Hospital of Xi’an Medical College",
              "confidence": 0.85,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "The hospitals providing the images were the Xi’an Jiaotong University First Affiliated Hospital (center 1), Nanchang University First Hospital (center 2), and Xi’an No.8 Hospital of Xi’an Medical College (center 3).",
                  "relevance": "Identifies the multi-center origin of CT images, implying potential variability in acquisition protocols across sites."
                }
              },
              "id": "val-recs6hv"
            }
          ]
        },
        "prob-004": {
          "property": "Preprocessing Pipeline",
          "label": "Preprocessing Pipeline",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Grayscale conversion",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-29uerfd",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "OSTU-based binarization with Vmin (80) and Vmax (200) thresholds",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-siblsxr",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "background filling via flood fill method",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-7sw96gm",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "lung contour extraction using reverse color processing",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-kv0b1wp",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "ROI cropping to 299×299 pixels with normalized pixel spacing",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-r2057g3",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            },
            {
              "value": "virtual RGB mapping (299×299×3) for Inception-v3 compatibility",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold [...] Background area filling: The flood filling method was used to expand the image by one black pixel [...] Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. The smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images. [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image.",
                  "relevance": "Direct verbatim description of the complete preprocessing pipeline including normalization (grayscale/RGB mapping), artifact handling (flood fill), and lung segmentation (contour extraction). Specifies technical parameters like pixel thresholds (Vmin/Vmax), output dimensions (299×299), and Inception-v3 compatibility requirements."
                }
              },
              "id": "val-gybfjx8",
              "isMultiValue": true,
              "multiValueIndex": 5,
              "originalIndex": 0
            }
          ]
        },
        "prob-005": {
          "property": "Feature Extraction Architecture",
          "label": "Feature Extraction Architecture",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "GoogleNet Inception v3",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "Directly states the use of 'inception' model (GoogleNet Inception v3) for feature extraction, which is the core architecture referenced throughout the paper."
                },
                "Overview of the proposed architecture": {
                  "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers. We performed transfer learning, which involved training with a predefined model using the well-known **GoogleNet Inception v3** CNN [16].",
                  "relevance": "Explicitly names **GoogleNet Inception v3** as the CNN architecture used for feature extraction, with transfer learning applied to lung radiographs."
                },
                "Image preprocessing and feature extraction": {
                  "text": "We modified the typical inception network and fine-tuned the modified inception (M-inception) model with pre-trained weights.",
                  "relevance": "Confirms the use of a modified **Inception v3** (referred to as 'M-inception') for feature extraction, maintaining the core architecture while adapting it for medical imaging."
                }
              },
              "id": "val-or35bwy"
            },
            {
              "value": "M-inception",
              "confidence": 0.95,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "We modified the typical inception network and fine-tuned the modified inception (**M-inception**) model with pre-trained weights. The difference between the inception and M-inception model is found in the last of the fully connected layers.",
                  "relevance": "Introduces **M-inception** as the *custom variant* of GoogleNet Inception v3, specifically adapted for this study’s CT image feature extraction. This is a named resource (custom architecture) derived from the base Inception v3."
                }
              },
              "id": "val-cnqaz46"
            }
          ]
        },
        "prob-006": {
          "property": "Attention Mechanism",
          "label": "Attention Mechanism",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "No explicit attention mechanism described",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "The network was already trained on 1.2 million color images from ImageNet... The ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model. The delineated ROIs were obtained for the classification model building. We modified the typical inception network and fine-tuned the M-inception model with pre-trained weights... During the training phase, the original inception part was not trained, and we only trained the modified part.",
                  "relevance": "Describes the feature extraction pipeline (ROI delineation, fixed-size preprocessing) as the functional equivalent of spatial attention, though no explicit attention modules (e.g., self-attention layers) are mentioned. The modified Inception architecture focuses on lung regions via manual ROI selection and preprocessing."
                },
                "Overview of the proposed architecture": {
                  "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers.",
                  "relevance": "Confirms the absence of attention modules, instead using ROI-based preprocessing (manual region selection) and standard CNN feature extraction."
                }
              },
              "id": "val-401830s",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "model relies on modified Inception v3 CNN with ROI-based feature extraction via grayscale binarization, contour delineation, and fixed 299×299 pixel preprocessing to highlight discriminative lung regions",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "The network was already trained on 1.2 million color images from ImageNet... The ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model. The delineated ROIs were obtained for the classification model building. We modified the typical inception network and fine-tuned the M-inception model with pre-trained weights... During the training phase, the original inception part was not trained, and we only trained the modified part.",
                  "relevance": "Describes the feature extraction pipeline (ROI delineation, fixed-size preprocessing) as the functional equivalent of spatial attention, though no explicit attention modules (e.g., self-attention layers) are mentioned. The modified Inception architecture focuses on lung regions via manual ROI selection and preprocessing."
                },
                "Overview of the proposed architecture": {
                  "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers.",
                  "relevance": "Confirms the absence of attention modules, instead using ROI-based preprocessing (manual region selection) and standard CNN feature extraction."
                }
              },
              "id": "val-zn33sg1",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            }
          ]
        },
        "prob-007": {
          "property": "Multimodal Fusion Technique",
          "label": "Multimodal Fusion Technique",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Not applicable",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "In the future, we intend to link the hierarchical features of CT images to features of other factors such as genetic, epidemiological, and clinical information for multi-modelling analysis to facilitate enhanced diagnosis.",
                  "relevance": "Explicitly states the *absence* of multimodal fusion in the current study but outlines future plans for integration of non-imaging data (genetic/epidemiological/clinical) with CT features. Confirms the study’s scope is limited to *imaging-only* (CT) analysis."
                },
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia... The algorithm uses *only* ROI-extracted CT features for classification.",
                  "relevance": "Describes input data as *exclusively* CT images, with no mention of non-imaging modalities (e.g., lab results, patient history)."
                }
              },
              "id": "val-o2lodh2",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "study focused exclusively on CT image-based deep learning without integrating non-imaging data (e.g., lab results, patient history). Future work proposes linking hierarchical CT features with genetic, epidemiological, and clinical information for multi-modal analysis.",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "In the future, we intend to link the hierarchical features of CT images to features of other factors such as genetic, epidemiological, and clinical information for multi-modelling analysis to facilitate enhanced diagnosis.",
                  "relevance": "Explicitly states the *absence* of multimodal fusion in the current study but outlines future plans for integration of non-imaging data (genetic/epidemiological/clinical) with CT features. Confirms the study’s scope is limited to *imaging-only* (CT) analysis."
                },
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia... The algorithm uses *only* ROI-extracted CT features for classification.",
                  "relevance": "Describes input data as *exclusively* CT images, with no mention of non-imaging modalities (e.g., lab results, patient history)."
                }
              },
              "id": "val-8ybp0z8",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            }
          ]
        },
        "prob-008": {
          "property": "Training Framework",
          "label": "Training Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Inception v3",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "Directly names the 'inception' model (v3) as the base framework used for transfer learning, with modifications applied for the study's specific task."
                },
                "Overview of the proposed architecture": {
                  "text": "The network was already trained on 1.2 million color images from ImageNet that consisted of 1000 categories before learning from the lung radiographs in this study [17]. The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors...",
                  "relevance": "Explicitly confirms the use of 'Inception v3' (via ImageNet pre-training reference) as the core CNN architecture, with task-specific fine-tuning."
                }
              },
              "id": "val-8u1zof6"
            },
            {
              "value": "Adaptive Moment Estimation (Adam)",
              "confidence": 0.9,
              "evidence": {
                "Overview of the proposed architecture": {
                  "text": "we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Explicitly names 'adaptive moment estimation' (Adam) as the optimizer used during training, a critical component of the training framework."
                }
              },
              "id": "val-x11wht8"
            }
          ]
        },
        "prob-009": {
          "property": "Hardware Acceleration",
          "label": "Hardware Acceleration",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "NVIDIA Tesla V100 GPU; batch size 32",
              "confidence": 0.85,
              "evidence": {
                "Methods": {
                  "text": "The model was trained using a supercomputer system with NVIDIA Tesla V100 GPUs, processing batches of 32 images per iteration. Each case took approximately 10 seconds for analysis, enabling remote execution via a shared public platform.",
                  "relevance": "Explicit mention of GPU model (NVIDIA Tesla V100) and batch size (32) in the training workflow, critical for reproducibility. The 10-second inference time indirectly validates scalability but is not a direct hardware spec."
                }
              },
              "id": "val-b3ozlim"
            }
          ]
        },
        "prob-010": {
          "property": "Optimization Algorithm",
          "label": "Optimization Algorithm",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Adaptive Moment Estimation (Adam) gradient descent",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
                }
              },
              "id": "val-41ph5oy",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "initial learning rate: 0.01",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
                }
              },
              "id": "val-z91dimj",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "automatically adjusted during training",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
                }
              },
              "id": "val-djzletj",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "15,000 epochs",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Directly states the optimization algorithm (Adam), learning rate (0.01), automatic adjustment, and training epochs (15,000), which are critical hyperparameters for model convergence and feature extraction."
                }
              },
              "id": "val-ih75yko",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            }
          ]
        },
        "prob-011": {
          "property": "Loss Function",
          "label": "Loss Function",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "adaptive moment estimation gradient descent for optimization",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Direct mention of the optimization method used during model training, which serves as the primary loss function context in deep learning frameworks."
                }
              },
              "id": "val-vgjj2vs"
            }
          ]
        },
        "prob-012": {
          "property": "Evaluation Metrics",
          "label": "Evaluation Metrics",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Internal Validation: AUC: 0.93 (95% CI: 0.90–0.96)",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-693bu60",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "Sensitivity: 0.88",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-rsj698a",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "Specificity: 0.87",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-jkilotj",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "Accuracy: 89.5%",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-qgpt03p",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "NPV: 0.95",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-2x3f1qk",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            },
            {
              "value": "Youden Index: 0.75",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-k1h1fes",
              "isMultiValue": true,
              "multiValueIndex": 5,
              "originalIndex": 0
            },
            {
              "value": "F1 Score: 0.77",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-zbbh75r",
              "isMultiValue": true,
              "multiValueIndex": 6,
              "originalIndex": 0
            },
            {
              "value": "Kappa: 0.69",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Directly lists primary evaluation metrics for internal validation with 95% confidence intervals, justifying clinical utility for COVID-19 screening via CT imaging."
                }
              },
              "id": "val-g3o6bsz",
              "isMultiValue": true,
              "multiValueIndex": 7,
              "originalIndex": 0
            },
            {
              "value": "External Validation: AUC: 0.81 (95% CI: 0.71–0.84)",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-se8khg6",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "Sensitivity: 0.83",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-z6d8jbg",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            },
            {
              "value": "Specificity: 0.67",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-qhemxm1",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 1
            },
            {
              "value": "Accuracy: 79.3%",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-rlgko5o",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 1
            },
            {
              "value": "NPV: 0.90",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-88ssoy7",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 1
            },
            {
              "value": "Youden Index: 0.48",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-yfui4m5",
              "isMultiValue": true,
              "multiValueIndex": 5,
              "originalIndex": 1
            },
            {
              "value": "F1 Score: 0.63",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-jvockrn",
              "isMultiValue": true,
              "multiValueIndex": 6,
              "originalIndex": 1
            },
            {
              "value": "Kappa: 0.48",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation... sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively...",
                  "relevance": "Provides external validation metrics with 95% confidence intervals, critical for assessing generalizability of the deep learning model across multiple centers."
                }
              },
              "id": "val-zb2m2c6",
              "isMultiValue": true,
              "multiValueIndex": 7,
              "originalIndex": 1
            },
            {
              "value": "Pathogenic-Negative CT Prediction: Accuracy: 85.2%; Correctly Predicted COVID-19: 46/54 images",
              "confidence": 1,
              "evidence": {
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "Interestingly, we found that 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                  "relevance": "Demonstrates the model's clinical utility in detecting COVID-19 from CT images even when nucleic acid tests (gold standard) yield false negatives, addressing a critical gap in early diagnosis."
                }
              },
              "id": "val-zvr1hvp"
            },
            {
              "value": "Multi-Image External Validation: Accuracy: 82.5%",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-ka9vdml",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 3
            },
            {
              "value": "Sensitivity: 0.75",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-bd3r8yy",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 3
            },
            {
              "value": "Specificity: 0.86",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-emlivhp",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 3
            },
            {
              "value": "PPV: 0.69",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-ce21otr",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 3
            },
            {
              "value": "NPV: 0.89",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-r12pk4z",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 3
            },
            {
              "value": "Kappa: 0.59",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Provides metrics for patient-level validation (vs. image-level), which is more clinically relevant for diagnostic workflows. NPV and PPV are critical for screening applications."
                }
              },
              "id": "val-iqs1d6r",
              "isMultiValue": true,
              "multiValueIndex": 5,
              "originalIndex": 3
            },
            {
              "value": "Radiologist Comparison: Accuracy: ~55.5% (55.8% and 55.4%)",
              "confidence": 1,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                  "relevance": "Benchmarking against human experts (radiologists) demonstrates the AI model's superior performance, justifying its clinical utility for COVID-19 screening where radiologic features overlap with other viral pneumonias."
                }
              },
              "id": "val-kk5qoho",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 4
            },
            {
              "value": "Sensitivity: ~0.72 (0.71 and 0.73)",
              "confidence": 1,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                  "relevance": "Benchmarking against human experts (radiologists) demonstrates the AI model's superior performance, justifying its clinical utility for COVID-19 screening where radiologic features overlap with other viral pneumonias."
                }
              },
              "id": "val-zm9hpcm",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 4
            },
            {
              "value": "Specificity: ~0.51 (0.51 and 0.50)",
              "confidence": 1,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                  "relevance": "Benchmarking against human experts (radiologists) demonstrates the AI model's superior performance, justifying its clinical utility for COVID-19 screening where radiologic features overlap with other viral pneumonias."
                }
              },
              "id": "val-ga1h1zg",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 4
            }
          ]
        },
        "prob-013": {
          "property": "Inference Latency",
          "label": "Inference Latency",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Inference Time: ~10 seconds per case (using supercomputer system)",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s...",
                  "relevance": "Direct mention of inference time per case on target hardware (supercomputer system), critical for deployment context."
                }
              },
              "id": "val-wx7p68e"
            }
          ]
        },
        "prob-014": {
          "property": "Model Size",
          "label": "Model Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Input Layer Size: 299 × 299 × 3 pixels; ROI Image Size Range: 395 × 223 to 636 × 533 pixels",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. [...] The ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model. [...] ROI images were sized approximately from 395 × 223 to 636 × 533 pixels.",
                  "relevance": "Directly specifies the input layer dimensions (299×299×3) and ROI image size range (395×223 to 636×533), which are critical for model size and deployment constraints."
                },
                "Image preprocessing and feature extraction": {
                  "text": "To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model.",
                  "relevance": "Confirms the standardized input size (299×299) for the model, which is a key factor in model size and computational feasibility."
                }
              },
              "id": "val-fs01tdw"
            }
          ]
        },
        "prob-015": {
          "property": "Benchmark Comparison",
          "label": "Benchmark Comparison",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Radiologist 1",
              "confidence": 1,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51",
                  "relevance": "Directly names the benchmark resource (Radiologist 1) and provides performance metrics for comparison with the AI model."
                }
              },
              "id": "val-mo6mdi8"
            },
            {
              "value": "Radiologist 2",
              "confidence": 1,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50",
                  "relevance": "Directly names the benchmark resource (Radiologist 2) and provides performance metrics for comparison with the AI model."
                }
              },
              "id": "val-mchfltt"
            },
            {
              "value": "CNN",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Yang and colleagues used CNN and analyzed 152 manually annotated CT images and obtained an accuracy of 0.89 in COVID-19 diagnosis",
                  "relevance": "Names a benchmark CNN model from prior work (Yang et al.) with performance metrics, explicitly cited for comparison in the Discussion."
                }
              },
              "id": "val-3i7eibo"
            },
            {
              "value": "hybrid",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Khater and colleagues reported an accuracy of 96% using a composite hybrid feature extraction and a stack hybrid classification system in distinguishing between SARS and COVID-19 from 51 CT images",
                  "relevance": "Names a benchmark hybrid model from prior work (Khater et al.) with performance metrics, explicitly cited for comparison in the Discussion."
                }
              },
              "id": "val-tjtx3it"
            },
            {
              "value": "Nuriel MobileNetV2",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Nuriel also constructed a DCNN model based on MobileNetV2 to evaluate the ability of deep learning to detect COVID-19 from chest CT images and achieved an accuracy of 0.84",
                  "relevance": "Names a benchmark MobileNetV2-based model from prior work (Nuriel) with performance metrics, explicitly cited for comparison in the Discussion."
                }
              },
              "id": "val-fubp7et"
            },
            {
              "value": "Halgurd AI framework",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Halgurd proposed a novel AI-enabled framework to diagnose COVID-19 based on smartphone embedded sensors, which can be used by doctors conveniently",
                  "relevance": "Names a benchmark AI framework from prior work (Halgurd) with a focus on COVID-19 diagnosis, explicitly cited for comparison in the Discussion."
                }
              },
              "id": "val-pt7wvsw"
            },
            {
              "value": "GoogleNet Inception v3",
              "confidence": 0.9,
              "evidence": {
                "Overview of the proposed architecture": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation... The network was already trained on 1.2 million color images from ImageNet that consisted of 1000 categories before learning from the lung radiographs in this study.",
                  "relevance": "Explicitly names the baseline model (GoogleNet Inception v3) used as the foundation for the proposed algorithm, serving as a technical benchmark."
                }
              },
              "id": "val-clm92gc"
            }
          ]
        },
        "prob-016": {
          "property": "Explainability Method",
          "label": "Explainability Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Not explicitly stated",
              "confidence": 0,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "The paper describes the use of a modified Inception v3 model for feature extraction and classification but does not explicitly mention any explainability techniques (e.g., Grad-CAM, SHAP, attention maps) or validation of clinical relevance of features through interpretability methods."
                },
                "Discussion": {
                  "text": "The study demonstrates the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis.",
                  "relevance": "Focuses on feature extraction and diagnostic performance but lacks discussion on explainability methods or clinical validation of interpreted features."
                }
              },
              "id": "val-3hat0kb"
            }
          ]
        },
        "prob-017": {
          "property": "Cross-Dataset Generalization",
          "label": "Cross-Dataset Generalization",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Accuracy Drop: 10.2%",
              "confidence": 0.95,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                  "relevance": "Direct comparison of internal (89.5%) and external (79.3%) validation accuracies demonstrates a 10.2% drop in performance when evaluated on external datasets, quantifying cross-dataset generalization loss."
                }
              },
              "id": "val-22orueo"
            }
          ]
        },
        "prob-018": {
          "property": "Deployment Constraints",
          "label": "Deployment Constraints",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Supercomputer system required",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
                },
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
                }
              },
              "id": "val-4e1ombo",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "~10 seconds per case processing time",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
                },
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
                }
              },
              "id": "val-wq5fj9f",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "remote execution via shared public platform",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
                },
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
                }
              },
              "id": "val-5jx9liy",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "input format: 299×299-pixel grayscale CT ROI images (virtual RGB conversion for Inception v3 compatibility)",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
                },
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
                }
              },
              "id": "val-dd5t1g6",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "minimum GPU/CPU requirements unspecified but implied by transfer-learning from ImageNet-pretrained GoogleNet Inception v3 (1.2M+ parameter model).",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm [...] The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors [...] The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size [...] Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly specifies hardware (supercomputer), processing time (~10s/case), remote deployment capability, and input format constraints (299×299 pixels). Inception v3’s ImageNet pretraining implies GPU/CPU requirements for 1.2M+ parameter models."
                },
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Explicitly states deployment environment (supercomputer) and remote accessibility, critical for clinical adoption."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Defines strict input constraints: fixed 299×299-pixel grayscale ROIs converted to virtual RGB for Inception v3 compatibility."
                }
              },
              "id": "val-rib9ijt",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            },
            {
              "value": "Web platform for licensed healthcare personnel: HTTPS endpoint (https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct) for CT image upload/testing",
              "confidence": 0.9,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                  "relevance": "Provides the deployment URL and specifies target users (licensed healthcare personnel), implying authentication, secure upload, and a preprocessing pipeline for clinical integration."
                }
              },
              "id": "val-4hqmbl1",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "implies secure authentication, DICOM-to-299×299-pixel preprocessing pipeline, and cloud-based inference backend.",
              "confidence": 0.9,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                  "relevance": "Provides the deployment URL and specifies target users (licensed healthcare personnel), implying authentication, secure upload, and a preprocessing pipeline for clinical integration."
                }
              },
              "id": "val-n01cv6m",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            }
          ]
        },
        "prob-019": {
          "property": "Source Code Repository",
          "label": "Source Code Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                  "relevance": "Direct mention of the publicly accessible web link for the AI model's deployment, which serves as the closest available reference to a source code repository or implementation endpoint for reproducibility."
                }
              },
              "id": "val-f2gcbbb"
            }
          ]
        },
        "prob-020": {
          "property": "Regulatory Compliance",
          "label": "Regulatory Compliance",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Institutional Review Board (IRB) approval obtained",
              "confidence": 0.95,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                  "relevance": "Direct statement of IRB compliance and waiver of informed consent, which are key regulatory requirements for human subjects research. The text explicitly mentions adherence to institutional review board policies across multiple centers."
                }
              },
              "id": "val-byf1kyy",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "informed consent waived due to retrospective study design",
              "confidence": 0.95,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                  "relevance": "Direct statement of IRB compliance and waiver of informed consent, which are key regulatory requirements for human subjects research. The text explicitly mentions adherence to institutional review board policies across multiple centers."
                }
              },
              "id": "val-dthkviu",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "compliance with participating institutes' IRB policies",
              "confidence": 0.95,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
                  "relevance": "Direct statement of IRB compliance and waiver of informed consent, which are key regulatory requirements for human subjects research. The text explicitly mentions adherence to institutional review board policies across multiple centers."
                }
              },
              "id": "val-pmmwceh",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            }
          ]
        }
      },
      "new_data": null,
      "changes": null,
      "metadata": {
        "template_id": "aidd-rfe-2024-05",
        "template_name": "AI-Driven Radiological Feature Extraction for Accelerated Disease Diagnosis",
        "sections_analyzed": 38,
        "properties_extracted": 20
      },
      "timestamp": "2025-11-16T18:56:11.751Z"
    },
    "completedSteps": {
      "metadata": true,
      "researchFields": true,
      "researchProblems": true,
      "template": true,
      "paperContent": true
    },
    "timestamp": "2025-11-16T18:56:11.753Z"
  }
}