{
  "token": "eval_1763665696393_he9pv5hw0",
  "metadata": {
    "title": "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography",
    "authors": [
      "Jun Chen",
      "Lianlian Wu",
      "Jun Zhang",
      "Liang Zhang",
      "Dexin Gong",
      "Yilin Zhao",
      "Qiuxiang Chen",
      "Shulan Huang",
      "Ming Yang",
      "Xiao Yang",
      "Shan Hu",
      "Yonggui Wang",
      "Xiao Hu",
      "Biqing Zheng",
      "Kuo Zhang",
      "Huiling Wu",
      "Zehua Dong",
      "Youming Xu",
      "Yijie Zhu",
      "Xi Chen",
      "Mengjiao Zhang",
      "Lilei Yu",
      "Fan Cheng",
      "Honggang Yu"
    ],
    "abstract": "Abstract\n                  Computed tomography (CT) is the preferred imaging method for diagnosing 2019 novel coronavirus (COVID19) pneumonia. We aimed to construct a system based on deep learning for detecting COVID-19 pneumonia on high resolution CT. For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases in Renmin Hospital of Wuhan University were retrospectively collected. Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. An external test was conducted in Qianjiang Central Hospital to estimate the system’s robustness. The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. For 27 internal prospective patients, the system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. With the assistance of the model, the reading time of radiologists was greatly decreased by 65%. The deep learning model showed a comparable performance with expert radiologist, and greatly improved the efficiency of radiologists in clinical practice.",
    "doi": "10.1038/s41598-020-76282-0",
    "url": "http://dx.doi.org/10.1038/s41598-020-76282-0",
    "publicationDate": "2020-11-30T23:00:00.000Z",
    "status": "success",
    "venue": "Scientific Reports"
  },
  "researchFields": {
    "fields": [
      {
        "id": "R112133",
        "name": "Image and Video Processing",
        "score": 7.53571081161499,
        "description": ""
      },
      {
        "id": "R112118",
        "name": "Computer Vision and Pattern Recognition",
        "score": 5.577803611755371,
        "description": ""
      },
      {
        "id": "R114138",
        "name": "Medical Physics",
        "score": 3.984753131866455,
        "description": ""
      },
      {
        "id": "R194",
        "name": "Engineering",
        "score": 2.92790150642395,
        "description": ""
      },
      {
        "id": "R290",
        "name": "Communication Technology and New Media",
        "score": 2.8116958141326904,
        "description": ""
      }
    ],
    "selectedField": {
      "id": "R112133",
      "name": "Image and Video Processing",
      "score": 7.53571081161499,
      "description": ""
    },
    "status": "completed",
    "processing_info": {
      "step": "researchFields",
      "status": {
        "status": "completed",
        "step": "researchFields",
        "progress": 100,
        "message": "Research fields identified successfully",
        "timestamp": "2025-11-20T18:43:53.905Z"
      },
      "progress": 100,
      "message": "Research fields identified successfully",
      "timestamp": "2025-11-20T18:43:53.905Z"
    }
  },
  "researchProblems": {
    "orkg_problems": [],
    "llm_problem": {
      "title": "Automating Medical Image Diagnosis with Deep Learning for Improved Clinical Efficiency",
      "problem": "Developing reliable, scalable, and clinically interpretable deep learning systems to automate the diagnosis of complex medical conditions from imaging data (e.g., CT scans) while maintaining accuracy comparable to human experts. This challenge extends to other medical imaging modalities (e.g., MRI, X-ray) and diseases where early and accurate diagnosis is critical.",
      "domain": "Medical Imaging & AI-Assisted Diagnostics",
      "impact": "Potential to revolutionize healthcare by reducing diagnostic errors, accelerating treatment decisions, and alleviating workload burdens on radiologists. Scalable solutions could improve access to expert-level diagnostics in underserved regions and streamline workflows in high-pressure clinical environments (e.g., pandemics).",
      "motivation": "Addressing this problem could bridge gaps in diagnostic capacity, particularly during outbreaks or in resource-limited settings, while enabling data-driven precision medicine. It also raises opportunities for human-AI collaboration to enhance clinical decision-making.",
      "confidence": 0.95,
      "explanation": "The abstract explicitly frames a clear, generalizable problem (automating CT-based diagnosis with deep learning) and demonstrates its broader relevance through multi-site validation, efficiency gains, and comparison to human experts. The focus on COVID-19 is a specific use case, but the core challenge applies to any image-based diagnostic task. The high confidence stems from the well-articulated clinical impact and methodological rigor.",
      "model": "mistral-medium",
      "timestamp": "2025-11-20T18:44:08.996Z",
      "description": "Developing reliable, scalable, and clinically interpretable deep learning systems to automate the diagnosis of complex medical conditions from imaging data (e.g., CT scans) while maintaining accuracy comparable to human experts. This challenge extends to other medical imaging modalities (e.g., MRI, X-ray) and diseases where early and accurate diagnosis is critical.",
      "isLLMGenerated": true,
      "lastEdited": "2025-11-20T18:44:09.851Z"
    },
    "metadata": {
      "total_scanned": 0,
      "total_identified": 0,
      "total_similar": 0,
      "total_valid": 0,
      "field_id": "",
      "similarities_found": 0,
      "threshold_used": 0.5,
      "max_similarity": 0
    },
    "processing_info": {
      "step": "",
      "status": "completed",
      "progress": 0,
      "message": "",
      "timestamp": null
    },
    "selectedProblem": {
      "title": "Automating Medical Image Diagnosis with Deep Learning for Improved Clinical Efficiency",
      "description": "Developing reliable, scalable, and clinically interpretable deep learning systems to automate the diagnosis of complex medical conditions from imaging data (e.g., CT scans) while maintaining accuracy comparable to human experts. This challenge extends to other medical imaging modalities (e.g., MRI, X-ray) and diseases where early and accurate diagnosis is critical.",
      "isLLMGenerated": true,
      "confidence": 0.95
    },
    "original_llm_problem": {
      "title": "Automating Medical Image Diagnosis with Deep Learning for Improved Clinical Efficiency",
      "problem": "Developing reliable, scalable, and clinically interpretable deep learning systems to automate the diagnosis of complex medical conditions from imaging data (e.g., CT scans) while maintaining accuracy comparable to human experts. This challenge extends to other medical imaging modalities (e.g., MRI, X-ray) and diseases where early and accurate diagnosis is critical.",
      "confidence": 0.95,
      "explanation": "The abstract explicitly frames a clear, generalizable problem (automating CT-based diagnosis with deep learning) and demonstrates its broader relevance through multi-site validation, efficiency gains, and comparison to human experts. The focus on COVID-19 is a specific use case, but the core challenge applies to any image-based diagnostic task. The high confidence stems from the well-articulated clinical impact and methodological rigor.",
      "domain": "Medical Imaging & AI-Assisted Diagnostics",
      "impact": "Potential to revolutionize healthcare by reducing diagnostic errors, accelerating treatment decisions, and alleviating workload burdens on radiologists. Scalable solutions could improve access to expert-level diagnostics in underserved regions and streamline workflows in high-pressure clinical environments (e.g., pandemics).",
      "motivation": "Addressing this problem could bridge gaps in diagnostic capacity, particularly during outbreaks or in resource-limited settings, while enabling data-driven precision medicine. It also raises opportunities for human-AI collaboration to enhance clinical decision-making.",
      "model": "mistral-medium",
      "timestamp": "2025-11-20T18:44:08.996Z"
    }
  },
  "templates": {
    "available": {
      "template": {
        "id": "a2f7e1d9-cb4a-4e8f-b3a1-0d5c8f7e3b9a",
        "name": "Deep Learning-Based Automated Medical Image Diagnosis: Technical Implementation Framework",
        "description": "A structured template for designing, evaluating, and deploying deep learning systems in medical image diagnosis, emphasizing technical reproducibility, clinical interpretability, and performance benchmarking across imaging modalities (CT, MRI, X-ray).",
        "properties": [
          {
            "id": "prop-001",
            "label": "Primary Dataset",
            "description": "Named medical imaging dataset(s) used for training/validation (e.g., NIH ChestX-ray14, BraTS, LIDC-IDRI).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 100
            }
          },
          {
            "id": "prop-002",
            "label": "Dataset Modality",
            "description": "Imaging modality(ies) targeted (e.g., CT, MRI, X-ray, Ultrasound).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "enum": [
                "CT",
                "MRI",
                "X-ray",
                "Ultrasound",
                "PET",
                "Multimodal"
              ]
            }
          },
          {
            "id": "prop-003",
            "label": "Sample Size",
            "description": "Total number of annotated images/volumes in the dataset (training/validation/test splits).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 100,
              "max": 1000000
            }
          },
          {
            "id": "prop-004",
            "label": "Annotation Protocol",
            "description": "Methodology for ground truth generation (e.g., consensus of 3 radiologists, automated segmentation tools).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 500
            }
          },
          {
            "id": "prop-005",
            "label": "Model Architecture",
            "description": "Base deep learning architecture (e.g., ResNet-50, U-Net, Vision Transformer, nnU-Net).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 50
            }
          },
          {
            "id": "prop-006",
            "label": "Architecture Modifications",
            "description": "Custom layers or adaptations (e.g., attention mechanisms, skip connections, 3D convolutions).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 1000
            }
          },
          {
            "id": "prop-007",
            "label": "Number of Parameters",
            "description": "Total trainable parameters in the model (report in millions).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0.1,
              "max": 1000
            }
          },
          {
            "id": "prop-008",
            "label": "Deep Learning Framework",
            "description": "Software framework used (e.g., PyTorch 2.0, TensorFlow 2.12, MONAI).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 30
            }
          },
          {
            "id": "prop-009",
            "label": "Preprocessing Steps",
            "description": "Pipeline for normalization, resizing, augmentation (e.g., CLIPD windowing for CT, bias field correction for MRI).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 1500
            }
          },
          {
            "id": "prop-010",
            "label": "Training Hardware",
            "description": "GPU/TPU configuration (e.g., NVIDIA A100 80GB, Google TPU v4).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 5,
              "max_length": 50
            }
          },
          {
            "id": "prop-011",
            "label": "Batch Size",
            "description": "Batch size per GPU during training (report as integer).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 256
            }
          },
          {
            "id": "prop-012",
            "label": "Optimizer",
            "description": "Optimization algorithm (e.g., AdamW, SGD with momentum, Lion).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 20
            }
          },
          {
            "id": "prop-013",
            "label": "Learning Rate",
            "description": "Initial learning rate and schedule (e.g., 1e-4 with cosine decay).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 100
            }
          },
          {
            "id": "prop-014",
            "label": "Loss Function",
            "description": "Primary loss function (e.g., Dice + CE, focal loss, contrastive loss).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 50
            }
          },
          {
            "id": "prop-015",
            "label": "Training Epochs",
            "description": "Total epochs trained (report as integer).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 1000
            }
          },
          {
            "id": "prop-016",
            "label": "Regularization Techniques",
            "description": "Methods to prevent overfitting (e.g., dropout 0.3, weight decay 1e-5, mixup).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 300
            }
          },
          {
            "id": "prop-017",
            "label": "Primary Metric",
            "description": "Main evaluation metric (e.g., AUC-ROC, Dice coefficient, sensitivity at 95% specificity).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 50
            }
          },
          {
            "id": "prop-018",
            "label": "Metric Value",
            "description": "Quantitative result for the primary metric (report as decimal between 0–1 or percentage).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 100
            }
          },
          {
            "id": "prop-019",
            "label": "Inference Time",
            "description": "Average time per image/volume in milliseconds (report as integer).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 10000
            }
          },
          {
            "id": "prop-020",
            "label": "Interpretability Method",
            "description": "Technique for model explainability (e.g., Grad-CAM, SHAP, attention maps).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 30
            }
          },
          {
            "id": "prop-021",
            "label": "Clinical Validation",
            "description": "Description of prospective/retrospective clinical testing (e.g., reader study with 5 radiologists).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 500
            }
          },
          {
            "id": "prop-022",
            "label": "Deployment Framework",
            "description": "Tool for model serving (e.g., TensorRT, ONNX Runtime, NVIDIA Tritons).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 30
            }
          },
          {
            "id": "prop-023",
            "label": "Source Code Repository",
            "description": "Public link to implementation (GitHub, GitLab, etc.).",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "uri",
              "max_length": 200
            }
          },
          {
            "id": "prop-024",
            "label": "Paper DOI",
            "description": "Digital Object Identifier for the published work.",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "uri",
              "max_length": 100
            }
          }
        ],
        "metadata": {
          "research_field": "Image and Video Processing",
          "research_category": "Medical Deep Learning",
          "adaptability_score": 0.85,
          "total_properties": 24,
          "suggested_sections": [
            "Introduction/Problem Statement",
            "Dataset and Annotation",
            "Model Architecture",
            "Training Protocol",
            "Evaluation Metrics",
            "Clinical Integration",
            "Reproducibility"
          ],
          "creation_timestamp": "2025-11-20T18:45:01.524Z",
          "model": "mistral-medium",
          "template_version": "1.0"
        }
      }
    },
    "selectedTemplate": null,
    "llm_template": {
      "template": {
        "id": "a2f7e1d9-cb4a-4e8f-b3a1-0d5c8f7e3b9a",
        "name": "Deep Learning-Based Automated Medical Image Diagnosis: Technical Implementation Framework",
        "description": "A study reporting the design, evaluation, and deployment of deep learning systems in medical image diagnosis, emphasizing technical reproducibility, clinical interpretability, and performance benchmarking across imaging modalities (CT, MRI, X-ray).",
        "properties": [
          {
            "id": "prop-001",
            "label": "Primary Dataset",
            "description": "medical imaging dataset(s) used for training/validation (e.g., NIH ChestX-ray14, BraTS, LIDC-IDRI).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 100
            }
          },
          {
            "id": "prop-002",
            "label": "Dataset Modality",
            "description": "Imaging modality(ies) targeted (e.g., CT, MRI, X-ray, Ultrasound).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "enum": [
                "CT",
                "MRI",
                "X-ray",
                "Ultrasound",
                "PET",
                "Multimodal"
              ]
            }
          },
          {
            "id": "prop-003",
            "label": "Sample Size",
            "description": "Total number of annotated images/volumes in the dataset (training/validation/test splits).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 100,
              "max": 1000000
            }
          },
          {
            "id": "prop-004",
            "label": "Annotation Protocol",
            "description": "Methodology for ground truth generation (e.g., consensus of 3 radiologists, automated segmentation tools).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 500
            }
          },
          {
            "id": "prop-005",
            "label": "Model Architecture",
            "description": "Base deep learning architecture (e.g., ResNet-50, U-Net, Vision Transformer, nnU-Net).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 50
            }
          },
          {
            "id": "prop-006",
            "label": "Architecture Modifications",
            "description": "Custom layers or adaptations (e.g., attention mechanisms, skip connections, 3D convolutions).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 1000
            }
          },
          {
            "id": "prop-007",
            "label": "Number of Parameters",
            "description": "Total trainable parameters in the model (report in millions).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0.1,
              "max": 1000
            }
          },
          {
            "id": "prop-008",
            "label": "Deep Learning Framework",
            "description": "Software framework used (e.g., PyTorch 2.0, TensorFlow 2.12, MONAI).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 30
            }
          },
          {
            "id": "prop-009",
            "label": "Preprocessing Steps",
            "description": "Pipeline for normalization, resizing, augmentation (e.g., CLIPD windowing for CT, bias field correction for MRI).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 1500
            }
          },
          {
            "id": "prop-010",
            "label": "Training Hardware",
            "description": "GPU/TPU configuration (e.g., NVIDIA A100 80GB, Google TPU v4).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 5,
              "max_length": 50
            }
          },
          {
            "id": "prop-011",
            "label": "Batch Size",
            "description": "Batch size per GPU during training (report as integer).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 256
            }
          },
          {
            "id": "prop-012",
            "label": "Optimizer",
            "description": "Optimization algorithm (e.g., AdamW, SGD with momentum, Lion).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 20
            }
          },
          {
            "id": "prop-013",
            "label": "Learning Rate",
            "description": "Initial learning rate and schedule (e.g., 1e-4 with cosine decay).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 100
            }
          },
          {
            "id": "prop-014",
            "label": "Loss Function",
            "description": "Primary loss function (e.g., Dice + CE, focal loss, contrastive loss).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 50
            }
          },
          {
            "id": "prop-015",
            "label": "Training Epochs",
            "description": "Total epochs trained (report as integer).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 1000
            }
          },
          {
            "id": "prop-016",
            "label": "Regularization Techniques",
            "description": "Methods to prevent overfitting (e.g., dropout 0.3, weight decay 1e-5, mixup).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 300
            }
          },
          {
            "id": "prop-017",
            "label": "Primary Metric",
            "description": "Main evaluation metric (e.g., AUC-ROC, Dice coefficient, sensitivity at 95% specificity).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 50
            }
          },
          {
            "id": "prop-018",
            "label": "Metric Value",
            "description": "Quantitative result for the primary metric (report as decimal between 0–1 or percentage).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 100
            }
          },
          {
            "id": "prop-019",
            "label": "Inference Time",
            "description": "Average time per image/volume in milliseconds (report as integer).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 10000
            }
          },
          {
            "id": "prop-020",
            "label": "Interpretability Method",
            "description": "Technique for model explainability (e.g., Grad-CAM, SHAP, attention maps).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 30
            }
          },
          {
            "id": "prop-021",
            "label": "Clinical Validation",
            "description": "Description of prospective/retrospective clinical testing (e.g., reader study with 5 radiologists).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "max_length": 500
            }
          },
          {
            "id": "prop-022",
            "label": "Deployment Framework",
            "description": "Tool for model serving (e.g., TensorRT, ONNX Runtime, NVIDIA Tritons).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "string",
              "min_length": 2,
              "max_length": 30
            }
          },
          {
            "id": "prop-023",
            "label": "Source Code Repository",
            "description": "Public link to implementation (GitHub, GitLab, etc.).",
            "type": "url",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "uri",
              "max_length": 200
            }
          },
          {
            "id": "prop-024",
            "label": "Paper DOI",
            "description": "Digital Object Identifier for the published work.",
            "type": "url",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "format": "uri",
              "max_length": 100
            }
          }
        ],
        "metadata": {
          "research_field": "Image and Video Processing",
          "research_category": "Medical Deep Learning",
          "adaptability_score": 0.85,
          "total_properties": 24,
          "suggested_sections": [
            "Introduction/Problem Statement",
            "Dataset and Annotation",
            "Model Architecture",
            "Training Protocol",
            "Evaluation Metrics",
            "Clinical Integration",
            "Reproducibility"
          ],
          "creation_timestamp": "2025-11-20T18:45:01.524Z",
          "model": "mistral-medium",
          "template_version": "1.0"
        }
      }
    },
    "status": "success",
    "processing_info": {
      "step": "template",
      "status": "completed",
      "progress": 100,
      "message": "Template generated successfully",
      "timestamp": "2025-11-20T18:45:01.525Z"
    }
  },
  "paperContent": {
    "paperContent": {
      "prop-001": {
        "property": "Primary Dataset",
        "label": "Primary Dataset",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "of Wuhan University CT",
            "confidence": 1,
            "evidence": {
              "Datasets": {
                "text": "a total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients of other disease from Renmin Hospital of Wuhan University were collected for developing the model to detect COVID-19 pneumonia.",
                "relevance": "Directly identifies the primary dataset name and source institution used for model development."
              }
            },
            "id": "val-ovqi9uh"
          },
          {
            "value": "Qianjiang Central Hospital CT",
            "confidence": 1,
            "evidence": {
              "Datasets": {
                "text": "an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients) were retrospectively collected from Qianjiang Central Hospital, China.",
                "relevance": "Explicitly names the external validation dataset and its source institution."
              }
            },
            "id": "val-k40bezy"
          }
        ]
      },
      "prop-002": {
        "property": "Dataset Modality",
        "label": "Dataset Modality",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "high-resolution computed tomography (CT)",
            "confidence": 1,
            "evidence": {
              "Abstract": {
                "text": "We aimed to construct a system based on deep learning for detecting COVID-19 pneumonia on **high resolution CT**.",
                "relevance": "Explicit mention of the imaging modality used for COVID-19 pneumonia detection in the study's primary objective."
              },
              "Introduction": {
                "text": "computed tomography (CT) scan is still the most efficient modality for detecting and evaluating the severity of pneumonia... CT findings were positive in all 140 laboratory-confirmed COVID-19 patients, even in the early stage... the radiographic characteristics of pneumonia was included the clinical diagnostic standard in Hubei Province... This highlighted the importance of **CT** in the diagnosis of COVID-19 pneumonia.",
                "relevance": "Repeated emphasis on CT as the core diagnostic modality for COVID-19, including technical details about its superiority over other methods (e.g., X-ray) and its role in clinical guidelines."
              },
              "Datasets": {
                "text": "a total of 46,096 **CT scan** images from 51 COVID-19 pneumonia patients and 55 control patients... All **CT** scans were obtained in Renmin Hospital of Wuhan University. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients) were retrospectively collected from Qianjiang Central Hospital, China. The instruments used in this study included Optima CT680, Revolution CT and Bright Speed **CT** scanner (all GE Healthcare).",
                "relevance": "Explicit identification of the dataset modality as CT, including specific scanner models and institutional sources, confirming the modality's exclusivity in the study."
              },
              "Methods (Training algorithm)": {
                "text": "For detecting suspicious lesions on **CT** scans, 691 images of COVID-19 pneumonia infection lesions labelled by radiologists and 300 images randomly selected from patients of non-COVID-19 pneumonia were used. Taking the raw **CT** scan images as input with a resolution of 512×512...",
                "relevance": "Technical specification of CT scan images as the input modality for the deep learning model, including resolution details."
              },
              "Title": {
                "text": "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on **high-resolution computed tomography**",
                "relevance": "Direct declaration of the modality in the paper's title, confirming the study's focus."
              }
            },
            "id": "val-6aptxli"
          }
        ]
      },
      "prop-003": {
        "property": "Sample Size",
        "label": "Sample Size",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Training/Validation Dataset: 35,355 images from 106 patients (51 COVID-19, 55 controls)",
            "confidence": 0.98,
            "evidence": {
              "Datasets": {
                "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
              },
              "Patients": {
                "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
              }
            },
            "id": "val-gv6c9la",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "Prospective Test Dataset: 13,911 images from 27 patients",
            "confidence": 0.98,
            "evidence": {
              "Datasets": {
                "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
              },
              "Patients": {
                "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
              }
            },
            "id": "val-uxywmgj",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "External Test Dataset: 30,764 images from 100 patients (50 COVID-19, 50 controls)",
            "confidence": 0.98,
            "evidence": {
              "Datasets": {
                "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
              },
              "Patients": {
                "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
              }
            },
            "id": "val-yb5hbqg",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          }
        ]
      },
      "prop-004": {
        "property": "Annotation Protocol",
        "label": "Annotation Protocol",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Three radiologists with >5 years of clinical experience labeled infection lesions of COVID-19 pneumonia patients in the training dataset by consensus",
            "confidence": 0.98,
            "evidence": {
              "Datasets": {
                "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                "relevance": "Directly describes the annotation protocol involving multiple radiologists and consensus-based labeling for both training and testing datasets, which is the core methodology for ground truth generation."
              }
            },
            "id": "val-tn5rqza",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "selected images containing COVID-19 pneumonia lesions in the testing set were labeled by the same radiologists with combined consensus.",
            "confidence": 0.98,
            "evidence": {
              "Datasets": {
                "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                "relevance": "Directly describes the annotation protocol involving multiple radiologists and consensus-based labeling for both training and testing datasets, which is the core methodology for ground truth generation."
              }
            },
            "id": "val-qswc3de",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "Valid areas in CT images were labeled with the smallest rectangle containing all valid regions by researchers for UNet++ training.",
            "confidence": 0.92,
            "evidence": {
              "Training algorithm": {
                "text": "The training images were labelled with the smallest rectangle containing all valid areas by researchers.",
                "relevance": "Specifies a secondary annotation protocol for extracting valid regions in CT images, which is a preprocessing step critical for model training."
              }
            },
            "id": "val-bg9a98z"
          }
        ]
      },
      "prop-005": {
        "property": "Model Architecture",
        "label": "Model Architecture",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "UNet++",
            "confidence": 1,
            "evidence": {
              "Training algorithm": {
                "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation19, for the identification. Resnet-50 was used as backbone of UNet++ as previously described20.",
                "relevance": "Directly states the primary architecture (UNet++) and its backbone (ResNet-50) used for the COVID-19 detection model."
              }
            },
            "id": "val-m5cv1k5"
          },
          {
            "value": "ResNet-50",
            "confidence": 1,
            "evidence": {
              "Training algorithm": {
                "text": "Resnet-50 was used as backbone of UNet++ as previously described20. ResNet-5021 was pretrained using ImageNet dataset22, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                "relevance": "Explicitly identifies ResNet-50 as the backbone architecture integrated into UNet++ for feature extraction."
              }
            },
            "id": "val-39exuha"
          }
        ]
      },
      "prop-006": {
        "property": "Architecture Modifications",
        "label": "Architecture Modifications",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "UNet++ with nested dense convolutional blocks bridging encoder-decoder semantic gap",
            "confidence": 0.98,
            "evidence": {
              "Training algorithm": {
                "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
              }
            },
            "id": "val-dd6nyyi",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "ResNet-50 backbone pretrained on ImageNet",
            "confidence": 0.98,
            "evidence": {
              "Training algorithm": {
                "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
              }
            },
            "id": "val-zuv7wjr",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "encoder-decoder architecture with down-sampling and up-sampling for pixel-wise segmentation",
            "confidence": 0.98,
            "evidence": {
              "Training algorithm": {
                "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
              }
            },
            "id": "val-5zy9kl1",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          },
          {
            "value": "Confidence cutoff threshold of 0.50 for suspicious lesion prediction",
            "confidence": 0.95,
            "evidence": {
              "Training algorithm": {
                "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
              }
            },
            "id": "val-he8842x",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 1
          },
          {
            "value": "minimum prediction box pixel threshold of 25",
            "confidence": 0.95,
            "evidence": {
              "Training algorithm": {
                "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
              }
            },
            "id": "val-d5kw97i",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 1
          },
          {
            "value": "quadrant-based logic for per-patient prediction (3 consecutive images with lesions in same quadrant required for positive case classification)",
            "confidence": 0.95,
            "evidence": {
              "Training algorithm": {
                "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
              }
            },
            "id": "val-dxioc6e",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 1
          },
          {
            "value": "Valid area extraction module using UNet++ trained on 289 CT images to filter non-lung regions and reduce false positives",
            "confidence": 0.92,
            "evidence": {
              "Training algorithm": {
                "text": "We first trained UNet++ to extract valid areas in CT images using 289 randomly selected CT images and tested it in other 600 randomly selected CT images. [...] Valid areas were further extracted and unnecessary fields were filter out to avoid possible false positives.",
                "relevance": "Describes a preprocessing modification to the architecture (valid area extraction) to improve robustness, trained as a separate UNet++ module."
              }
            },
            "id": "val-2rn736g"
          }
        ]
      },
      "prop-008": {
        "property": "Deep Learning Framework",
        "label": "Deep Learning Framework",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Keras",
            "confidence": 1,
            "evidence": {
              "Training algorithm": {
                "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                "relevance": "Direct mention of Keras as the software framework used for training the UNet++ model."
              }
            },
            "id": "val-7p05vqx"
          }
        ]
      },
      "prop-009": {
        "property": "Preprocessing Steps",
        "label": "Preprocessing Steps",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "CT image filtering for valid lung fields",
            "confidence": 0.98,
            "evidence": {
              "Datasets": {
                "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
              },
              "Training algorithm": {
                "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
              }
            },
            "id": "val-f4qukik",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "512×512 resolution normalization",
            "confidence": 0.98,
            "evidence": {
              "Datasets": {
                "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
              },
              "Training algorithm": {
                "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
              }
            },
            "id": "val-1ph2w9q",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "lesion annotation by expert radiologists using smallest bounding rectangles",
            "confidence": 0.98,
            "evidence": {
              "Datasets": {
                "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
              },
              "Training algorithm": {
                "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
              }
            },
            "id": "val-gdvknky",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          },
          {
            "value": "exclusion of images without clear lung fields",
            "confidence": 0.98,
            "evidence": {
              "Datasets": {
                "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
              },
              "Training algorithm": {
                "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
              }
            },
            "id": "val-yey5wzn",
            "isMultiValue": true,
            "multiValueIndex": 3,
            "originalIndex": 0
          },
          {
            "value": "confidence thresholding (cutoff: 0.50) and minimum lesion size filtering (≥25 pixels).",
            "confidence": 0.98,
            "evidence": {
              "Datasets": {
                "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
              },
              "Training algorithm": {
                "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
              }
            },
            "id": "val-fv3se04",
            "isMultiValue": true,
            "multiValueIndex": 4,
            "originalIndex": 0
          }
        ]
      },
      "prop-017": {
        "property": "Primary Metric",
        "label": "Primary Metric",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "per-patient accuracy",
            "confidence": 0.95,
            "evidence": {
              "Abstract": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                "relevance": "Explicitly states the primary evaluation metric (per-patient accuracy) with its value (95.24%), directly aligning with the property definition of 'main evaluation metric'. This is the most prominent metric highlighted in the abstract, indicating its central role in model evaluation."
              }
            },
            "id": "val-8w8u6hn"
          },
          {
            "value": "per-image accuracy",
            "confidence": 0.92,
            "evidence": {
              "Abstract": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                "relevance": "Explicitly mentions 'per-image accuracy' as a key metric (98.85%), which is a standard primary metric for pixel/lesion-level evaluation in medical imaging. While secondary to per-patient accuracy in the abstract, it is still framed as a core performance indicator."
              },
              "The performance of the model on retrospective dataset": {
                "text": "A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                "relevance": "Reinforces 'per-image accuracy' (98.85%) as a primary metric in the dedicated performance section, listed alongside other standard metrics but positioned first, suggesting its importance."
              }
            },
            "id": "val-jhzrewo"
          }
        ]
      },
      "prop-018": {
        "property": "Metric Value",
        "label": "Metric Value",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Per-patient Accuracy: 95.24%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
              }
            },
            "id": "val-tsp8l6y"
          },
          {
            "value": "Per-image Accuracy: 98.85%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
              }
            },
            "id": "val-kprxcaj"
          },
          {
            "value": "Per-patient Sensitivity: 100%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
              }
            },
            "id": "val-br67dlw"
          },
          {
            "value": "Per-patient Specificity: 93.55%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
              }
            },
            "id": "val-fppzvag"
          },
          {
            "value": "Per-patient PPV: 84.62%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
              }
            },
            "id": "val-8sthpep"
          },
          {
            "value": "Per-patient NPV: 100%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
              }
            },
            "id": "val-jarcpvs"
          },
          {
            "value": "Per-image Sensitivity: 94.34%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
              }
            },
            "id": "val-st7j7q2"
          },
          {
            "value": "Per-image Specificity: 99.16%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
              }
            },
            "id": "val-eqhl9k3"
          },
          {
            "value": "Per-image PPV: 88.37%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
              }
            },
            "id": "val-73bzqtn"
          },
          {
            "value": "Per-image NPV: 99.61%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
              }
            },
            "id": "val-vggd8to"
          },
          {
            "value": "Prospective Dataset Accuracy: 92.59%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
              }
            },
            "id": "val-ql2efnd"
          },
          {
            "value": "Prospective Dataset Sensitivity: 100%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
              }
            },
            "id": "val-bid7xki"
          },
          {
            "value": "Prospective Dataset Specificity: 81.82%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
              }
            },
            "id": "val-2wyk0ly"
          },
          {
            "value": "Prospective Dataset PPV: 88.89%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
              }
            },
            "id": "val-ytfdb56"
          },
          {
            "value": "Prospective Dataset NPV: 100%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
              }
            },
            "id": "val-lst8h4w"
          },
          {
            "value": "External Dataset Accuracy: 96%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
              }
            },
            "id": "val-vin792k"
          },
          {
            "value": "External Dataset Sensitivity: 98%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
              }
            },
            "id": "val-ybidoxr"
          },
          {
            "value": "External Dataset Specificity: 94%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
              }
            },
            "id": "val-ke2hba7"
          },
          {
            "value": "External Dataset PPV: 94.23%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
              }
            },
            "id": "val-xkus4kh"
          },
          {
            "value": "External Dataset NPV: 97.92%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
              }
            },
            "id": "val-p5t4ene"
          },
          {
            "value": "Radiologist Reading Time Reduction: 65%",
            "confidence": 0.95,
            "evidence": {
              "Results": {
                "text": "With the assistance of the model, the reading time of radiologists was greatly decreased by 65%. [...] The average prediction time for model was 41.34 s per patient (IQR 39.76–44.48). [...] the average reading time for [the expert radiologist] to determine whether each patient has viral pneumonia was 116.12 s per case (IQR 85.69–118.17). [...] the average reading time of the expert was greatly decreased by 65%.",
                "relevance": "Direct evidence of efficiency gains (time reduction) achieved by the AI model, including absolute and relative metrics for radiologist workflow improvement. The 65% reduction is explicitly stated, while the derived ~40.64s is calculated from the 116.12s baseline (116.12 * (1 - 0.65) ≈ 40.64)."
              }
            },
            "id": "val-7fae6od"
          }
        ]
      },
      "prop-019": {
        "property": "Inference Time",
        "label": "Inference Time",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Prediction Time: 41.34 seconds per patient (IQR 39.76–44.48)",
            "confidence": 0.95,
            "evidence": {
              "The performance of the model in consecutive prospective patients": {
                "text": "The average prediction time for model was 41.34 s per patient (IQR 39.76–44.48).",
                "relevance": "Directly states the model's average inference time per patient, including interquartile range for context."
              }
            },
            "id": "val-hu9wacd"
          }
        ]
      },
      "prop-021": {
        "property": "Clinical Validation",
        "label": "Clinical Validation",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Retrospective validation with 46,096 CT images from 106 patients (51 COVID-19, 55 controls) at Renmin Hospital of Wuhan University",
            "confidence": 0.98,
            "evidence": {
              "Abstract": {
                "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases in Renmin Hospital of Wuhan University were retrospectively collected. The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                "relevance": "Direct description of retrospective clinical validation metrics and dataset composition."
              }
            },
            "id": "val-cfig15g"
          },
          {
            "value": "Prospective validation with 27 consecutive patients (13,911 images) at Renmin Hospital of Wuhan University",
            "confidence": 0.99,
            "evidence": {
              "Testing of the model in retrospective data": {
                "text": "Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. The model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. Inconsistent results between the expert and model were reviewed by three radiologists (expert + 2 senior radiologists with 10+ years experience).",
                "relevance": "Explicit details of prospective study design, comparator (expert radiologist), and multi-radiologist review process."
              },
              "Evaluating the efficiency of radiologist with the assistance of AI": {
                "text": "After 10 days of wash out period, the same expert radiologist re-read all CT images of 27 prospective patients with the assistance of the AI model. The results for determining whether each patient has viral pneumonia were not changed, while the average reading time of the expert was greatly decreased by 65%.",
                "relevance": "Confirms diagnostic agreement between AI and expert radiologist in prospective setting."
              }
            },
            "id": "val-hlb3zgj"
          },
          {
            "value": "expert radiologist (30 years experience) as comparator with 100% diagnostic agreement after AI-assisted review.",
            "confidence": 0.99,
            "evidence": {
              "Testing of the model in retrospective data": {
                "text": "Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. The model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. Inconsistent results between the expert and model were reviewed by three radiologists (expert + 2 senior radiologists with 10+ years experience).",
                "relevance": "Explicit details of prospective study design, comparator (expert radiologist), and multi-radiologist review process."
              },
              "Evaluating the efficiency of radiologist with the assistance of AI": {
                "text": "After 10 days of wash out period, the same expert radiologist re-read all CT images of 27 prospective patients with the assistance of the AI model. The results for determining whether each patient has viral pneumonia were not changed, while the average reading time of the expert was greatly decreased by 65%.",
                "relevance": "Confirms diagnostic agreement between AI and expert radiologist in prospective setting."
              }
            },
            "id": "val-2cs8qal"
          },
          {
            "value": "External validation with 100 patients (13,734 COVID-19 images, 17,030 control images) from Qianjiang Central Hospital",
            "confidence": 0.97,
            "evidence": {
              "The performance of the model on external dataset": {
                "text": "To estimate the robustness of the system, an external test was conducted using a dataset containing 100 patients from Qianjiang Central Hospital, China. Among them, 50 patients were COVID-19 patients (13,734 images), and 50 were normal control patients (17,030 images). The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                "relevance": "Comprehensive external validation metrics and dataset composition."
              }
            },
            "id": "val-5ntg9do"
          },
          {
            "value": "Reader study: 1 expert radiologist (30 years experience, 300+ viral pneumonia cases) vs. AI model",
            "confidence": 0.95,
            "evidence": {
              "Evaluating the efficiency of radiologist in the traditional way": {
                "text": "An expert radiologist (associate chief physician, 30 years experience, independently diagnosed ~300 viral pneumonia cases) read 27 prospective patients with average time of 116.12s per case. With AI assistance, reading time decreased by 65% to 41.34s per patient, with identical diagnostic results.",
                "relevance": "Quantitative comparison of AI-assisted vs. traditional radiologist workflow efficiency."
              },
              "Comparison between the efficiency of radiologist with or without the assistance of AI": {
                "text": "The average reading time of the expert was greatly decreased by 65% with AI assistance, while diagnostic results remained unchanged.",
                "relevance": "Confirms non-inferiority of AI-assisted diagnostics with time efficiency gains."
              }
            },
            "id": "val-6oxmgvz"
          },
          {
            "value": "Multi-radiologist consensus labeling: 3 radiologists (>5 years experience) labeled COVID-19 lesions in training dataset",
            "confidence": 0.93,
            "evidence": {
              "Datasets": {
                "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                "relevance": "Describes ground truth generation process via multi-expert consensus."
              }
            },
            "id": "val-slk9a0p"
          },
          {
            "value": "testing set labels combined by consensus.",
            "confidence": 0.93,
            "evidence": {
              "Datasets": {
                "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                "relevance": "Describes ground truth generation process via multi-expert consensus."
              }
            },
            "id": "val-q71lpxw"
          }
        ]
      },
      "prop-022": {
        "property": "Deployment Framework",
        "label": "Deployment Framework",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Keras",
            "confidence": 0.95,
            "evidence": {
              "Training algorithm": {
                "text": "UNet++ was used to train in Keras in an image-to-image manner.",
                "relevance": "Direct mention of the deployment framework (Keras) used for training the UNet++ model, which is central to the study's implementation."
              }
            },
            "id": "val-6cozhzp"
          },
          {
            "value": "GitHub",
            "confidence": 0.9,
            "evidence": {
              "Introduction": {
                "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel.",
                "relevance": "Explicit reference to GitHub as the platform for hosting the model's source code and deployment resources, indicating its role in the framework's accessibility."
              }
            },
            "id": "val-j8txtwh"
          }
        ]
      },
      "prop-023": {
        "property": "Source Code Repository",
        "label": "Source Code Repository",
        "type": "url",
        "metadata": {
          "property_type": "url",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "https://github.com/endo-angel/ct-angel",
            "confidence": 1,
            "evidence": {
              "Introduction": {
                "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel, and an open-access website has been made available to provide free provide to the present system (https://121.40.75.149/znyx-ncov/index).",
                "relevance": "Direct mention of the GitHub repository URL for the source code of the deep learning-based model, explicitly labeled as the 'source code' location."
              }
            },
            "id": "val-14tw4j3"
          }
        ]
      },
      "prop-024": {
        "property": "Paper DOI",
        "label": "Paper DOI",
        "type": "url",
        "metadata": {
          "property_type": "url",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "10.1038/s41598-020-76282-0",
            "confidence": 1,
            "evidence": {
              "Cite this article": {
                "text": "Chen, J., Wu, L., Zhang, J. et al. Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography. Sci Rep 10, 19196 (2020). https://doi.org/10.1038/s41598-020-76282-0",
                "relevance": "Direct mention of the DOI in the citation section of the paper, providing unambiguous evidence of the Digital Object Identifier for the published work."
              }
            },
            "id": "val-9bqg8bz"
          }
        ]
      }
    },
    "text_sections": {
      "Abstract": "Computed tomography (CT) is the preferred imaging method for diagnosing 2019 novel coronavirus (COVID19) pneumonia. We aimed to construct a system based on deep learning for detecting COVID-19 pneumonia on high resolution CT. For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases in Renmin Hospital of Wuhan University were retrospectively collected. Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. An external test was conducted in Qianjiang Central Hospital to estimate the system’s robustness. The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. For 27 internal prospective patients, the system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. With the assistance of the model, the reading time of radiologists was greatly decreased by 65%. The deep learning model showed a comparable performance with expert radiologist, and greatly improved the efficiency of radiologists in clinical practice.",
      "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography": "Scientific Reports\nvolume 10, Article number: 19196 (2020)\n            Cite this article\n15k Accesses\n444 Citations\n17 Altmetric\nMetrics details",
      "Introduction": "In December 2019, a new coronavirus infection disease (hereinafter referred to as COVID-19) was first reported in Wuhan. Subsequently, the outbreak began to spread widely in China and even abroad1,2,3.\nThe clinical manifestations of the COVID-19 pneumonia is complicated and could be characterized as fever, cough, myalgia, headache, and gastrointestinal symptoms onset4. Although the nucleic acid detection was considered determinant for identifying the COVID-19 infection and more rapid detection kit for the novel coronavirus has come into mass production, computed tomography (CT) scan is still the most efficient modality for detecting and evaluating the severity of pneumonia5. An update series demonstrate that CT findings were positive in all 140 laboratory-confirmed COVID-19 patients, even in the early stage4,6. In the fifth version of diagnostic manual of COVID-19 launched by the National Health and Health Commission of China, the radiographic characteristics of pneumonia was included the clinical diagnostic standard in Hubei Province7. Subsequently, 14,840 new cases of COVID-19 were reported within 1 day on Feb 13, 2020 in Wuhan, including 13,332 cases of clinical diagnoses8. This highlighted the importance of CT in the diagnosis of COVID-19 pneumonia.\nDue to the outbreak of the COVID-19, thousands of patients waited in line for CT examination in the designated fever outpatient hospital at Wuhan and other cities. As of Feb 14, there are 5,534 suspected cases, 38,107 confirmed patients receiving treatment in hospital, and 77,323 cases under medical observation in Hubei province9. Most of them need to undergo CT examination, however, there are less than 4,500 radiologists in cities of Hubei according to the China Health Statistical Yearbook (2018)10. Meanwhile, because the lung infection foci are small in the early stage of the COVID-19 infection, thinner layer (2.5 mm, 1.25 mm or even 0.625 mm) scanning were usually needed instead of conventional CT scan (5 mm) for diagnosis, which would be more time-consuming. All these made radiologists overloaded, delay the diagnosis and isolation of patients, affect patient’s treatment and prognosis, and ultimately, affect the control of COVID-19 epidemic.\nDeep learning, an important breakthrough in the domain of AI in the past decade, has huge potential at extracting tiny features by the basic unit of DCNN’s sampling kernel in image analysis11. Our group also succeeded in recruiting this technique in minor lesion detection and real-time assistance to doctors in gastrointestinal endoscopy12,13,14,15,16.\nIn the present research, we construct and validate a system based on deep learning for identification of viral pneumonia on CT. Our model has comparable performance with expert radiologist, but take much less time. The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel, and an open-access website has been made available to provide free provide to the present system (https://121.40.75.149/znyx-ncov/index).",
      "Patients": "The baseline characteristics and CT findings of 51 patients of 2019-CoV pneumonia and 55 control patients in retrospective dataset were shown in Tables 1 and 2, respectively. Baseline characteristics were comparable between training and testing datasets. The 31 control patients in retrospective testing dataset include 2 lung cancer, 4 tuberculosis, 2 bronchiectasis, 2 nonviral pneumonia, 1 lung bullae and 20 with no obvious finding in CT scan.",
      "Diagnostic testing for COVID-19": "Patient's respiratory secretions were collected and transferred to a sterile test tube with a virus transport medium. Fluorescent RT-PCR analysis of samples was performed using the COVID-19 nucleic acid detection kit developed by Shanghai Geneodx Biotechnology Co., Ltd. This detection kit was approved by the US National Drug Administration (NMPA) on January 26, 2019 and recommended by the Centers for Disease Control and Prevention (CDC)18. The rapid, high-precision COVID-19 detection kit greatly accelerated the confirmation of human COVID-19 infection.",
      "Datasets": "As shown in Fig. 1, a total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients of other disease from Renmin Hospital of Wuhan University were collected for developing the model to detect COVID-19 pneumonia. After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets. Enrolled images in training dataset covered almost all common CT features of COVID19 pneumonia, as presented in Fig. 2. Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus. For prospectively testing the model, 13,911 images of 27 consecutive patients undergoing CT scans in Feb 5, 2020 in Renmin Hospital of Wuhan University were further collected. All CT scans were obtained in Renmin Hospital of Wuhan University. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients) were retrospectively collected from Qianjiang Central Hospital, China. The instruments used in this study included Optima CT680, Revolution CT and Bright Speed CT scanner (all GE Healthcare).\nWorkflow diagram for the development and evaluation of the model for detecting COVID19 pneumonia.\nRepresentative images of COVID19 pneumonia. More than six common Computed tomography (CT) features of COVID19 pneumonia were covered in selected images. 1(a–d), the lesions were mainly ground-glass-like, with thickened blood vessels walking and including gas-bronchial signs in 1(c); 2(a–d), the lesions were mainly ground glass changes, and paving stone-like changes were observed on 2(d); 3(a–c), the lesions become solid with a large range, and air-bronchial signs are seen inside; 4, the lesion is located in the lower lobe of both lungs, and is mainly grid-like change with ground glass lesion; 5(a,b), the lesions are mainly consolidation; 6(a,b), the lesions are mainly large ground glass shadows, showing white lung-like changes, with air-bronchial signs.",
      "Training algorithm": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation19, for the identification. Resnet-50 was used as backbone of UNet++ as previously described20. ResNet-5021 was pretrained using ImageNet dataset22, and all the pre-training parameters of ResNet-50 are loaded to UNet++. The network architecture of UNet++ was shown in Fig. 3. Briefly, UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. We first trained UNet++ to extract valid areas in CT images using 289 randomly selected CT images and tested it in other 600 randomly selected CT images. The training images were labelled with the smallest rectangle containing all valid areas by researchers. The model successfully extracted valid areas in 600 images in testing set with an accuracy of 100%. For detecting suspicious lesions on CT scans, 691 images of COVID-19 pneumonia infection lesions labelled by radiologists and 300 images randomly selected from patients of non-COVID-19 pneumonia were used. Taking the raw CT scan images as input with a resolution of 512 × 512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner. The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. The training curves of UNet++ for extracting valid areas and detecting suspicious lesions in CT images were shown in Supplementary Figure 1 and Supplementary Figure 2, respectively. The prediction schematic of the model was shown in Fig. 4. Raw images were firstly input into the model, and after processing of the model, prediction boxes framing suspicious lesions were output. Valid areas were further extracted and unnecessary fields were filter out to avoid possible false positives. To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.\nThe network architecture of UNet++. UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++, and all the pre-training parameters of ResNet-50 are loaded to UNet++.\nProcessing and prediction schematic of the model. Raw images were firstly input into the model, and after processing of the model, prediction boxes framing suspicious lesions were output. Valid areas were further extracted and unnecessary fields were filter out to avoid possible false positives. To predict by case, a logic linking the prediction results of consecutive images was added. Computed tomography (CT) images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
      "Testing of the model in retrospective data": "To evaluate the performance of the model on CT scan images, five metrics including the accuracy, sensitivity, specificity, positive prediction value (PPV) and negative prediction value (NPV) were calculated as follows: accuracy = true predictions/total number of cases, sensitivity = true positive/positive, specificity = true negative/negative, PPV = true positive/(true positive + false positive), NPV = true negative/(true negative + false negative). The “true positive” is the number of correctly predicted COVID-19 pneumonia cases/images, “false positive” is the number of mistakenly predicted COVID-19 pneumonia cases/images, “positive” is the number of cases/images of COVID-19 pneumonia patients, “true negative” is the number of correctly predicted non-COVID-19 pneumonia cases/images, “false negative” is the number of mistakenly predicted non-COVID-19 pneumonia cases/images and ‘negative’ is the number of non-COVID-19 pneumonia cases/images enrolled. For image-based metrics, 636 images containing infection lesions identified by radiologists among 11 patients of COVID-19 pneumonia were used as the positive sample, and 9369 CT scan images from 31 patients of non-COVID-19 pneumonia were used as the negative sample.",
      "Evaluating the efficiency of radiologist in the traditional way": "To evaluate the performance and cost of time of radiologist against 2019-CoV pneumonia, prospectively consecutive patients undergoing CT scans were enrolled in the designated CT rooms in Feb 5, 2020 in Renmin Hospital of Wuhan University. An expert radiologist was required to read all CT images of enrolled patients using the working computer, and determine if each patient has viral pneumonia. The research assistant used a stopwatch to record the expert’s reading time. The expert radiologist was associate chief physician of the Radiology Department of Renmin Hospital of Wuhan University, with clinical experience of 30 years, and independently diagnosed about 300 viral pneumonia. Hospitalized viral pneumonia cases judged by radiologists were all diagnosed using COVID-19 nucleic acid detection kit to confirm COVID-19 infection. The computed radiography imaging system used by the radiologist was VisionPACS (Intechhosun, Being, China).",
      "Comparison between the model and radiologist in prospective data": "The CT scan images of the prospective patients as above were collected and imported into the model for prediction. The model’s performance and cost of time were compared with that of the expert radiologist. Inconsistent results between the expert and model were reviewed by three radiologists, including the expert and other two radiologists, senior staff members of the Radiology Department of Renmin Hospital of Wuhan University, with clinical experience about 10 years, and independently diagnosed about 150 viral pneumonia.",
      "Evaluating the efficiency of radiologist with the assistance of AI": "To evaluate the performance and cost of time of radiologist against 2019-CoV pneumonia with the assistance of our model, the prediction results of the model (whether a patient has viral pneumonia, and labels marking lesions) were copied to the working computer in the designated CT rooms. After 10 days of wash out period (in Feb 16, 2020), the same expert radiologist was required to re-read all CT images of 27 prospective patients using the working computer where results of the model could be viewed, and determine if each patient has viral pneumonia. The research assistant used a stopwatch to record the expert’s reading time again. Hospitalized viral pneumonia cases judged by radiologists were all diagnosed using COVID-19 nucleic acid detection kit to confirm COVID-19 infection. The computed radiography imaging system used by the radiologist was VisionPACS (Intechhosun, Being, China).",
      "Statistical analysis": "A two-tailed paired Student's t test with a significance level of 0.05 was used to compare differences in the cost time of the model and radiologist.",
      "The performance of the model on retrospective dataset": "Among 4382 CT images from 11 patients of COVID-19 pneumonia and 9369 images from 31 control patients, the model correctly diagnosed the patients with a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%. Representative images predicted by the model were shown in Fig. 5.\nRepresentative images of the model’s predictions. (A) Computed tomography (CT) images of COVID19 pneumonia. The predictions between the artificial intelligence model and radiologists were consistent. Green boxes, labels from radiologists; red boxes, labels from the model. (B) CT images of the control. The first image is an ordinary bacterial pneumonia, showing a consolidation of the right lower lobe. The second image has a tumorous lesion in the lung, showing a mass in the left upper lobe, with spiculation sign seen at the edges, and showing leaf-like growth with vacuoles inside. The third image is a secondary pulmonary tuberculosis, showing a left apical fibrous cord. The fourth image is a bronchiectasis complicated with infection, showing bronchodilation and expansion, cystic changes, and surrounding patches of infection. The fifth image shows normal lungs.",
      "The performance of the model in consecutive prospective patients": "Twenty-seven patients were enrolled in the prospective dataset in Renmin Hospital of Wuhan University. Sixteen (59.26%) patients were diagnosed as viral pneumonia by the expert radiologist, and the other eleven patients were not. Two other radiologists reviewed the CT imaging, approved the expert’s results, and summarized that the CT characteristics of the 11 patients not diagnosed by the expert include 5 ground glass nodules, 3 diminutive nodules, 2 normal and 1 fibrosclerosis.\nThe model successfully detected all the 16 patients of viral pneumonia diagnosed by the expert. Among the other 11 patients, 2 were also detected by the model. The predictions in one case was fibrosclerosis lesion, and the other one was normal stomach bubble. Using results of the radiologists as the gold standard, the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. Among the 16 patients diagnosed as viral pneumonia by radiologists, 8 admitted patients were confirmed as COVID-19 infection, and the others were outpatients that difficult to follow nucleic acid results. The average prediction time for model was 41.34 s per patient (IQR 39.76–44.48). The performance of the model on detecting COVID-19 pneumonia was shown in Table 3.",
      "The performance of the model on external dataset": "To estimate the robustness of the system, an external test was conducted using a dataset containing 100 patients from Qianjiang Central Hospital, China. Among them, 50 patients were COVID-19 patients, with a median age of 50 [IQR (48, 58.75)] and 32% (16/50) female; the other 50 were normal control patients, with a median age of 40 [IQR (38.5, 51)] and 44% (22/50) female. The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
      "Comparison between the efficiency of radiologist with or without the assistance of AI": "In the first time the expert radiologist read CT scan images of the 27 prospective patients, the average reading time for him to determine whether each patient has viral pneumonia was 116.12 s per case (IQR 85.69–118.17). After 10 days of wash out period, the same expert radiologist re-read the CT images of the 27 prospective patients with the assistance of the AI model. The results for determining whether each patient has viral pneumonia were not changed, while the average reading time of the expert was greatly decreased by 65%. This indicates that the efficiency of radiologist could be greatly improved with the assistance of AI.\nA website has been made available to provide free access to the present model (https://121.40.75.149/znyx-ncov/index) (Fig. 6). CT scan images could be uploaded by both clinicians and researches as a second opinion consulting service, especially in other provinces or countries unfamiliar with the radiologic characteristics of COVID-19. Cases of COVID-19 pneumonia were also been made available on the open-access website, which might be a useful resource for radiologists and researchers for fighting COVID-19 pneumonia. Furthermore, the module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel.\nMain interface of the open-access artificial intelligence platform which provides fast and sensitive assistance for detecting COVID19 pneumonia.",
      "Discussion": "As of Feb 14, 2020, the national health commission had reported 66,492 confirmed cases, 1,523 deaths and 8,969 suspected cases23. In the face of such large number of patients and high contagiosity of the novel coronavirus (with an estimated reproduction number R0 of 2.2 ~ 6.47), timely diagnosis and isolation are the keys to prevent further spread of the virus24,25,26,27,28. CT scan is the most efficient modality for screening and clinically diagnosing COVID-19 pneumonia5,7. However, compared to the needs of the patients, the number of radiologists is quite small, especially in Hubei province, China, which could greatly delay the diagnosis and isolation of patients, affect patient’s treatment and prognosis, and ultimately, affect the overall control of COVID-19 epidemic.\nDeep learning, a technology has shown great performance on extracting tiny features in radiology data, may hold the promise to alleviate this problem11. Recently, Ardila D, et al. achieved end-to-end lung cancer screening on low-dose chest CT with an AUC of 94.4%29. Chae KJ, et al. successfully used the convolutional neural network to classify small (≤ 2 cm) pulmonary nodules on CT scan images30. However, there was rare research being conducted to detect viral pneumonia11,29,30. Most previous studies detected pneumonia on X-ray using deep learning while not focused on viral pneumonia. Furthermore, CT is more sensitive and commonly used than X-ray for identifying COVID-19. In our previous work, we succeeded in recruiting deep learning in minor lesion detection and real-time assistance to doctors in gastrointestinal endoscopy12,13,14,15,16. Here, we enrolled this technique in identification of COVID-19 pneumonia in CT images. Results from both retrospective and prospective patients showed that the model was comparable to the level of expert radiologist, and hold great potential to reduce diagnosing time. (Fig. 7).\nAbstract diagram. Computed tomography (CT) is the most efficient modality for screening and clinically diagnosing COVID-19 pneumonia. However, compared to the needs of the patients, the number of radiologists is quite small. After enrolling artificial intelligence in identifying COVID-19 pneumonia in CT images, the efficiency of diagnosis is greatly improved. The artificial intelligence holds great potential to relieve the pressure of frontline radiologists, accelerates the diagnosis, isolation and treatment of COVID19 patients, and therefore contribute to the control of the epidemic.\nEarly diagnosis and early isolation of suspected patients are the most important ways to prevent the spread of epidemic19. Due to the sudden outbreak of COVID19, the radiology department is overloaded and patients have to wait for long times for chest CT scan, which largely increase the risk of cross-infection. In recent days, radiologists’ daily workload is huge in Hubei province, and a CT scan report has to be awaited several hours to achieve. Based on the number of suspected patients and close contacts in being, radiologists in the hardest hit, Hubei province, China, may not be enough to resist the rapid spread of the virus, which holds high estimated R0 of 2.2 ~ 6.4725,26,27,28. It could be inferred that before radiologists fulfilling the demands of existing patients, newly infected cases would appear, and the overall burden of radiologists is more overwhelming like a growing snowball. Relieving the pressure of radiologists is essential for the control of virus spreading. In the present study, our model achieved a comparable performance but with much shorter time compared with expert radiologists. It holds great potential to relieve the pressure of radiologists in clinical practice, and contribute to the control of the epidemic.\nTimely diagnosis and early treatment of infected patients is important for patients’ prognosis31. The fatality rate of COVID19 patients in Hubei province is significantly higher than that of other regions, which probably due to delayed treatment and shortage of medical resources8,32. Accelerating diagnosis efficiency is significant for improving patient outcomes. In the present study, our model helped expert radiologists achieve the same work with much shorter time, which greatly accelerats the efficiency of diagnosis in clinical practice, and may contribute to the improvement of patient outcome.\nIn addition to relieving radiologists’ pressure and accelerating diagnosis efficiency, artificial intelligence also holds the potential to reduce miss diagnosis of COVID-19 patients. The lung infection foci are sometimes mild in the early stage of the COVID-19 infection5, and requires careful observation under 0.625 mm layer scanning. Radiologists vary in skills, and could be affected by subjective status and outside pressure. One miss diagnosis could lead to multiple spread. The model is highly sensitive and stable, and would never be affected by work burden and work time. As a preliminary screening tool, it might help radiologists improve the sensitivity and reduce miss diagnosis.\nNotably, the sensitivity per patient is better while the other performance per patient is worse than the performance per image. Each patient has a large number of CT images (about 500), most of which were negative images without lesions. The specificity is equal to the true negative divided by all the negatives. The denominator increases hundreds of times when calculating specificity by image, while the numerator (false positive) does not increase so much, therefore, the specificity per image is higher than that of per patient. The same principles could be applied to accuracy and PPV. For sensitivity, a few images having suspicious lesions may be missed in COVID19 patients (sensitivity per image), while the probability that all images having suspicious lesions in a patient would be much lower (sensitivity per patient).\nOn the basis of the accuracy and efficiency of the model in detecting COVID-19 pneumonia, a cloud-based open-access artificial intelligence platform was constructed to provide assistance for detecting COVID-19 pneumonia worldwide. CT scan images could be uploaded freely by both clinicians and researches as an assistant tool, especially in other provinces or countries unfamiliar with the radiologic characteristics of COVID-19. This free open-access website can read images in batches, provide high-level auxiliary diagnostic services for different hospitals in free, and expand the boundaries of regions and manpower. Cases of COVID-19 pneumonia were also been made available on the open-access website, which might be a useful resource for radiologists and researchers for fighting COVID-19 pneumonia.\nIn summary, the deep learning-based model achieved a comparable performance with expert radiologist using much shorter time. It holds great potential to improve the efficiency of diagnosis, relieve the pressure of frontline radiologists, accelerates the diagnosis, isolation and treatment of COVID19 patients, and therefore contribute to the control of the epidemic.",
      "Acknowledgements": "This work was partly supported by the Grant from Project of Hubei Provincial Clinical Research Center for Digestive Disease Minimally Invasive Incision (Grant No. 2018BCC337), Hubei Province Major Science and Technology Innovation Project (Grant No. 2018-916-000-008), the National Natural Science Foundation of China [Grant Nos. 81672387 to Yu Honggang].",
      "Author information": "These authors contributed equally: Jun Chen, Lianlian Wu and Jun Zhang.",
      "Authors and Affiliations": "Department of Internal Medicine, Renmin Hospital of Wuhan University, 99 Zhangzhidong Road, Wuhan, 430060, Hubei Province, China\nLianlian Wu, Jun Zhang, Dexin Gong, Huiling Wu, Zehua Dong, Youming Xu, Yijie Zhu, Xi Chen, Mengjiao Zhang, Lilei Yu & Honggang Yu\nDepartment of Urinary Surgery, Renmin Hospital of Wuhan University, 99 Zhangzhidong Road, Wuhan, 430060, Hubei Province, China\nFan Cheng\nDepartment of Radiology, Renmin Hospital of Wuhan University, Wuhan, China\nJun Chen, Liang Zhang & Yilin Zhao\nQianjiang Central Hospital, Qianjiang, China\nQiuxiang Chen, Shulan Huang, Ming Yang & Xiao Yang\nWuhan EndoAngel Medical Technology Company, Wuhan, China\nShan Hu, Xiao Hu, Biqing Zheng & Kuo Zhang\nHubei Provincial Clinical Research Center for Digestive Disease Minimally Invasive Incision, Renmin Hospital of Wuhan University, Wuhan, China\nLianlian Wu, Jun Zhang, Dexin Gong, Huiling Wu, Zehua Dong, Youming Xu, Yijie Zhu, Xi Chen & Honggang Yu\nKey Laboratory of Hubei Province for Digestive System Disease, Renmin Hospital of Wuhan University, Wuhan, China\nLianlian Wu, Jun Zhang, Dexin Gong, Huiling Wu, Zehua Dong, Youming Xu, Yijie Zhu, Xi Chen & Honggang Yu\nHubei Key Laboratory of Critical Zone Evolution, School of Geography and Information Engineering, China University of Geosciences, Wuhan, China\nYonggui Wang\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar",
      "Contributions": "YH, YL and CF conceived and supervised the overall study. WL and GD contributed to writing of the manuscript. YH, YL, HS and WY contributed to critical revision of the report. CJ, ZL, ZJ, GD, WH, DZ, XY, ZY, CX, ZM, CQ, HS, YM and YX contributed to collecting and analyzing the data of patients. CJ, ZL and ZY contributed to label CT images. HS, HX, ZB, ZK and WY developed the system. All authors reviewed and approved the final version of the manuscript. The funder of the study had no role in study design, data collection, data analysis, data interpretation, or writing of the report. The corresponding author had full access to all the data in the study and had final responsibility for the decision to submit for publication.",
      "Corresponding authors": "Correspondence to\n                Lilei Yu, Fan Cheng or Honggang Yu.",
      "Competing interests": "The authors declare no competing interests.",
      "Publisher's note": "Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
      "Rights and permissions": "Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReprints and permissions",
      "Cite this article": "Chen, J., Wu, L., Zhang, J. et al. Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography.\n                    Sci Rep 10, 19196 (2020). https://doi.org/10.1038/s41598-020-76282-0\nDownload citation\nReceived: 24 March 2020\nAccepted: 03 August 2020\nPublished: 05 November 2020\nVersion of record: 05 November 2020\nDOI: https://doi.org/10.1038/s41598-020-76282-0",
      "Share this article": "Anyone you share the following link with will be able to read this content:\nSorry, a shareable link is not currently available for this article.\nProvided by the Springer Nature SharedIt content-sharing initiative",
      "A hybrid inception-dilated-ResNet architecture for deep learning-based prediction of COVID-19 severity": "Scientific Reports (2025)",
      "A Non-invasive Approach for the Classification of the Coronavirus Disease from CT Scan Images Using Machine Learning in Combination with Hybrid Texture Features": "New Generation Computing (2025)",
      "Advancing Pulmonary Infection Diagnosis: A Comprehensive Review of Deep Learning Approaches in Radiological Data Analysis": "Archives of Computational Methods in Engineering (2025)",
      "Impact of human and artificial intelligence collaboration on workload reduction in medical image interpretation": "npj Digital Medicine (2024)",
      "A predictive model to explore risk factors for severe COVID-19": "Scientific Reports (2024)\nAdvertisement",
      "Quick links": "Scientific Reports\n                    \n                    (Sci Rep)\nISSN 2045-2322 (online)",
      "Regional websites": "© 2025 Springer Nature Limited\nSign up for the Nature Briefing: Microbiology newsletter — what matters in microbiology research, free to your inbox weekly."
    },
    "status": "completed",
    "evaluationComparison": {
      "type": "update_summary",
      "component": "content_analysis",
      "original_data": {
        "prop-001": {
          "property": "Primary Dataset",
          "label": "Primary Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "of Wuhan University CT",
              "confidence": 1,
              "evidence": {
                "Datasets": {
                  "text": "a total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients of other disease from Renmin Hospital of Wuhan University were collected for developing the model to detect COVID-19 pneumonia.",
                  "relevance": "Directly identifies the primary dataset name and source institution used for model development."
                }
              },
              "id": "val-ovqi9uh"
            },
            {
              "value": "Qianjiang Central Hospital CT",
              "confidence": 1,
              "evidence": {
                "Datasets": {
                  "text": "an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients) were retrospectively collected from Qianjiang Central Hospital, China.",
                  "relevance": "Explicitly names the external validation dataset and its source institution."
                }
              },
              "id": "val-k40bezy"
            }
          ]
        },
        "prop-002": {
          "property": "Dataset Modality",
          "label": "Dataset Modality",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "high-resolution computed tomography (CT)",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "We aimed to construct a system based on deep learning for detecting COVID-19 pneumonia on **high resolution CT**.",
                  "relevance": "Explicit mention of the imaging modality used for COVID-19 pneumonia detection in the study's primary objective."
                },
                "Introduction": {
                  "text": "computed tomography (CT) scan is still the most efficient modality for detecting and evaluating the severity of pneumonia... CT findings were positive in all 140 laboratory-confirmed COVID-19 patients, even in the early stage... the radiographic characteristics of pneumonia was included the clinical diagnostic standard in Hubei Province... This highlighted the importance of **CT** in the diagnosis of COVID-19 pneumonia.",
                  "relevance": "Repeated emphasis on CT as the core diagnostic modality for COVID-19, including technical details about its superiority over other methods (e.g., X-ray) and its role in clinical guidelines."
                },
                "Datasets": {
                  "text": "a total of 46,096 **CT scan** images from 51 COVID-19 pneumonia patients and 55 control patients... All **CT** scans were obtained in Renmin Hospital of Wuhan University. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients) were retrospectively collected from Qianjiang Central Hospital, China. The instruments used in this study included Optima CT680, Revolution CT and Bright Speed **CT** scanner (all GE Healthcare).",
                  "relevance": "Explicit identification of the dataset modality as CT, including specific scanner models and institutional sources, confirming the modality's exclusivity in the study."
                },
                "Methods (Training algorithm)": {
                  "text": "For detecting suspicious lesions on **CT** scans, 691 images of COVID-19 pneumonia infection lesions labelled by radiologists and 300 images randomly selected from patients of non-COVID-19 pneumonia were used. Taking the raw **CT** scan images as input with a resolution of 512×512...",
                  "relevance": "Technical specification of CT scan images as the input modality for the deep learning model, including resolution details."
                },
                "Title": {
                  "text": "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on **high-resolution computed tomography**",
                  "relevance": "Direct declaration of the modality in the paper's title, confirming the study's focus."
                }
              },
              "id": "val-6aptxli"
            }
          ]
        },
        "prop-003": {
          "property": "Sample Size",
          "label": "Sample Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Training/Validation Dataset: 35,355 images from 106 patients (51 COVID-19, 55 controls)",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-gv6c9la",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "Prospective Test Dataset: 13,911 images from 27 patients",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-uxywmgj",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "External Test Dataset: 30,764 images from 100 patients (50 COVID-19, 50 controls)",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-yb5hbqg",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            }
          ]
        },
        "prop-004": {
          "property": "Annotation Protocol",
          "label": "Annotation Protocol",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Three radiologists with >5 years of clinical experience labeled infection lesions of COVID-19 pneumonia patients in the training dataset by consensus",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Directly describes the annotation protocol involving multiple radiologists and consensus-based labeling for both training and testing datasets, which is the core methodology for ground truth generation."
                }
              },
              "id": "val-tn5rqza",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "selected images containing COVID-19 pneumonia lesions in the testing set were labeled by the same radiologists with combined consensus.",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Directly describes the annotation protocol involving multiple radiologists and consensus-based labeling for both training and testing datasets, which is the core methodology for ground truth generation."
                }
              },
              "id": "val-qswc3de",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "Valid areas in CT images were labeled with the smallest rectangle containing all valid regions by researchers for UNet++ training.",
              "confidence": 0.92,
              "evidence": {
                "Training algorithm": {
                  "text": "The training images were labelled with the smallest rectangle containing all valid areas by researchers.",
                  "relevance": "Specifies a secondary annotation protocol for extracting valid regions in CT images, which is a preprocessing step critical for model training."
                }
              },
              "id": "val-bg9a98z"
            }
          ]
        },
        "prop-005": {
          "property": "Model Architecture",
          "label": "Model Architecture",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "UNet++",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation19, for the identification. Resnet-50 was used as backbone of UNet++ as previously described20.",
                  "relevance": "Directly states the primary architecture (UNet++) and its backbone (ResNet-50) used for the COVID-19 detection model."
                }
              },
              "id": "val-m5cv1k5"
            },
            {
              "value": "ResNet-50",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "Resnet-50 was used as backbone of UNet++ as previously described20. ResNet-5021 was pretrained using ImageNet dataset22, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Explicitly identifies ResNet-50 as the backbone architecture integrated into UNet++ for feature extraction."
                }
              },
              "id": "val-39exuha"
            }
          ]
        },
        "prop-006": {
          "property": "Architecture Modifications",
          "label": "Architecture Modifications",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "UNet++ with nested dense convolutional blocks bridging encoder-decoder semantic gap",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-dd6nyyi",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "ResNet-50 backbone pretrained on ImageNet",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-zuv7wjr",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "encoder-decoder architecture with down-sampling and up-sampling for pixel-wise segmentation",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-5zy9kl1",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "Confidence cutoff threshold of 0.50 for suspicious lesion prediction",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-he8842x",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "minimum prediction box pixel threshold of 25",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-d5kw97i",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            },
            {
              "value": "quadrant-based logic for per-patient prediction (3 consecutive images with lesions in same quadrant required for positive case classification)",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-dxioc6e",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 1
            },
            {
              "value": "Valid area extraction module using UNet++ trained on 289 CT images to filter non-lung regions and reduce false positives",
              "confidence": 0.92,
              "evidence": {
                "Training algorithm": {
                  "text": "We first trained UNet++ to extract valid areas in CT images using 289 randomly selected CT images and tested it in other 600 randomly selected CT images. [...] Valid areas were further extracted and unnecessary fields were filter out to avoid possible false positives.",
                  "relevance": "Describes a preprocessing modification to the architecture (valid area extraction) to improve robustness, trained as a separate UNet++ module."
                }
              },
              "id": "val-2rn736g"
            }
          ]
        },
        "prop-007": {
          "property": "Number of Parameters",
          "label": "Number of Parameters",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-008": {
          "property": "Deep Learning Framework",
          "label": "Deep Learning Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Keras",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Direct mention of Keras as the software framework used for training the UNet++ model."
                }
              },
              "id": "val-7p05vqx"
            }
          ]
        },
        "prop-009": {
          "property": "Preprocessing Steps",
          "label": "Preprocessing Steps",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "CT image filtering for valid lung fields",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-f4qukik",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "512×512 resolution normalization",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-1ph2w9q",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "lesion annotation by expert radiologists using smallest bounding rectangles",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-gdvknky",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "exclusion of images without clear lung fields",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-yey5wzn",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "confidence thresholding (cutoff: 0.50) and minimum lesion size filtering (≥25 pixels).",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-fv3se04",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            }
          ]
        },
        "prop-010": {
          "property": "Training Hardware",
          "label": "Training Hardware",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-011": {
          "property": "Batch Size",
          "label": "Batch Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-012": {
          "property": "Optimizer",
          "label": "Optimizer",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-013": {
          "property": "Learning Rate",
          "label": "Learning Rate",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-014": {
          "property": "Loss Function",
          "label": "Loss Function",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-015": {
          "property": "Training Epochs",
          "label": "Training Epochs",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-016": {
          "property": "Regularization Techniques",
          "label": "Regularization Techniques",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-017": {
          "property": "Primary Metric",
          "label": "Primary Metric",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "per-patient accuracy",
              "confidence": 0.95,
              "evidence": {
                "Abstract": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Explicitly states the primary evaluation metric (per-patient accuracy) with its value (95.24%), directly aligning with the property definition of 'main evaluation metric'. This is the most prominent metric highlighted in the abstract, indicating its central role in model evaluation."
                }
              },
              "id": "val-8w8u6hn"
            },
            {
              "value": "per-image accuracy",
              "confidence": 0.92,
              "evidence": {
                "Abstract": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Explicitly mentions 'per-image accuracy' as a key metric (98.85%), which is a standard primary metric for pixel/lesion-level evaluation in medical imaging. While secondary to per-patient accuracy in the abstract, it is still framed as a core performance indicator."
                },
                "The performance of the model on retrospective dataset": {
                  "text": "A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Reinforces 'per-image accuracy' (98.85%) as a primary metric in the dedicated performance section, listed alongside other standard metrics but positioned first, suggesting its importance."
                }
              },
              "id": "val-jhzrewo"
            }
          ]
        },
        "prop-018": {
          "property": "Metric Value",
          "label": "Metric Value",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Per-patient Accuracy: 95.24%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-tsp8l6y"
            },
            {
              "value": "Per-image Accuracy: 98.85%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-kprxcaj"
            },
            {
              "value": "Per-patient Sensitivity: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-br67dlw"
            },
            {
              "value": "Per-patient Specificity: 93.55%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-fppzvag"
            },
            {
              "value": "Per-patient PPV: 84.62%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-8sthpep"
            },
            {
              "value": "Per-patient NPV: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-jarcpvs"
            },
            {
              "value": "Per-image Sensitivity: 94.34%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-st7j7q2"
            },
            {
              "value": "Per-image Specificity: 99.16%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-eqhl9k3"
            },
            {
              "value": "Per-image PPV: 88.37%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-73bzqtn"
            },
            {
              "value": "Per-image NPV: 99.61%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-vggd8to"
            },
            {
              "value": "Prospective Dataset Accuracy: 92.59%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ql2efnd"
            },
            {
              "value": "Prospective Dataset Sensitivity: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-bid7xki"
            },
            {
              "value": "Prospective Dataset Specificity: 81.82%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-2wyk0ly"
            },
            {
              "value": "Prospective Dataset PPV: 88.89%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ytfdb56"
            },
            {
              "value": "Prospective Dataset NPV: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-lst8h4w"
            },
            {
              "value": "External Dataset Accuracy: 96%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-vin792k"
            },
            {
              "value": "External Dataset Sensitivity: 98%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ybidoxr"
            },
            {
              "value": "External Dataset Specificity: 94%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ke2hba7"
            },
            {
              "value": "External Dataset PPV: 94.23%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-xkus4kh"
            },
            {
              "value": "External Dataset NPV: 97.92%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-p5t4ene"
            },
            {
              "value": "Radiologist Reading Time Reduction: 65%",
              "confidence": 0.95,
              "evidence": {
                "Results": {
                  "text": "With the assistance of the model, the reading time of radiologists was greatly decreased by 65%. [...] The average prediction time for model was 41.34 s per patient (IQR 39.76–44.48). [...] the average reading time for [the expert radiologist] to determine whether each patient has viral pneumonia was 116.12 s per case (IQR 85.69–118.17). [...] the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Direct evidence of efficiency gains (time reduction) achieved by the AI model, including absolute and relative metrics for radiologist workflow improvement. The 65% reduction is explicitly stated, while the derived ~40.64s is calculated from the 116.12s baseline (116.12 * (1 - 0.65) ≈ 40.64)."
                }
              },
              "id": "val-7fae6od"
            }
          ]
        },
        "prop-019": {
          "property": "Inference Time",
          "label": "Inference Time",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Prediction Time: 41.34 seconds per patient (IQR 39.76–44.48)",
              "confidence": 0.95,
              "evidence": {
                "The performance of the model in consecutive prospective patients": {
                  "text": "The average prediction time for model was 41.34 s per patient (IQR 39.76–44.48).",
                  "relevance": "Directly states the model's average inference time per patient, including interquartile range for context."
                }
              },
              "id": "val-hu9wacd"
            }
          ]
        },
        "prop-020": {
          "property": "Interpretability Method",
          "label": "Interpretability Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-021": {
          "property": "Clinical Validation",
          "label": "Clinical Validation",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Retrospective validation with 46,096 CT images from 106 patients (51 COVID-19, 55 controls) at Renmin Hospital of Wuhan University",
              "confidence": 0.98,
              "evidence": {
                "Abstract": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases in Renmin Hospital of Wuhan University were retrospectively collected. The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Direct description of retrospective clinical validation metrics and dataset composition."
                }
              },
              "id": "val-cfig15g"
            },
            {
              "value": "Prospective validation with 27 consecutive patients (13,911 images) at Renmin Hospital of Wuhan University",
              "confidence": 0.99,
              "evidence": {
                "Testing of the model in retrospective data": {
                  "text": "Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. The model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. Inconsistent results between the expert and model were reviewed by three radiologists (expert + 2 senior radiologists with 10+ years experience).",
                  "relevance": "Explicit details of prospective study design, comparator (expert radiologist), and multi-radiologist review process."
                },
                "Evaluating the efficiency of radiologist with the assistance of AI": {
                  "text": "After 10 days of wash out period, the same expert radiologist re-read all CT images of 27 prospective patients with the assistance of the AI model. The results for determining whether each patient has viral pneumonia were not changed, while the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Confirms diagnostic agreement between AI and expert radiologist in prospective setting."
                }
              },
              "id": "val-hlb3zgj"
            },
            {
              "value": "expert radiologist (30 years experience) as comparator with 100% diagnostic agreement after AI-assisted review.",
              "confidence": 0.99,
              "evidence": {
                "Testing of the model in retrospective data": {
                  "text": "Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. The model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. Inconsistent results between the expert and model were reviewed by three radiologists (expert + 2 senior radiologists with 10+ years experience).",
                  "relevance": "Explicit details of prospective study design, comparator (expert radiologist), and multi-radiologist review process."
                },
                "Evaluating the efficiency of radiologist with the assistance of AI": {
                  "text": "After 10 days of wash out period, the same expert radiologist re-read all CT images of 27 prospective patients with the assistance of the AI model. The results for determining whether each patient has viral pneumonia were not changed, while the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Confirms diagnostic agreement between AI and expert radiologist in prospective setting."
                }
              },
              "id": "val-2cs8qal"
            },
            {
              "value": "External validation with 100 patients (13,734 COVID-19 images, 17,030 control images) from Qianjiang Central Hospital",
              "confidence": 0.97,
              "evidence": {
                "The performance of the model on external dataset": {
                  "text": "To estimate the robustness of the system, an external test was conducted using a dataset containing 100 patients from Qianjiang Central Hospital, China. Among them, 50 patients were COVID-19 patients (13,734 images), and 50 were normal control patients (17,030 images). The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Comprehensive external validation metrics and dataset composition."
                }
              },
              "id": "val-5ntg9do"
            },
            {
              "value": "Reader study: 1 expert radiologist (30 years experience, 300+ viral pneumonia cases) vs. AI model",
              "confidence": 0.95,
              "evidence": {
                "Evaluating the efficiency of radiologist in the traditional way": {
                  "text": "An expert radiologist (associate chief physician, 30 years experience, independently diagnosed ~300 viral pneumonia cases) read 27 prospective patients with average time of 116.12s per case. With AI assistance, reading time decreased by 65% to 41.34s per patient, with identical diagnostic results.",
                  "relevance": "Quantitative comparison of AI-assisted vs. traditional radiologist workflow efficiency."
                },
                "Comparison between the efficiency of radiologist with or without the assistance of AI": {
                  "text": "The average reading time of the expert was greatly decreased by 65% with AI assistance, while diagnostic results remained unchanged.",
                  "relevance": "Confirms non-inferiority of AI-assisted diagnostics with time efficiency gains."
                }
              },
              "id": "val-6oxmgvz"
            },
            {
              "value": "Multi-radiologist consensus labeling: 3 radiologists (>5 years experience) labeled COVID-19 lesions in training dataset",
              "confidence": 0.93,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Describes ground truth generation process via multi-expert consensus."
                }
              },
              "id": "val-slk9a0p"
            },
            {
              "value": "testing set labels combined by consensus.",
              "confidence": 0.93,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Describes ground truth generation process via multi-expert consensus."
                }
              },
              "id": "val-q71lpxw"
            }
          ]
        },
        "prop-022": {
          "property": "Deployment Framework",
          "label": "Deployment Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Keras",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Direct mention of the deployment framework (Keras) used for training the UNet++ model, which is central to the study's implementation."
                }
              },
              "id": "val-6cozhzp"
            },
            {
              "value": "GitHub",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel.",
                  "relevance": "Explicit reference to GitHub as the platform for hosting the model's source code and deployment resources, indicating its role in the framework's accessibility."
                }
              },
              "id": "val-j8txtwh"
            },
            {
              "value": "Open-access website",
              "confidence": 0.85,
              "evidence": {
                "Introduction": {
                  "text": "an open-access website has been made available to provide free provide to the present system (https://121.40.75.149/znyx-ncov/index).",
                  "relevance": "Describes a web-based deployment framework for the model, serving as an interface for clinical use and global access."
                },
                "Discussion": {
                  "text": "a cloud-based open-access artificial intelligence platform was constructed to provide assistance for detecting COVID-19 pneumonia worldwide.",
                  "relevance": "Reinforces the role of the open-access website as a deployment framework for the model, emphasizing its scalability and accessibility."
                }
              },
              "id": "val-rxttq4h"
            }
          ]
        },
        "prop-023": {
          "property": "Source Code Repository",
          "label": "Source Code Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/endo-angel/ct-angel",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel, and an open-access website has been made available to provide free provide to the present system (https://121.40.75.149/znyx-ncov/index).",
                  "relevance": "Direct mention of the GitHub repository URL for the source code of the deep learning-based model, explicitly labeled as the 'source code' location."
                }
              },
              "id": "val-14tw4j3"
            }
          ]
        },
        "prop-024": {
          "property": "Paper DOI",
          "label": "Paper DOI",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "10.1038/s41598-020-76282-0",
              "confidence": 1,
              "evidence": {
                "Cite this article": {
                  "text": "Chen, J., Wu, L., Zhang, J. et al. Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography. Sci Rep 10, 19196 (2020). https://doi.org/10.1038/s41598-020-76282-0",
                  "relevance": "Direct mention of the DOI in the citation section of the paper, providing unambiguous evidence of the Digital Object Identifier for the published work."
                }
              },
              "id": "val-9bqg8bz"
            }
          ]
        }
      },
      "new_data": {
        "prop-001": {
          "property": "Primary Dataset",
          "label": "Primary Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "of Wuhan University CT",
              "confidence": 1,
              "evidence": {
                "Datasets": {
                  "text": "a total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients of other disease from Renmin Hospital of Wuhan University were collected for developing the model to detect COVID-19 pneumonia.",
                  "relevance": "Directly identifies the primary dataset name and source institution used for model development."
                }
              },
              "id": "val-ovqi9uh"
            },
            {
              "value": "Qianjiang Central Hospital CT",
              "confidence": 1,
              "evidence": {
                "Datasets": {
                  "text": "an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients) were retrospectively collected from Qianjiang Central Hospital, China.",
                  "relevance": "Explicitly names the external validation dataset and its source institution."
                }
              },
              "id": "val-k40bezy"
            }
          ]
        },
        "prop-002": {
          "property": "Dataset Modality",
          "label": "Dataset Modality",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "high-resolution computed tomography (CT)",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "We aimed to construct a system based on deep learning for detecting COVID-19 pneumonia on **high resolution CT**.",
                  "relevance": "Explicit mention of the imaging modality used for COVID-19 pneumonia detection in the study's primary objective."
                },
                "Introduction": {
                  "text": "computed tomography (CT) scan is still the most efficient modality for detecting and evaluating the severity of pneumonia... CT findings were positive in all 140 laboratory-confirmed COVID-19 patients, even in the early stage... the radiographic characteristics of pneumonia was included the clinical diagnostic standard in Hubei Province... This highlighted the importance of **CT** in the diagnosis of COVID-19 pneumonia.",
                  "relevance": "Repeated emphasis on CT as the core diagnostic modality for COVID-19, including technical details about its superiority over other methods (e.g., X-ray) and its role in clinical guidelines."
                },
                "Datasets": {
                  "text": "a total of 46,096 **CT scan** images from 51 COVID-19 pneumonia patients and 55 control patients... All **CT** scans were obtained in Renmin Hospital of Wuhan University. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients) were retrospectively collected from Qianjiang Central Hospital, China. The instruments used in this study included Optima CT680, Revolution CT and Bright Speed **CT** scanner (all GE Healthcare).",
                  "relevance": "Explicit identification of the dataset modality as CT, including specific scanner models and institutional sources, confirming the modality's exclusivity in the study."
                },
                "Methods (Training algorithm)": {
                  "text": "For detecting suspicious lesions on **CT** scans, 691 images of COVID-19 pneumonia infection lesions labelled by radiologists and 300 images randomly selected from patients of non-COVID-19 pneumonia were used. Taking the raw **CT** scan images as input with a resolution of 512×512...",
                  "relevance": "Technical specification of CT scan images as the input modality for the deep learning model, including resolution details."
                },
                "Title": {
                  "text": "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on **high-resolution computed tomography**",
                  "relevance": "Direct declaration of the modality in the paper's title, confirming the study's focus."
                }
              },
              "id": "val-6aptxli"
            }
          ]
        },
        "prop-003": {
          "property": "Sample Size",
          "label": "Sample Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Training/Validation Dataset: 35,355 images from 106 patients (51 COVID-19, 55 controls)",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-gv6c9la",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "Prospective Test Dataset: 13,911 images from 27 patients",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-uxywmgj",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "External Test Dataset: 30,764 images from 100 patients (50 COVID-19, 50 controls)",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-yb5hbqg",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            }
          ]
        },
        "prop-004": {
          "property": "Annotation Protocol",
          "label": "Annotation Protocol",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Three radiologists with >5 years of clinical experience labeled infection lesions of COVID-19 pneumonia patients in the training dataset by consensus",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Directly describes the annotation protocol involving multiple radiologists and consensus-based labeling for both training and testing datasets, which is the core methodology for ground truth generation."
                }
              },
              "id": "val-tn5rqza",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "selected images containing COVID-19 pneumonia lesions in the testing set were labeled by the same radiologists with combined consensus.",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Directly describes the annotation protocol involving multiple radiologists and consensus-based labeling for both training and testing datasets, which is the core methodology for ground truth generation."
                }
              },
              "id": "val-qswc3de",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "Valid areas in CT images were labeled with the smallest rectangle containing all valid regions by researchers for UNet++ training.",
              "confidence": 0.92,
              "evidence": {
                "Training algorithm": {
                  "text": "The training images were labelled with the smallest rectangle containing all valid areas by researchers.",
                  "relevance": "Specifies a secondary annotation protocol for extracting valid regions in CT images, which is a preprocessing step critical for model training."
                }
              },
              "id": "val-bg9a98z"
            }
          ]
        },
        "prop-005": {
          "property": "Model Architecture",
          "label": "Model Architecture",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "UNet++",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation19, for the identification. Resnet-50 was used as backbone of UNet++ as previously described20.",
                  "relevance": "Directly states the primary architecture (UNet++) and its backbone (ResNet-50) used for the COVID-19 detection model."
                }
              },
              "id": "val-m5cv1k5"
            },
            {
              "value": "ResNet-50",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "Resnet-50 was used as backbone of UNet++ as previously described20. ResNet-5021 was pretrained using ImageNet dataset22, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Explicitly identifies ResNet-50 as the backbone architecture integrated into UNet++ for feature extraction."
                }
              },
              "id": "val-39exuha"
            }
          ]
        },
        "prop-006": {
          "property": "Architecture Modifications",
          "label": "Architecture Modifications",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "UNet++ with nested dense convolutional blocks bridging encoder-decoder semantic gap",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-dd6nyyi",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "ResNet-50 backbone pretrained on ImageNet",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-zuv7wjr",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "encoder-decoder architecture with down-sampling and up-sampling for pixel-wise segmentation",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-5zy9kl1",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "Confidence cutoff threshold of 0.50 for suspicious lesion prediction",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-he8842x",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "minimum prediction box pixel threshold of 25",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-d5kw97i",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            },
            {
              "value": "quadrant-based logic for per-patient prediction (3 consecutive images with lesions in same quadrant required for positive case classification)",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-dxioc6e",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 1
            },
            {
              "value": "Valid area extraction module using UNet++ trained on 289 CT images to filter non-lung regions and reduce false positives",
              "confidence": 0.92,
              "evidence": {
                "Training algorithm": {
                  "text": "We first trained UNet++ to extract valid areas in CT images using 289 randomly selected CT images and tested it in other 600 randomly selected CT images. [...] Valid areas were further extracted and unnecessary fields were filter out to avoid possible false positives.",
                  "relevance": "Describes a preprocessing modification to the architecture (valid area extraction) to improve robustness, trained as a separate UNet++ module."
                }
              },
              "id": "val-2rn736g"
            }
          ]
        },
        "prop-007": {
          "property": "Number of Parameters",
          "label": "Number of Parameters",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-008": {
          "property": "Deep Learning Framework",
          "label": "Deep Learning Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Keras",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Direct mention of Keras as the software framework used for training the UNet++ model."
                }
              },
              "id": "val-7p05vqx"
            }
          ]
        },
        "prop-009": {
          "property": "Preprocessing Steps",
          "label": "Preprocessing Steps",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "CT image filtering for valid lung fields",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-f4qukik",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "512×512 resolution normalization",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-1ph2w9q",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "lesion annotation by expert radiologists using smallest bounding rectangles",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-gdvknky",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "exclusion of images without clear lung fields",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-yey5wzn",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "confidence thresholding (cutoff: 0.50) and minimum lesion size filtering (≥25 pixels).",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-fv3se04",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            }
          ]
        },
        "prop-010": {
          "property": "Training Hardware",
          "label": "Training Hardware",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-011": {
          "property": "Batch Size",
          "label": "Batch Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-012": {
          "property": "Optimizer",
          "label": "Optimizer",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-013": {
          "property": "Learning Rate",
          "label": "Learning Rate",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-014": {
          "property": "Loss Function",
          "label": "Loss Function",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-015": {
          "property": "Training Epochs",
          "label": "Training Epochs",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-016": {
          "property": "Regularization Techniques",
          "label": "Regularization Techniques",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-017": {
          "property": "Primary Metric",
          "label": "Primary Metric",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "per-patient accuracy",
              "confidence": 0.95,
              "evidence": {
                "Abstract": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Explicitly states the primary evaluation metric (per-patient accuracy) with its value (95.24%), directly aligning with the property definition of 'main evaluation metric'. This is the most prominent metric highlighted in the abstract, indicating its central role in model evaluation."
                }
              },
              "id": "val-8w8u6hn"
            },
            {
              "value": "per-image accuracy",
              "confidence": 0.92,
              "evidence": {
                "Abstract": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Explicitly mentions 'per-image accuracy' as a key metric (98.85%), which is a standard primary metric for pixel/lesion-level evaluation in medical imaging. While secondary to per-patient accuracy in the abstract, it is still framed as a core performance indicator."
                },
                "The performance of the model on retrospective dataset": {
                  "text": "A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Reinforces 'per-image accuracy' (98.85%) as a primary metric in the dedicated performance section, listed alongside other standard metrics but positioned first, suggesting its importance."
                }
              },
              "id": "val-jhzrewo"
            }
          ]
        },
        "prop-018": {
          "property": "Metric Value",
          "label": "Metric Value",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Per-patient Accuracy: 95.24%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-tsp8l6y"
            },
            {
              "value": "Per-image Accuracy: 98.85%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-kprxcaj"
            },
            {
              "value": "Per-patient Sensitivity: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-br67dlw"
            },
            {
              "value": "Per-patient Specificity: 93.55%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-fppzvag"
            },
            {
              "value": "Per-patient PPV: 84.62%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-8sthpep"
            },
            {
              "value": "Per-patient NPV: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-jarcpvs"
            },
            {
              "value": "Per-image Sensitivity: 94.34%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-st7j7q2"
            },
            {
              "value": "Per-image Specificity: 99.16%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-eqhl9k3"
            },
            {
              "value": "Per-image PPV: 88.37%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-73bzqtn"
            },
            {
              "value": "Per-image NPV: 99.61%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-vggd8to"
            },
            {
              "value": "Prospective Dataset Accuracy: 92.59%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ql2efnd"
            },
            {
              "value": "Prospective Dataset Sensitivity: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-bid7xki"
            },
            {
              "value": "Prospective Dataset Specificity: 81.82%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-2wyk0ly"
            },
            {
              "value": "Prospective Dataset PPV: 88.89%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ytfdb56"
            },
            {
              "value": "Prospective Dataset NPV: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-lst8h4w"
            },
            {
              "value": "External Dataset Accuracy: 96%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-vin792k"
            },
            {
              "value": "External Dataset Sensitivity: 98%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ybidoxr"
            },
            {
              "value": "External Dataset Specificity: 94%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ke2hba7"
            },
            {
              "value": "External Dataset PPV: 94.23%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-xkus4kh"
            },
            {
              "value": "External Dataset NPV: 97.92%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-p5t4ene"
            },
            {
              "value": "Radiologist Reading Time Reduction: 65%",
              "confidence": 0.95,
              "evidence": {
                "Results": {
                  "text": "With the assistance of the model, the reading time of radiologists was greatly decreased by 65%. [...] The average prediction time for model was 41.34 s per patient (IQR 39.76–44.48). [...] the average reading time for [the expert radiologist] to determine whether each patient has viral pneumonia was 116.12 s per case (IQR 85.69–118.17). [...] the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Direct evidence of efficiency gains (time reduction) achieved by the AI model, including absolute and relative metrics for radiologist workflow improvement. The 65% reduction is explicitly stated, while the derived ~40.64s is calculated from the 116.12s baseline (116.12 * (1 - 0.65) ≈ 40.64)."
                }
              },
              "id": "val-7fae6od"
            }
          ]
        },
        "prop-019": {
          "property": "Inference Time",
          "label": "Inference Time",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Prediction Time: 41.34 seconds per patient (IQR 39.76–44.48)",
              "confidence": 0.95,
              "evidence": {
                "The performance of the model in consecutive prospective patients": {
                  "text": "The average prediction time for model was 41.34 s per patient (IQR 39.76–44.48).",
                  "relevance": "Directly states the model's average inference time per patient, including interquartile range for context."
                }
              },
              "id": "val-hu9wacd"
            }
          ]
        },
        "prop-020": {
          "property": "Interpretability Method",
          "label": "Interpretability Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-021": {
          "property": "Clinical Validation",
          "label": "Clinical Validation",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Retrospective validation with 46,096 CT images from 106 patients (51 COVID-19, 55 controls) at Renmin Hospital of Wuhan University",
              "confidence": 0.98,
              "evidence": {
                "Abstract": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases in Renmin Hospital of Wuhan University were retrospectively collected. The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Direct description of retrospective clinical validation metrics and dataset composition."
                }
              },
              "id": "val-cfig15g"
            },
            {
              "value": "Prospective validation with 27 consecutive patients (13,911 images) at Renmin Hospital of Wuhan University",
              "confidence": 0.99,
              "evidence": {
                "Testing of the model in retrospective data": {
                  "text": "Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. The model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. Inconsistent results between the expert and model were reviewed by three radiologists (expert + 2 senior radiologists with 10+ years experience).",
                  "relevance": "Explicit details of prospective study design, comparator (expert radiologist), and multi-radiologist review process."
                },
                "Evaluating the efficiency of radiologist with the assistance of AI": {
                  "text": "After 10 days of wash out period, the same expert radiologist re-read all CT images of 27 prospective patients with the assistance of the AI model. The results for determining whether each patient has viral pneumonia were not changed, while the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Confirms diagnostic agreement between AI and expert radiologist in prospective setting."
                }
              },
              "id": "val-hlb3zgj"
            },
            {
              "value": "expert radiologist (30 years experience) as comparator with 100% diagnostic agreement after AI-assisted review.",
              "confidence": 0.99,
              "evidence": {
                "Testing of the model in retrospective data": {
                  "text": "Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. The model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. Inconsistent results between the expert and model were reviewed by three radiologists (expert + 2 senior radiologists with 10+ years experience).",
                  "relevance": "Explicit details of prospective study design, comparator (expert radiologist), and multi-radiologist review process."
                },
                "Evaluating the efficiency of radiologist with the assistance of AI": {
                  "text": "After 10 days of wash out period, the same expert radiologist re-read all CT images of 27 prospective patients with the assistance of the AI model. The results for determining whether each patient has viral pneumonia were not changed, while the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Confirms diagnostic agreement between AI and expert radiologist in prospective setting."
                }
              },
              "id": "val-2cs8qal"
            },
            {
              "value": "External validation with 100 patients (13,734 COVID-19 images, 17,030 control images) from Qianjiang Central Hospital",
              "confidence": 0.97,
              "evidence": {
                "The performance of the model on external dataset": {
                  "text": "To estimate the robustness of the system, an external test was conducted using a dataset containing 100 patients from Qianjiang Central Hospital, China. Among them, 50 patients were COVID-19 patients (13,734 images), and 50 were normal control patients (17,030 images). The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Comprehensive external validation metrics and dataset composition."
                }
              },
              "id": "val-5ntg9do"
            },
            {
              "value": "Reader study: 1 expert radiologist (30 years experience, 300+ viral pneumonia cases) vs. AI model",
              "confidence": 0.95,
              "evidence": {
                "Evaluating the efficiency of radiologist in the traditional way": {
                  "text": "An expert radiologist (associate chief physician, 30 years experience, independently diagnosed ~300 viral pneumonia cases) read 27 prospective patients with average time of 116.12s per case. With AI assistance, reading time decreased by 65% to 41.34s per patient, with identical diagnostic results.",
                  "relevance": "Quantitative comparison of AI-assisted vs. traditional radiologist workflow efficiency."
                },
                "Comparison between the efficiency of radiologist with or without the assistance of AI": {
                  "text": "The average reading time of the expert was greatly decreased by 65% with AI assistance, while diagnostic results remained unchanged.",
                  "relevance": "Confirms non-inferiority of AI-assisted diagnostics with time efficiency gains."
                }
              },
              "id": "val-6oxmgvz"
            },
            {
              "value": "Multi-radiologist consensus labeling: 3 radiologists (>5 years experience) labeled COVID-19 lesions in training dataset",
              "confidence": 0.93,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Describes ground truth generation process via multi-expert consensus."
                }
              },
              "id": "val-slk9a0p"
            },
            {
              "value": "testing set labels combined by consensus.",
              "confidence": 0.93,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Describes ground truth generation process via multi-expert consensus."
                }
              },
              "id": "val-q71lpxw"
            }
          ]
        },
        "prop-022": {
          "property": "Deployment Framework",
          "label": "Deployment Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Keras",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Direct mention of the deployment framework (Keras) used for training the UNet++ model, which is central to the study's implementation."
                }
              },
              "id": "val-6cozhzp"
            },
            {
              "value": "GitHub",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel.",
                  "relevance": "Explicit reference to GitHub as the platform for hosting the model's source code and deployment resources, indicating its role in the framework's accessibility."
                }
              },
              "id": "val-j8txtwh"
            }
          ]
        },
        "prop-023": {
          "property": "Source Code Repository",
          "label": "Source Code Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/endo-angel/ct-angel",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel, and an open-access website has been made available to provide free provide to the present system (https://121.40.75.149/znyx-ncov/index).",
                  "relevance": "Direct mention of the GitHub repository URL for the source code of the deep learning-based model, explicitly labeled as the 'source code' location."
                }
              },
              "id": "val-14tw4j3"
            }
          ]
        },
        "prop-024": {
          "property": "Paper DOI",
          "label": "Paper DOI",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "10.1038/s41598-020-76282-0",
              "confidence": 1,
              "evidence": {
                "Cite this article": {
                  "text": "Chen, J., Wu, L., Zhang, J. et al. Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography. Sci Rep 10, 19196 (2020). https://doi.org/10.1038/s41598-020-76282-0",
                  "relevance": "Direct mention of the DOI in the citation section of the paper, providing unambiguous evidence of the Digital Object Identifier for the published work."
                }
              },
              "id": "val-9bqg8bz"
            }
          ]
        }
      },
      "changes": {
        "modified_properties": {
          "prop-022": {
            "old_values": [
              {
                "value": "Keras",
                "confidence": 0.95,
                "evidence": {
                  "Training algorithm": {
                    "text": "UNet++ was used to train in Keras in an image-to-image manner.",
                    "relevance": "Direct mention of the deployment framework (Keras) used for training the UNet++ model, which is central to the study's implementation."
                  }
                },
                "id": "val-6cozhzp"
              },
              {
                "value": "GitHub",
                "confidence": 0.9,
                "evidence": {
                  "Introduction": {
                    "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel.",
                    "relevance": "Explicit reference to GitHub as the platform for hosting the model's source code and deployment resources, indicating its role in the framework's accessibility."
                  }
                },
                "id": "val-j8txtwh"
              },
              {
                "value": "Open-access website",
                "confidence": 0.85,
                "evidence": {
                  "Introduction": {
                    "text": "an open-access website has been made available to provide free provide to the present system (https://121.40.75.149/znyx-ncov/index).",
                    "relevance": "Describes a web-based deployment framework for the model, serving as an interface for clinical use and global access."
                  },
                  "Discussion": {
                    "text": "a cloud-based open-access artificial intelligence platform was constructed to provide assistance for detecting COVID-19 pneumonia worldwide.",
                    "relevance": "Reinforces the role of the open-access website as a deployment framework for the model, emphasizing its scalability and accessibility."
                  }
                },
                "id": "val-rxttq4h"
              }
            ],
            "new_values": [
              {
                "value": "Keras",
                "confidence": 0.95,
                "evidence": {
                  "Training algorithm": {
                    "text": "UNet++ was used to train in Keras in an image-to-image manner.",
                    "relevance": "Direct mention of the deployment framework (Keras) used for training the UNet++ model, which is central to the study's implementation."
                  }
                },
                "id": "val-6cozhzp"
              },
              {
                "value": "GitHub",
                "confidence": 0.9,
                "evidence": {
                  "Introduction": {
                    "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel.",
                    "relevance": "Explicit reference to GitHub as the platform for hosting the model's source code and deployment resources, indicating its role in the framework's accessibility."
                  }
                },
                "id": "val-j8txtwh"
              }
            ],
            "old_type": "resource",
            "new_type": "resource"
          }
        },
        "changes_summary": {
          "text_updates": 0,
          "type_updates": 0,
          "deletions": 1,
          "additions": 0
        }
      },
      "timestamp": "2025-11-20T19:08:09.946Z"
    }
  },
  "timestamp": "2025-11-20T19:08:16.393Z",
  "evaluationData": {
    "token": "eval_1763664218745_9pbygaphi",
    "metadata": {
      "metadata": {
        "title": "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography",
        "authors": [
          "Jun Chen",
          "Lianlian Wu",
          "Jun Zhang",
          "Liang Zhang",
          "Dexin Gong",
          "Yilin Zhao",
          "Qiuxiang Chen",
          "Shulan Huang",
          "Ming Yang",
          "Xiao Yang",
          "Shan Hu",
          "Yonggui Wang",
          "Xiao Hu",
          "Biqing Zheng",
          "Kuo Zhang",
          "Huiling Wu",
          "Zehua Dong",
          "Youming Xu",
          "Yijie Zhu",
          "Xi Chen",
          "Mengjiao Zhang",
          "Lilei Yu",
          "Fan Cheng",
          "Honggang Yu"
        ],
        "abstract": "Abstract\n                  Computed tomography (CT) is the preferred imaging method for diagnosing 2019 novel coronavirus (COVID19) pneumonia. We aimed to construct a system based on deep learning for detecting COVID-19 pneumonia on high resolution CT. For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases in Renmin Hospital of Wuhan University were retrospectively collected. Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. An external test was conducted in Qianjiang Central Hospital to estimate the system’s robustness. The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. For 27 internal prospective patients, the system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. With the assistance of the model, the reading time of radiologists was greatly decreased by 65%. The deep learning model showed a comparable performance with expert radiologist, and greatly improved the efficiency of radiologists in clinical practice.",
        "doi": "10.1038/s41598-020-76282-0",
        "url": "http://dx.doi.org/10.1038/s41598-020-76282-0",
        "publicationDate": "2020-11-30T23:00:00.000Z",
        "venue": "Scientific Reports",
        "status": "success"
      },
      "timestamp": "2025-11-20T18:43:45.738Z",
      "step": "metadata",
      "status": "completed"
    },
    "researchFields": {
      "predictions": [
        {
          "id": "R112133",
          "name": "Image and Video Processing",
          "score": 7.53571081161499,
          "description": ""
        },
        {
          "id": "R112118",
          "name": "Computer Vision and Pattern Recognition",
          "score": 5.577803611755371,
          "description": ""
        },
        {
          "id": "R114138",
          "name": "Medical Physics",
          "score": 3.984753131866455,
          "description": ""
        },
        {
          "id": "R194",
          "name": "Engineering",
          "score": 2.92790150642395,
          "description": ""
        },
        {
          "id": "R290",
          "name": "Communication Technology and New Media",
          "score": 2.8116958141326904,
          "description": ""
        }
      ],
      "selectedField": {
        "id": "R112133",
        "name": "Image and Video Processing",
        "score": 7.53571081161499,
        "description": ""
      },
      "confidence_scores": [
        7.53571081161499,
        5.577803611755371,
        3.984753131866455,
        2.92790150642395,
        2.8116958141326904
      ],
      "usingFallback": false
    },
    "researchProblems": {
      "predictions": [],
      "selectedProblem": {
        "title": "Automating Medical Image Diagnosis with Deep Learning for Improved Clinical Efficiency",
        "description": "Developing reliable, scalable, and clinically interpretable deep learning systems to automate the diagnosis of complex medical conditions from imaging data (e.g., CT scans) while maintaining accuracy comparable to human experts. This challenge extends to other medical imaging modalities (e.g., MRI, X-ray) and diseases where early and accurate diagnosis is critical.",
        "isLLMGenerated": true,
        "confidence": 0.95
      },
      "llm_problem": {
        "title": "Automating Medical Image Diagnosis with Deep Learning for Improved Clinical Efficiency",
        "problem": "Developing reliable, scalable, and clinically interpretable deep learning systems to automate the diagnosis of complex medical conditions from imaging data (e.g., CT scans) while maintaining accuracy comparable to human experts. This challenge extends to other medical imaging modalities (e.g., MRI, X-ray) and diseases where early and accurate diagnosis is critical.",
        "domain": "Medical Imaging & AI-Assisted Diagnostics",
        "impact": "Potential to revolutionize healthcare by reducing diagnostic errors, accelerating treatment decisions, and alleviating workload burdens on radiologists. Scalable solutions could improve access to expert-level diagnostics in underserved regions and streamline workflows in high-pressure clinical environments (e.g., pandemics).",
        "motivation": "Addressing this problem could bridge gaps in diagnostic capacity, particularly during outbreaks or in resource-limited settings, while enabling data-driven precision medicine. It also raises opportunities for human-AI collaboration to enhance clinical decision-making.",
        "confidence": 0.95,
        "explanation": "The abstract explicitly frames a clear, generalizable problem (automating CT-based diagnosis with deep learning) and demonstrates its broader relevance through multi-site validation, efficiency gains, and comparison to human experts. The focus on COVID-19 is a specific use case, but the core challenge applies to any image-based diagnostic task. The high confidence stems from the well-articulated clinical impact and methodological rigor.",
        "model": "mistral-medium",
        "timestamp": "2025-11-20T18:44:08.996Z",
        "description": "Developing reliable, scalable, and clinically interpretable deep learning systems to automate the diagnosis of complex medical conditions from imaging data (e.g., CT scans) while maintaining accuracy comparable to human experts. This challenge extends to other medical imaging modalities (e.g., MRI, X-ray) and diseases where early and accurate diagnosis is critical.",
        "isLLMGenerated": true,
        "lastEdited": "2025-11-20T18:44:09.851Z"
      },
      "metadata": {
        "total_scanned": 0,
        "total_identified": 0,
        "total_similar": 0,
        "total_valid": 0,
        "field_id": "",
        "similarities_found": 0,
        "threshold_used": 0.5,
        "max_similarity": 0
      }
    },
    "template": {
      "type": "property_update",
      "component": "template",
      "property_id": "prop-024",
      "property_name": "Paper DOI",
      "field_name": "required",
      "original_data": {
        "required": false
      },
      "new_data": {
        "required": true
      },
      "changes": {
        "property": "prop-024",
        "field": "required",
        "from": false,
        "to": true,
        "is_new_property": false
      },
      "timestamp": "2025-11-20T18:50:07.686Z"
    },
    "paperContent": {
      "type": "update_summary",
      "component": "content_analysis",
      "original_data": {
        "prop-001": {
          "property": "Primary Dataset",
          "label": "Primary Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "of Wuhan University CT",
              "confidence": 1,
              "evidence": {
                "Datasets": {
                  "text": "a total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients of other disease from Renmin Hospital of Wuhan University were collected for developing the model to detect COVID-19 pneumonia.",
                  "relevance": "Directly identifies the primary dataset name and source institution used for model development."
                }
              },
              "id": "val-ovqi9uh"
            },
            {
              "value": "Qianjiang Central Hospital CT",
              "confidence": 1,
              "evidence": {
                "Datasets": {
                  "text": "an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients) were retrospectively collected from Qianjiang Central Hospital, China.",
                  "relevance": "Explicitly names the external validation dataset and its source institution."
                }
              },
              "id": "val-k40bezy"
            }
          ]
        },
        "prop-002": {
          "property": "Dataset Modality",
          "label": "Dataset Modality",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "high-resolution computed tomography (CT)",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "We aimed to construct a system based on deep learning for detecting COVID-19 pneumonia on **high resolution CT**.",
                  "relevance": "Explicit mention of the imaging modality used for COVID-19 pneumonia detection in the study's primary objective."
                },
                "Introduction": {
                  "text": "computed tomography (CT) scan is still the most efficient modality for detecting and evaluating the severity of pneumonia... CT findings were positive in all 140 laboratory-confirmed COVID-19 patients, even in the early stage... the radiographic characteristics of pneumonia was included the clinical diagnostic standard in Hubei Province... This highlighted the importance of **CT** in the diagnosis of COVID-19 pneumonia.",
                  "relevance": "Repeated emphasis on CT as the core diagnostic modality for COVID-19, including technical details about its superiority over other methods (e.g., X-ray) and its role in clinical guidelines."
                },
                "Datasets": {
                  "text": "a total of 46,096 **CT scan** images from 51 COVID-19 pneumonia patients and 55 control patients... All **CT** scans were obtained in Renmin Hospital of Wuhan University. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients) were retrospectively collected from Qianjiang Central Hospital, China. The instruments used in this study included Optima CT680, Revolution CT and Bright Speed **CT** scanner (all GE Healthcare).",
                  "relevance": "Explicit identification of the dataset modality as CT, including specific scanner models and institutional sources, confirming the modality's exclusivity in the study."
                },
                "Methods (Training algorithm)": {
                  "text": "For detecting suspicious lesions on **CT** scans, 691 images of COVID-19 pneumonia infection lesions labelled by radiologists and 300 images randomly selected from patients of non-COVID-19 pneumonia were used. Taking the raw **CT** scan images as input with a resolution of 512×512...",
                  "relevance": "Technical specification of CT scan images as the input modality for the deep learning model, including resolution details."
                },
                "Title": {
                  "text": "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on **high-resolution computed tomography**",
                  "relevance": "Direct declaration of the modality in the paper's title, confirming the study's focus."
                }
              },
              "id": "val-6aptxli"
            }
          ]
        },
        "prop-003": {
          "property": "Sample Size",
          "label": "Sample Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Training/Validation Dataset: 35,355 images from 106 patients (51 COVID-19, 55 controls)",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-gv6c9la",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "Prospective Test Dataset: 13,911 images from 27 patients",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-uxywmgj",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "External Test Dataset: 30,764 images from 100 patients (50 COVID-19, 50 controls)",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-yb5hbqg",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            }
          ]
        },
        "prop-004": {
          "property": "Annotation Protocol",
          "label": "Annotation Protocol",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Three radiologists with >5 years of clinical experience labeled infection lesions of COVID-19 pneumonia patients in the training dataset by consensus",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Directly describes the annotation protocol involving multiple radiologists and consensus-based labeling for both training and testing datasets, which is the core methodology for ground truth generation."
                }
              },
              "id": "val-tn5rqza",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "selected images containing COVID-19 pneumonia lesions in the testing set were labeled by the same radiologists with combined consensus.",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Directly describes the annotation protocol involving multiple radiologists and consensus-based labeling for both training and testing datasets, which is the core methodology for ground truth generation."
                }
              },
              "id": "val-qswc3de",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "Valid areas in CT images were labeled with the smallest rectangle containing all valid regions by researchers for UNet++ training.",
              "confidence": 0.92,
              "evidence": {
                "Training algorithm": {
                  "text": "The training images were labelled with the smallest rectangle containing all valid areas by researchers.",
                  "relevance": "Specifies a secondary annotation protocol for extracting valid regions in CT images, which is a preprocessing step critical for model training."
                }
              },
              "id": "val-bg9a98z"
            }
          ]
        },
        "prop-005": {
          "property": "Model Architecture",
          "label": "Model Architecture",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "UNet++",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation19, for the identification. Resnet-50 was used as backbone of UNet++ as previously described20.",
                  "relevance": "Directly states the primary architecture (UNet++) and its backbone (ResNet-50) used for the COVID-19 detection model."
                }
              },
              "id": "val-m5cv1k5"
            },
            {
              "value": "ResNet-50",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "Resnet-50 was used as backbone of UNet++ as previously described20. ResNet-5021 was pretrained using ImageNet dataset22, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Explicitly identifies ResNet-50 as the backbone architecture integrated into UNet++ for feature extraction."
                }
              },
              "id": "val-39exuha"
            }
          ]
        },
        "prop-006": {
          "property": "Architecture Modifications",
          "label": "Architecture Modifications",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "UNet++ with nested dense convolutional blocks bridging encoder-decoder semantic gap",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-dd6nyyi",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "ResNet-50 backbone pretrained on ImageNet",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-zuv7wjr",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "encoder-decoder architecture with down-sampling and up-sampling for pixel-wise segmentation",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-5zy9kl1",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "Confidence cutoff threshold of 0.50 for suspicious lesion prediction",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-he8842x",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "minimum prediction box pixel threshold of 25",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-d5kw97i",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            },
            {
              "value": "quadrant-based logic for per-patient prediction (3 consecutive images with lesions in same quadrant required for positive case classification)",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-dxioc6e",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 1
            },
            {
              "value": "Valid area extraction module using UNet++ trained on 289 CT images to filter non-lung regions and reduce false positives",
              "confidence": 0.92,
              "evidence": {
                "Training algorithm": {
                  "text": "We first trained UNet++ to extract valid areas in CT images using 289 randomly selected CT images and tested it in other 600 randomly selected CT images. [...] Valid areas were further extracted and unnecessary fields were filter out to avoid possible false positives.",
                  "relevance": "Describes a preprocessing modification to the architecture (valid area extraction) to improve robustness, trained as a separate UNet++ module."
                }
              },
              "id": "val-2rn736g"
            }
          ]
        },
        "prop-007": {
          "property": "Number of Parameters",
          "label": "Number of Parameters",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-008": {
          "property": "Deep Learning Framework",
          "label": "Deep Learning Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Keras",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Direct mention of Keras as the software framework used for training the UNet++ model."
                }
              },
              "id": "val-7p05vqx"
            }
          ]
        },
        "prop-009": {
          "property": "Preprocessing Steps",
          "label": "Preprocessing Steps",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "CT image filtering for valid lung fields",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-f4qukik",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "512×512 resolution normalization",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-1ph2w9q",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "lesion annotation by expert radiologists using smallest bounding rectangles",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-gdvknky",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "exclusion of images without clear lung fields",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-yey5wzn",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "confidence thresholding (cutoff: 0.50) and minimum lesion size filtering (≥25 pixels).",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-fv3se04",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            }
          ]
        },
        "prop-010": {
          "property": "Training Hardware",
          "label": "Training Hardware",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-011": {
          "property": "Batch Size",
          "label": "Batch Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-012": {
          "property": "Optimizer",
          "label": "Optimizer",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-013": {
          "property": "Learning Rate",
          "label": "Learning Rate",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-014": {
          "property": "Loss Function",
          "label": "Loss Function",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-015": {
          "property": "Training Epochs",
          "label": "Training Epochs",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-016": {
          "property": "Regularization Techniques",
          "label": "Regularization Techniques",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-017": {
          "property": "Primary Metric",
          "label": "Primary Metric",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "per-patient accuracy",
              "confidence": 0.95,
              "evidence": {
                "Abstract": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Explicitly states the primary evaluation metric (per-patient accuracy) with its value (95.24%), directly aligning with the property definition of 'main evaluation metric'. This is the most prominent metric highlighted in the abstract, indicating its central role in model evaluation."
                }
              },
              "id": "val-8w8u6hn"
            },
            {
              "value": "per-image accuracy",
              "confidence": 0.92,
              "evidence": {
                "Abstract": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Explicitly mentions 'per-image accuracy' as a key metric (98.85%), which is a standard primary metric for pixel/lesion-level evaluation in medical imaging. While secondary to per-patient accuracy in the abstract, it is still framed as a core performance indicator."
                },
                "The performance of the model on retrospective dataset": {
                  "text": "A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Reinforces 'per-image accuracy' (98.85%) as a primary metric in the dedicated performance section, listed alongside other standard metrics but positioned first, suggesting its importance."
                }
              },
              "id": "val-jhzrewo"
            }
          ]
        },
        "prop-018": {
          "property": "Metric Value",
          "label": "Metric Value",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Per-patient Accuracy: 95.24%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-tsp8l6y"
            },
            {
              "value": "Per-image Accuracy: 98.85%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-kprxcaj"
            },
            {
              "value": "Per-patient Sensitivity: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-br67dlw"
            },
            {
              "value": "Per-patient Specificity: 93.55%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-fppzvag"
            },
            {
              "value": "Per-patient PPV: 84.62%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-8sthpep"
            },
            {
              "value": "Per-patient NPV: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-jarcpvs"
            },
            {
              "value": "Per-image Sensitivity: 94.34%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-st7j7q2"
            },
            {
              "value": "Per-image Specificity: 99.16%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-eqhl9k3"
            },
            {
              "value": "Per-image PPV: 88.37%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-73bzqtn"
            },
            {
              "value": "Per-image NPV: 99.61%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-vggd8to"
            },
            {
              "value": "Prospective Dataset Accuracy: 92.59%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ql2efnd"
            },
            {
              "value": "Prospective Dataset Sensitivity: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-bid7xki"
            },
            {
              "value": "Prospective Dataset Specificity: 81.82%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-2wyk0ly"
            },
            {
              "value": "Prospective Dataset PPV: 88.89%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ytfdb56"
            },
            {
              "value": "Prospective Dataset NPV: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-lst8h4w"
            },
            {
              "value": "External Dataset Accuracy: 96%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-vin792k"
            },
            {
              "value": "External Dataset Sensitivity: 98%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ybidoxr"
            },
            {
              "value": "External Dataset Specificity: 94%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ke2hba7"
            },
            {
              "value": "External Dataset PPV: 94.23%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-xkus4kh"
            },
            {
              "value": "External Dataset NPV: 97.92%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-p5t4ene"
            },
            {
              "value": "Radiologist Reading Time Reduction: 65%",
              "confidence": 0.95,
              "evidence": {
                "Results": {
                  "text": "With the assistance of the model, the reading time of radiologists was greatly decreased by 65%. [...] The average prediction time for model was 41.34 s per patient (IQR 39.76–44.48). [...] the average reading time for [the expert radiologist] to determine whether each patient has viral pneumonia was 116.12 s per case (IQR 85.69–118.17). [...] the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Direct evidence of efficiency gains (time reduction) achieved by the AI model, including absolute and relative metrics for radiologist workflow improvement. The 65% reduction is explicitly stated, while the derived ~40.64s is calculated from the 116.12s baseline (116.12 * (1 - 0.65) ≈ 40.64)."
                }
              },
              "id": "val-7fae6od"
            }
          ]
        },
        "prop-019": {
          "property": "Inference Time",
          "label": "Inference Time",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Prediction Time: 41.34 seconds per patient (IQR 39.76–44.48)",
              "confidence": 0.95,
              "evidence": {
                "The performance of the model in consecutive prospective patients": {
                  "text": "The average prediction time for model was 41.34 s per patient (IQR 39.76–44.48).",
                  "relevance": "Directly states the model's average inference time per patient, including interquartile range for context."
                }
              },
              "id": "val-hu9wacd"
            }
          ]
        },
        "prop-020": {
          "property": "Interpretability Method",
          "label": "Interpretability Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-021": {
          "property": "Clinical Validation",
          "label": "Clinical Validation",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Retrospective validation with 46,096 CT images from 106 patients (51 COVID-19, 55 controls) at Renmin Hospital of Wuhan University",
              "confidence": 0.98,
              "evidence": {
                "Abstract": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases in Renmin Hospital of Wuhan University were retrospectively collected. The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Direct description of retrospective clinical validation metrics and dataset composition."
                }
              },
              "id": "val-cfig15g"
            },
            {
              "value": "Prospective validation with 27 consecutive patients (13,911 images) at Renmin Hospital of Wuhan University",
              "confidence": 0.99,
              "evidence": {
                "Testing of the model in retrospective data": {
                  "text": "Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. The model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. Inconsistent results between the expert and model were reviewed by three radiologists (expert + 2 senior radiologists with 10+ years experience).",
                  "relevance": "Explicit details of prospective study design, comparator (expert radiologist), and multi-radiologist review process."
                },
                "Evaluating the efficiency of radiologist with the assistance of AI": {
                  "text": "After 10 days of wash out period, the same expert radiologist re-read all CT images of 27 prospective patients with the assistance of the AI model. The results for determining whether each patient has viral pneumonia were not changed, while the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Confirms diagnostic agreement between AI and expert radiologist in prospective setting."
                }
              },
              "id": "val-hlb3zgj"
            },
            {
              "value": "expert radiologist (30 years experience) as comparator with 100% diagnostic agreement after AI-assisted review.",
              "confidence": 0.99,
              "evidence": {
                "Testing of the model in retrospective data": {
                  "text": "Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. The model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. Inconsistent results between the expert and model were reviewed by three radiologists (expert + 2 senior radiologists with 10+ years experience).",
                  "relevance": "Explicit details of prospective study design, comparator (expert radiologist), and multi-radiologist review process."
                },
                "Evaluating the efficiency of radiologist with the assistance of AI": {
                  "text": "After 10 days of wash out period, the same expert radiologist re-read all CT images of 27 prospective patients with the assistance of the AI model. The results for determining whether each patient has viral pneumonia were not changed, while the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Confirms diagnostic agreement between AI and expert radiologist in prospective setting."
                }
              },
              "id": "val-2cs8qal"
            },
            {
              "value": "External validation with 100 patients (13,734 COVID-19 images, 17,030 control images) from Qianjiang Central Hospital",
              "confidence": 0.97,
              "evidence": {
                "The performance of the model on external dataset": {
                  "text": "To estimate the robustness of the system, an external test was conducted using a dataset containing 100 patients from Qianjiang Central Hospital, China. Among them, 50 patients were COVID-19 patients (13,734 images), and 50 were normal control patients (17,030 images). The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Comprehensive external validation metrics and dataset composition."
                }
              },
              "id": "val-5ntg9do"
            },
            {
              "value": "Reader study: 1 expert radiologist (30 years experience, 300+ viral pneumonia cases) vs. AI model",
              "confidence": 0.95,
              "evidence": {
                "Evaluating the efficiency of radiologist in the traditional way": {
                  "text": "An expert radiologist (associate chief physician, 30 years experience, independently diagnosed ~300 viral pneumonia cases) read 27 prospective patients with average time of 116.12s per case. With AI assistance, reading time decreased by 65% to 41.34s per patient, with identical diagnostic results.",
                  "relevance": "Quantitative comparison of AI-assisted vs. traditional radiologist workflow efficiency."
                },
                "Comparison between the efficiency of radiologist with or without the assistance of AI": {
                  "text": "The average reading time of the expert was greatly decreased by 65% with AI assistance, while diagnostic results remained unchanged.",
                  "relevance": "Confirms non-inferiority of AI-assisted diagnostics with time efficiency gains."
                }
              },
              "id": "val-6oxmgvz"
            },
            {
              "value": "Multi-radiologist consensus labeling: 3 radiologists (>5 years experience) labeled COVID-19 lesions in training dataset",
              "confidence": 0.93,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Describes ground truth generation process via multi-expert consensus."
                }
              },
              "id": "val-slk9a0p"
            },
            {
              "value": "testing set labels combined by consensus.",
              "confidence": 0.93,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Describes ground truth generation process via multi-expert consensus."
                }
              },
              "id": "val-q71lpxw"
            }
          ]
        },
        "prop-022": {
          "property": "Deployment Framework",
          "label": "Deployment Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Keras",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Direct mention of the deployment framework (Keras) used for training the UNet++ model, which is central to the study's implementation."
                }
              },
              "id": "val-6cozhzp"
            },
            {
              "value": "GitHub",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel.",
                  "relevance": "Explicit reference to GitHub as the platform for hosting the model's source code and deployment resources, indicating its role in the framework's accessibility."
                }
              },
              "id": "val-j8txtwh"
            },
            {
              "value": "Open-access website",
              "confidence": 0.85,
              "evidence": {
                "Introduction": {
                  "text": "an open-access website has been made available to provide free provide to the present system (https://121.40.75.149/znyx-ncov/index).",
                  "relevance": "Describes a web-based deployment framework for the model, serving as an interface for clinical use and global access."
                },
                "Discussion": {
                  "text": "a cloud-based open-access artificial intelligence platform was constructed to provide assistance for detecting COVID-19 pneumonia worldwide.",
                  "relevance": "Reinforces the role of the open-access website as a deployment framework for the model, emphasizing its scalability and accessibility."
                }
              },
              "id": "val-rxttq4h"
            }
          ]
        },
        "prop-023": {
          "property": "Source Code Repository",
          "label": "Source Code Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/endo-angel/ct-angel",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel, and an open-access website has been made available to provide free provide to the present system (https://121.40.75.149/znyx-ncov/index).",
                  "relevance": "Direct mention of the GitHub repository URL for the source code of the deep learning-based model, explicitly labeled as the 'source code' location."
                }
              },
              "id": "val-14tw4j3"
            }
          ]
        },
        "prop-024": {
          "property": "Paper DOI",
          "label": "Paper DOI",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "10.1038/s41598-020-76282-0",
              "confidence": 1,
              "evidence": {
                "Cite this article": {
                  "text": "Chen, J., Wu, L., Zhang, J. et al. Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography. Sci Rep 10, 19196 (2020). https://doi.org/10.1038/s41598-020-76282-0",
                  "relevance": "Direct mention of the DOI in the citation section of the paper, providing unambiguous evidence of the Digital Object Identifier for the published work."
                }
              },
              "id": "val-9bqg8bz"
            }
          ]
        }
      },
      "new_data": {
        "prop-001": {
          "property": "Primary Dataset",
          "label": "Primary Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "of Wuhan University CT",
              "confidence": 1,
              "evidence": {
                "Datasets": {
                  "text": "a total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients of other disease from Renmin Hospital of Wuhan University were collected for developing the model to detect COVID-19 pneumonia.",
                  "relevance": "Directly identifies the primary dataset name and source institution used for model development."
                }
              },
              "id": "val-ovqi9uh"
            },
            {
              "value": "Qianjiang Central Hospital CT",
              "confidence": 1,
              "evidence": {
                "Datasets": {
                  "text": "an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients) were retrospectively collected from Qianjiang Central Hospital, China.",
                  "relevance": "Explicitly names the external validation dataset and its source institution."
                }
              },
              "id": "val-k40bezy"
            }
          ]
        },
        "prop-002": {
          "property": "Dataset Modality",
          "label": "Dataset Modality",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "high-resolution computed tomography (CT)",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "We aimed to construct a system based on deep learning for detecting COVID-19 pneumonia on **high resolution CT**.",
                  "relevance": "Explicit mention of the imaging modality used for COVID-19 pneumonia detection in the study's primary objective."
                },
                "Introduction": {
                  "text": "computed tomography (CT) scan is still the most efficient modality for detecting and evaluating the severity of pneumonia... CT findings were positive in all 140 laboratory-confirmed COVID-19 patients, even in the early stage... the radiographic characteristics of pneumonia was included the clinical diagnostic standard in Hubei Province... This highlighted the importance of **CT** in the diagnosis of COVID-19 pneumonia.",
                  "relevance": "Repeated emphasis on CT as the core diagnostic modality for COVID-19, including technical details about its superiority over other methods (e.g., X-ray) and its role in clinical guidelines."
                },
                "Datasets": {
                  "text": "a total of 46,096 **CT scan** images from 51 COVID-19 pneumonia patients and 55 control patients... All **CT** scans were obtained in Renmin Hospital of Wuhan University. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients) were retrospectively collected from Qianjiang Central Hospital, China. The instruments used in this study included Optima CT680, Revolution CT and Bright Speed **CT** scanner (all GE Healthcare).",
                  "relevance": "Explicit identification of the dataset modality as CT, including specific scanner models and institutional sources, confirming the modality's exclusivity in the study."
                },
                "Methods (Training algorithm)": {
                  "text": "For detecting suspicious lesions on **CT** scans, 691 images of COVID-19 pneumonia infection lesions labelled by radiologists and 300 images randomly selected from patients of non-COVID-19 pneumonia were used. Taking the raw **CT** scan images as input with a resolution of 512×512...",
                  "relevance": "Technical specification of CT scan images as the input modality for the deep learning model, including resolution details."
                },
                "Title": {
                  "text": "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on **high-resolution computed tomography**",
                  "relevance": "Direct declaration of the modality in the paper's title, confirming the study's focus."
                }
              },
              "id": "val-6aptxli"
            }
          ]
        },
        "prop-003": {
          "property": "Sample Size",
          "label": "Sample Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Training/Validation Dataset: 35,355 images from 106 patients (51 COVID-19, 55 controls)",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-gv6c9la",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "Prospective Test Dataset: 13,911 images from 27 patients",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-uxywmgj",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "External Test Dataset: 30,764 images from 100 patients (50 COVID-19, 50 controls)",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "A total of 46,096 CT scan images from 51 COVID-19 pneumonia patients and 55 control patients... were collected for developing the model... After filtering those images without good lung fields, 35,355 images were selected and split into training and retrospectively testing datasets... For prospectively testing the model, 13,911 images of 27 consecutive patients... were further collected. To estimate the robustness of the system, an external dataset containing 100 patients (13,734 images from 50 COVID-19 and 17,030 images from 50 normal control patients)...",
                  "relevance": "Directly states the exact sample sizes for each dataset split (training/validation, prospective test, external test) with patient/control breakdowns and image counts"
                },
                "Patients": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases...",
                  "relevance": "Provides the foundational patient cohort numbers that generate the image-level datasets"
                }
              },
              "id": "val-yb5hbqg",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            }
          ]
        },
        "prop-004": {
          "property": "Annotation Protocol",
          "label": "Annotation Protocol",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Three radiologists with >5 years of clinical experience labeled infection lesions of COVID-19 pneumonia patients in the training dataset by consensus",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Directly describes the annotation protocol involving multiple radiologists and consensus-based labeling for both training and testing datasets, which is the core methodology for ground truth generation."
                }
              },
              "id": "val-tn5rqza",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "selected images containing COVID-19 pneumonia lesions in the testing set were labeled by the same radiologists with combined consensus.",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Directly describes the annotation protocol involving multiple radiologists and consensus-based labeling for both training and testing datasets, which is the core methodology for ground truth generation."
                }
              },
              "id": "val-qswc3de",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "Valid areas in CT images were labeled with the smallest rectangle containing all valid regions by researchers for UNet++ training.",
              "confidence": 0.92,
              "evidence": {
                "Training algorithm": {
                  "text": "The training images were labelled with the smallest rectangle containing all valid areas by researchers.",
                  "relevance": "Specifies a secondary annotation protocol for extracting valid regions in CT images, which is a preprocessing step critical for model training."
                }
              },
              "id": "val-bg9a98z"
            }
          ]
        },
        "prop-005": {
          "property": "Model Architecture",
          "label": "Model Architecture",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "UNet++",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation19, for the identification. Resnet-50 was used as backbone of UNet++ as previously described20.",
                  "relevance": "Directly states the primary architecture (UNet++) and its backbone (ResNet-50) used for the COVID-19 detection model."
                }
              },
              "id": "val-m5cv1k5"
            },
            {
              "value": "ResNet-50",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "Resnet-50 was used as backbone of UNet++ as previously described20. ResNet-5021 was pretrained using ImageNet dataset22, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Explicitly identifies ResNet-50 as the backbone architecture integrated into UNet++ for feature extraction."
                }
              },
              "id": "val-39exuha"
            }
          ]
        },
        "prop-006": {
          "property": "Architecture Modifications",
          "label": "Architecture Modifications",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "UNet++ with nested dense convolutional blocks bridging encoder-decoder semantic gap",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-dd6nyyi",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "ResNet-50 backbone pretrained on ImageNet",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-zuv7wjr",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "encoder-decoder architecture with down-sampling and up-sampling for pixel-wise segmentation",
              "confidence": 0.98,
              "evidence": {
                "Training algorithm": {
                  "text": "This work is built on the top of UNet++, a novel and powerful architecture for medical image segmentation... UNet++ consists of encoder and decoder connecting through a series of nested dense convolutional blocks. The semantic gap between the feature maps of the encoder and decoder is bridged prior to fusion. The encoder extract features by down-sampling; the decoder map features to the original image by up-sampling, make classification by pixels, and thus achieve the purpose of segmentation. Resnet-50 was used as backbone of UNet++ as previously described. ResNet-50 was pretrained using ImageNet dataset, and all the pre-training parameters of ResNet-50 are loaded to UNet++.",
                  "relevance": "Direct description of UNet++ architecture modifications including nested dense convolutional blocks, semantic gap bridging, and ResNet-50 backbone integration, all of which are custom adaptations to the base UNet++ model for COVID-19 detection."
                }
              },
              "id": "val-5zy9kl1",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "Confidence cutoff threshold of 0.50 for suspicious lesion prediction",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-he8842x",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 1
            },
            {
              "value": "minimum prediction box pixel threshold of 25",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-d5kw97i",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 1
            },
            {
              "value": "quadrant-based logic for per-patient prediction (3 consecutive images with lesions in same quadrant required for positive case classification)",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25. [...] To predict by case, a logic linking the prediction results of consecutive images was added. CT images with the above prediction results were divided into four quadrants, and results would be output only when three consecutive images were predicted to have lesions in the same quadrant.",
                  "relevance": "Describes post-processing modifications to the model's prediction logic, including confidence thresholds, pixel constraints, and spatial-temporal aggregation rules for case-level classification."
                }
              },
              "id": "val-dxioc6e",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 1
            },
            {
              "value": "Valid area extraction module using UNet++ trained on 289 CT images to filter non-lung regions and reduce false positives",
              "confidence": 0.92,
              "evidence": {
                "Training algorithm": {
                  "text": "We first trained UNet++ to extract valid areas in CT images using 289 randomly selected CT images and tested it in other 600 randomly selected CT images. [...] Valid areas were further extracted and unnecessary fields were filter out to avoid possible false positives.",
                  "relevance": "Describes a preprocessing modification to the architecture (valid area extraction) to improve robustness, trained as a separate UNet++ module."
                }
              },
              "id": "val-2rn736g"
            }
          ]
        },
        "prop-007": {
          "property": "Number of Parameters",
          "label": "Number of Parameters",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-008": {
          "property": "Deep Learning Framework",
          "label": "Deep Learning Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Keras",
              "confidence": 1,
              "evidence": {
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Direct mention of Keras as the software framework used for training the UNet++ model."
                }
              },
              "id": "val-7p05vqx"
            }
          ]
        },
        "prop-009": {
          "property": "Preprocessing Steps",
          "label": "Preprocessing Steps",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "CT image filtering for valid lung fields",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-f4qukik",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "512×512 resolution normalization",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-1ph2w9q",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "lesion annotation by expert radiologists using smallest bounding rectangles",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-gdvknky",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "exclusion of images without clear lung fields",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-yey5wzn",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "confidence thresholding (cutoff: 0.50) and minimum lesion size filtering (≥25 pixels).",
              "confidence": 0.98,
              "evidence": {
                "Datasets": {
                  "text": "After filtering those images without good lung fields, 35,355 images were selected [...] The training images were labelled with the smallest rectangle containing all valid areas by researchers. [...] The suspicious region was predicted under a confidence cutoff value of 0.50, and a prediction box pixel of over 25.",
                  "relevance": "Explicitly describes filtering (valid lung fields), resolution normalization (implied by 512×512 input), annotation methodology (bounding rectangles), and post-processing thresholds (confidence/pixel cutoff)."
                },
                "Training algorithm": {
                  "text": "Taking the raw CT scan images as input with a resolution of 512×512, and the labelled map from the expert as output, UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Confirms resolution normalization (512×512) and expert-driven annotation (labelled maps) as preprocessing steps."
                }
              },
              "id": "val-fv3se04",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            }
          ]
        },
        "prop-010": {
          "property": "Training Hardware",
          "label": "Training Hardware",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-011": {
          "property": "Batch Size",
          "label": "Batch Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-012": {
          "property": "Optimizer",
          "label": "Optimizer",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-013": {
          "property": "Learning Rate",
          "label": "Learning Rate",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-014": {
          "property": "Loss Function",
          "label": "Loss Function",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-015": {
          "property": "Training Epochs",
          "label": "Training Epochs",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-016": {
          "property": "Regularization Techniques",
          "label": "Regularization Techniques",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-017": {
          "property": "Primary Metric",
          "label": "Primary Metric",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "per-patient accuracy",
              "confidence": 0.95,
              "evidence": {
                "Abstract": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Explicitly states the primary evaluation metric (per-patient accuracy) with its value (95.24%), directly aligning with the property definition of 'main evaluation metric'. This is the most prominent metric highlighted in the abstract, indicating its central role in model evaluation."
                }
              },
              "id": "val-8w8u6hn"
            },
            {
              "value": "per-image accuracy",
              "confidence": 0.92,
              "evidence": {
                "Abstract": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Explicitly mentions 'per-image accuracy' as a key metric (98.85%), which is a standard primary metric for pixel/lesion-level evaluation in medical imaging. While secondary to per-patient accuracy in the abstract, it is still framed as a core performance indicator."
                },
                "The performance of the model on retrospective dataset": {
                  "text": "A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Reinforces 'per-image accuracy' (98.85%) as a primary metric in the dedicated performance section, listed alongside other standard metrics but positioned first, suggesting its importance."
                }
              },
              "id": "val-jhzrewo"
            }
          ]
        },
        "prop-018": {
          "property": "Metric Value",
          "label": "Metric Value",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Per-patient Accuracy: 95.24%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-tsp8l6y"
            },
            {
              "value": "Per-image Accuracy: 98.85%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-kprxcaj"
            },
            {
              "value": "Per-patient Sensitivity: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-br67dlw"
            },
            {
              "value": "Per-patient Specificity: 93.55%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-fppzvag"
            },
            {
              "value": "Per-patient PPV: 84.62%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-8sthpep"
            },
            {
              "value": "Per-patient NPV: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-jarcpvs"
            },
            {
              "value": "Per-image Sensitivity: 94.34%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-st7j7q2"
            },
            {
              "value": "Per-image Specificity: 99.16%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-eqhl9k3"
            },
            {
              "value": "Per-image PPV: 88.37%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-73bzqtn"
            },
            {
              "value": "Per-image NPV: 99.61%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. [...] the model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%. A per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61%.",
                  "relevance": "Direct verbatim metrics from the retrospective dataset evaluation section, representing primary quantitative performance results of the deep learning model for COVID-19 pneumonia detection on CT images."
                }
              },
              "id": "val-vggd8to"
            },
            {
              "value": "Prospective Dataset Accuracy: 92.59%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ql2efnd"
            },
            {
              "value": "Prospective Dataset Sensitivity: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-bid7xki"
            },
            {
              "value": "Prospective Dataset Specificity: 81.82%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-2wyk0ly"
            },
            {
              "value": "Prospective Dataset PPV: 88.89%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ytfdb56"
            },
            {
              "value": "Prospective Dataset NPV: 100%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-lst8h4w"
            },
            {
              "value": "External Dataset Accuracy: 96%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-vin792k"
            },
            {
              "value": "External Dataset Sensitivity: 98%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ybidoxr"
            },
            {
              "value": "External Dataset Specificity: 94%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-ke2hba7"
            },
            {
              "value": "External Dataset PPV: 94.23%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-xkus4kh"
            },
            {
              "value": "External Dataset NPV: 97.92%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. [...] the model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. [...] The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Direct quantitative metrics from prospective (27 patients) and external (100 patients) dataset evaluations, demonstrating model robustness and generalizability across different clinical sites (Renmin Hospital of Wuhan University and Qianjiang Central Hospital)."
                }
              },
              "id": "val-p5t4ene"
            },
            {
              "value": "Radiologist Reading Time Reduction: 65%",
              "confidence": 0.95,
              "evidence": {
                "Results": {
                  "text": "With the assistance of the model, the reading time of radiologists was greatly decreased by 65%. [...] The average prediction time for model was 41.34 s per patient (IQR 39.76–44.48). [...] the average reading time for [the expert radiologist] to determine whether each patient has viral pneumonia was 116.12 s per case (IQR 85.69–118.17). [...] the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Direct evidence of efficiency gains (time reduction) achieved by the AI model, including absolute and relative metrics for radiologist workflow improvement. The 65% reduction is explicitly stated, while the derived ~40.64s is calculated from the 116.12s baseline (116.12 * (1 - 0.65) ≈ 40.64)."
                }
              },
              "id": "val-7fae6od"
            }
          ]
        },
        "prop-019": {
          "property": "Inference Time",
          "label": "Inference Time",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Prediction Time: 41.34 seconds per patient (IQR 39.76–44.48)",
              "confidence": 0.95,
              "evidence": {
                "The performance of the model in consecutive prospective patients": {
                  "text": "The average prediction time for model was 41.34 s per patient (IQR 39.76–44.48).",
                  "relevance": "Directly states the model's average inference time per patient, including interquartile range for context."
                }
              },
              "id": "val-hu9wacd"
            }
          ]
        },
        "prop-020": {
          "property": "Interpretability Method",
          "label": "Interpretability Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prop-021": {
          "property": "Clinical Validation",
          "label": "Clinical Validation",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Retrospective validation with 46,096 CT images from 106 patients (51 COVID-19, 55 controls) at Renmin Hospital of Wuhan University",
              "confidence": 0.98,
              "evidence": {
                "Abstract": {
                  "text": "For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases in Renmin Hospital of Wuhan University were retrospectively collected. The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset.",
                  "relevance": "Direct description of retrospective clinical validation metrics and dataset composition."
                }
              },
              "id": "val-cfig15g"
            },
            {
              "value": "Prospective validation with 27 consecutive patients (13,911 images) at Renmin Hospital of Wuhan University",
              "confidence": 0.99,
              "evidence": {
                "Testing of the model in retrospective data": {
                  "text": "Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. The model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. Inconsistent results between the expert and model were reviewed by three radiologists (expert + 2 senior radiologists with 10+ years experience).",
                  "relevance": "Explicit details of prospective study design, comparator (expert radiologist), and multi-radiologist review process."
                },
                "Evaluating the efficiency of radiologist with the assistance of AI": {
                  "text": "After 10 days of wash out period, the same expert radiologist re-read all CT images of 27 prospective patients with the assistance of the AI model. The results for determining whether each patient has viral pneumonia were not changed, while the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Confirms diagnostic agreement between AI and expert radiologist in prospective setting."
                }
              },
              "id": "val-hlb3zgj"
            },
            {
              "value": "expert radiologist (30 years experience) as comparator with 100% diagnostic agreement after AI-assisted review.",
              "confidence": 0.99,
              "evidence": {
                "Testing of the model in retrospective data": {
                  "text": "Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. The model achieved a per-patient sensitivity of 100%, accuracy of 92.59%, specificity of 81.82%, PPV of 88.89% and NPV of 100% in the 27 prospective patients. Inconsistent results between the expert and model were reviewed by three radiologists (expert + 2 senior radiologists with 10+ years experience).",
                  "relevance": "Explicit details of prospective study design, comparator (expert radiologist), and multi-radiologist review process."
                },
                "Evaluating the efficiency of radiologist with the assistance of AI": {
                  "text": "After 10 days of wash out period, the same expert radiologist re-read all CT images of 27 prospective patients with the assistance of the AI model. The results for determining whether each patient has viral pneumonia were not changed, while the average reading time of the expert was greatly decreased by 65%.",
                  "relevance": "Confirms diagnostic agreement between AI and expert radiologist in prospective setting."
                }
              },
              "id": "val-2cs8qal"
            },
            {
              "value": "External validation with 100 patients (13,734 COVID-19 images, 17,030 control images) from Qianjiang Central Hospital",
              "confidence": 0.97,
              "evidence": {
                "The performance of the model on external dataset": {
                  "text": "To estimate the robustness of the system, an external test was conducted using a dataset containing 100 patients from Qianjiang Central Hospital, China. Among them, 50 patients were COVID-19 patients (13,734 images), and 50 were normal control patients (17,030 images). The system achieved an accuracy of 96%, a sensitivity of 98%, a specificity of 94%, a PPV of 94.23% and an NPV of 97.92% in external dataset.",
                  "relevance": "Comprehensive external validation metrics and dataset composition."
                }
              },
              "id": "val-5ntg9do"
            },
            {
              "value": "Reader study: 1 expert radiologist (30 years experience, 300+ viral pneumonia cases) vs. AI model",
              "confidence": 0.95,
              "evidence": {
                "Evaluating the efficiency of radiologist in the traditional way": {
                  "text": "An expert radiologist (associate chief physician, 30 years experience, independently diagnosed ~300 viral pneumonia cases) read 27 prospective patients with average time of 116.12s per case. With AI assistance, reading time decreased by 65% to 41.34s per patient, with identical diagnostic results.",
                  "relevance": "Quantitative comparison of AI-assisted vs. traditional radiologist workflow efficiency."
                },
                "Comparison between the efficiency of radiologist with or without the assistance of AI": {
                  "text": "The average reading time of the expert was greatly decreased by 65% with AI assistance, while diagnostic results remained unchanged.",
                  "relevance": "Confirms non-inferiority of AI-assisted diagnostics with time efficiency gains."
                }
              },
              "id": "val-6oxmgvz"
            },
            {
              "value": "Multi-radiologist consensus labeling: 3 radiologists (>5 years experience) labeled COVID-19 lesions in training dataset",
              "confidence": 0.93,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Describes ground truth generation process via multi-expert consensus."
                }
              },
              "id": "val-slk9a0p"
            },
            {
              "value": "testing set labels combined by consensus.",
              "confidence": 0.93,
              "evidence": {
                "Datasets": {
                  "text": "Three radiologists with more than 5 years of clinical experience labelled infection lesions of COVID-19 pneumonia patients in training dataset, and selected images containing COVID19 pneumonia lesions in testing set, and their labels were combined by consensus.",
                  "relevance": "Describes ground truth generation process via multi-expert consensus."
                }
              },
              "id": "val-q71lpxw"
            }
          ]
        },
        "prop-022": {
          "property": "Deployment Framework",
          "label": "Deployment Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Keras",
              "confidence": 0.95,
              "evidence": {
                "Training algorithm": {
                  "text": "UNet++ was used to train in Keras in an image-to-image manner.",
                  "relevance": "Direct mention of the deployment framework (Keras) used for training the UNet++ model, which is central to the study's implementation."
                }
              },
              "id": "val-6cozhzp"
            },
            {
              "value": "GitHub",
              "confidence": 0.9,
              "evidence": {
                "Introduction": {
                  "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel.",
                  "relevance": "Explicit reference to GitHub as the platform for hosting the model's source code and deployment resources, indicating its role in the framework's accessibility."
                }
              },
              "id": "val-j8txtwh"
            }
          ]
        },
        "prop-023": {
          "property": "Source Code Repository",
          "label": "Source Code Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://github.com/endo-angel/ct-angel",
              "confidence": 1,
              "evidence": {
                "Introduction": {
                  "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel, and an open-access website has been made available to provide free provide to the present system (https://121.40.75.149/znyx-ncov/index).",
                  "relevance": "Direct mention of the GitHub repository URL for the source code of the deep learning-based model, explicitly labeled as the 'source code' location."
                }
              },
              "id": "val-14tw4j3"
            }
          ]
        },
        "prop-024": {
          "property": "Paper DOI",
          "label": "Paper DOI",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "10.1038/s41598-020-76282-0",
              "confidence": 1,
              "evidence": {
                "Cite this article": {
                  "text": "Chen, J., Wu, L., Zhang, J. et al. Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography. Sci Rep 10, 19196 (2020). https://doi.org/10.1038/s41598-020-76282-0",
                  "relevance": "Direct mention of the DOI in the citation section of the paper, providing unambiguous evidence of the Digital Object Identifier for the published work."
                }
              },
              "id": "val-9bqg8bz"
            }
          ]
        }
      },
      "changes": {
        "modified_properties": {
          "prop-022": {
            "old_values": [
              {
                "value": "Keras",
                "confidence": 0.95,
                "evidence": {
                  "Training algorithm": {
                    "text": "UNet++ was used to train in Keras in an image-to-image manner.",
                    "relevance": "Direct mention of the deployment framework (Keras) used for training the UNet++ model, which is central to the study's implementation."
                  }
                },
                "id": "val-6cozhzp"
              },
              {
                "value": "GitHub",
                "confidence": 0.9,
                "evidence": {
                  "Introduction": {
                    "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel.",
                    "relevance": "Explicit reference to GitHub as the platform for hosting the model's source code and deployment resources, indicating its role in the framework's accessibility."
                  }
                },
                "id": "val-j8txtwh"
              },
              {
                "value": "Open-access website",
                "confidence": 0.85,
                "evidence": {
                  "Introduction": {
                    "text": "an open-access website has been made available to provide free provide to the present system (https://121.40.75.149/znyx-ncov/index).",
                    "relevance": "Describes a web-based deployment framework for the model, serving as an interface for clinical use and global access."
                  },
                  "Discussion": {
                    "text": "a cloud-based open-access artificial intelligence platform was constructed to provide assistance for detecting COVID-19 pneumonia worldwide.",
                    "relevance": "Reinforces the role of the open-access website as a deployment framework for the model, emphasizing its scalability and accessibility."
                  }
                },
                "id": "val-rxttq4h"
              }
            ],
            "new_values": [
              {
                "value": "Keras",
                "confidence": 0.95,
                "evidence": {
                  "Training algorithm": {
                    "text": "UNet++ was used to train in Keras in an image-to-image manner.",
                    "relevance": "Direct mention of the deployment framework (Keras) used for training the UNet++ model, which is central to the study's implementation."
                  }
                },
                "id": "val-6cozhzp"
              },
              {
                "value": "GitHub",
                "confidence": 0.9,
                "evidence": {
                  "Introduction": {
                    "text": "The module and source code developed in this work were shared for global researches in https://github.com/endo-angel/ct-angel.",
                    "relevance": "Explicit reference to GitHub as the platform for hosting the model's source code and deployment resources, indicating its role in the framework's accessibility."
                  }
                },
                "id": "val-j8txtwh"
              }
            ],
            "old_type": "resource",
            "new_type": "resource"
          }
        },
        "changes_summary": {
          "text_updates": 0,
          "type_updates": 0,
          "deletions": 1,
          "additions": 0
        }
      },
      "timestamp": "2025-11-20T19:08:09.946Z"
    },
    "completedSteps": {
      "metadata": true,
      "researchFields": true,
      "researchProblems": true,
      "template": true,
      "paperContent": true
    },
    "timestamp": "2025-11-20T19:08:09.947Z"
  }
}