{
  "token": "eval_1763378414253_dr89e43rv",
  "metadata": {
    "title": "A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19)",
    "authors": [
      "Shuai Wang",
      "Bo Kang",
      "Jinlu Ma",
      "Xianjun Zeng",
      "Mingming Xiao",
      "Jia Guo",
      "Mengjiao Cai",
      "Jingyi Yang",
      "Yaodong Li",
      "Xiangfei Meng",
      "Bo Xu"
    ],
    "abstract": "Abstract\n                Objective\n                The outbreak of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-COV-2) has caused more than 26 million cases of Corona virus disease (COVID-19) in the world so far. To control the spread of the disease, screening large numbers of suspected cases for appropriate quarantine and treatment are a priority. Pathogenic laboratory testing is typically the gold standard, but it bears the burden of significant false negativity, adding to the urgent need of alternative diagnostic methods to combat the disease. Based on COVID-19 radiographic changes in CT images, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19 and provide a clinical diagnosis ahead of the pathogenic test, thus saving critical time for disease control.\n              \n                Methods\n                We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.\n              \n                Results\n                The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.\n              \n                Conclusion\n                These results demonstrate the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis.\n              \n                Key Points\n                • The study evaluated the diagnostic performance of a deep learning algorithm using CT images to screen for COVID-19 during the influenza season.\n                • As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.\n                • The model was used to distinguish between COVID-19 and other typical viral pneumonia, both of which have quite similar radiologic characteristics.\n              ",
    "doi": "10.1007/s00330-021-07715-1",
    "url": "http://dx.doi.org/10.1007/s00330-021-07715-1",
    "publicationDate": "2021-02-28T23:00:00.000Z",
    "status": "success",
    "venue": "European Radiology"
  },
  "researchFields": {
    "fields": [
      {
        "id": "R112133",
        "name": "Image and Video Processing",
        "score": 5.4591569900512695,
        "description": ""
      },
      {
        "id": "R114138",
        "name": "Medical Physics",
        "score": 4.9425153732299805,
        "description": ""
      },
      {
        "id": "R104",
        "name": "Bioinformatics",
        "score": 4.005127429962158,
        "description": ""
      },
      {
        "id": "R114149",
        "name": "Tissues and Organs",
        "score": 3.6076555252075195,
        "description": ""
      },
      {
        "id": "R112118",
        "name": "Computer Vision and Pattern Recognition",
        "score": 3.5097458362579346,
        "description": ""
      }
    ],
    "selectedField": {
      "id": "R112133",
      "name": "Image and Video Processing",
      "score": 5.4591569900512695,
      "description": ""
    },
    "status": "completed",
    "processing_info": {
      "step": "researchFields",
      "status": {
        "status": "completed",
        "step": "researchFields",
        "progress": 100,
        "message": "Research fields identified successfully",
        "timestamp": "2025-11-17T10:24:54.564Z"
      },
      "progress": 100,
      "message": "Research fields identified successfully",
      "timestamp": "2025-11-17T10:24:54.564Z"
    }
  },
  "researchProblems": {
    "orkg_problems": [],
    "llm_problem": {
      "title": "Accelerating Disease Diagnosis Through AI-Driven Radiological Feature Extraction",
      "problem": "The challenge of developing rapid, accurate, and scalable diagnostic methods for infectious diseases—particularly in outbreak scenarios—where traditional laboratory testing may suffer from delays, false negatives, or limited availability. This requires leveraging alternative data sources (e.g., medical imaging) and computational techniques (e.g., AI) to augment or preempt conventional diagnostic workflows, ensuring timely intervention and resource allocation.",
      "domain": "Medical Diagnostics / Computational Healthcare",
      "impact": "Potential to revolutionize outbreak response by enabling early detection, reducing diagnostic bottlenecks, and improving patient triage in resource-constrained settings. Applications extend beyond COVID-19 to other infectious diseases (e.g., tuberculosis, influenza) and non-infectious conditions with distinct radiological signatures (e.g., cancers, neurological disorders).",
      "motivation": "Timely and accurate diagnosis is critical for controlling disease spread, optimizing treatment, and allocating healthcare resources efficiently. AI-driven methods can bridge gaps in traditional testing, especially during surges in demand or when laboratory infrastructure is overwhelmed.",
      "confidence": 0.95,
      "explanation": "The abstract explicitly frames the core problem—need for faster, reliable diagnostics during outbreaks—as a systemic challenge in healthcare. The proposed solution (AI + radiological data) is broadly applicable to other diseases with imaging biomarkers, and the motivation (outbreak control, resource efficiency) is universally relevant. The high confidence stems from the clarity of the problem statement and its detachment from COVID-19-specific details (e.g., the focus on 'radiological features' and 'alternative diagnostic methods' generalizes well).",
      "model": "mistral-medium",
      "timestamp": "2025-11-17T10:25:40.427Z",
      "description": "The challenge of developing rapid, accurate, and scalable diagnostic methods for infectious diseases—particularly in outbreak scenarios—where traditional laboratory testing may suffer from delays, false negatives, or limited availability. This requires leveraging alternative data sources (e.g., medical imaging) and computational techniques (e.g., AI) to augment or preempt conventional diagnostic workflows, ensuring timely intervention and resource allocation.",
      "isLLMGenerated": true,
      "lastEdited": "2025-11-17T10:25:41.276Z"
    },
    "metadata": {
      "total_scanned": 0,
      "total_identified": 0,
      "total_similar": 0,
      "total_valid": 0,
      "field_id": "",
      "similarities_found": 0,
      "threshold_used": 0.5,
      "max_similarity": 0
    },
    "processing_info": {
      "step": "",
      "status": "completed",
      "progress": 0,
      "message": "",
      "timestamp": null
    },
    "selectedProblem": {
      "title": "Accelerating Disease Diagnosis Through AI-Driven Radiological Feature Extraction",
      "description": "The challenge of developing rapid, accurate, and scalable diagnostic methods for infectious diseases—particularly in outbreak scenarios—where traditional laboratory testing may suffer from delays, false negatives, or limited availability. This requires leveraging alternative data sources (e.g., medical imaging) and computational techniques (e.g., AI) to augment or preempt conventional diagnostic workflows, ensuring timely intervention and resource allocation.",
      "isLLMGenerated": true,
      "confidence": 0.95
    },
    "original_llm_problem": {
      "title": "Accelerating Disease Diagnosis Through AI-Driven Radiological Feature Extraction",
      "problem": "The challenge of developing rapid, accurate, and scalable diagnostic methods for infectious diseases—particularly in outbreak scenarios—where traditional laboratory testing may suffer from delays, false negatives, or limited availability. This requires leveraging alternative data sources (e.g., medical imaging) and computational techniques (e.g., AI) to augment or preempt conventional diagnostic workflows, ensuring timely intervention and resource allocation.",
      "confidence": 0.95,
      "explanation": "The abstract explicitly frames the core problem—need for faster, reliable diagnostics during outbreaks—as a systemic challenge in healthcare. The proposed solution (AI + radiological data) is broadly applicable to other diseases with imaging biomarkers, and the motivation (outbreak control, resource efficiency) is universally relevant. The high confidence stems from the clarity of the problem statement and its detachment from COVID-19-specific details (e.g., the focus on 'radiological features' and 'alternative diagnostic methods' generalizes well).",
      "domain": "Medical Diagnostics / Computational Healthcare",
      "impact": "Potential to revolutionize outbreak response by enabling early detection, reducing diagnostic bottlenecks, and improving patient triage in resource-constrained settings. Applications extend beyond COVID-19 to other infectious diseases (e.g., tuberculosis, influenza) and non-infectious conditions with distinct radiological signatures (e.g., cancers, neurological disorders).",
      "motivation": "Timely and accurate diagnosis is critical for controlling disease spread, optimizing treatment, and allocating healthcare resources efficiently. AI-driven methods can bridge gaps in traditional testing, especially during surges in demand or when laboratory infrastructure is overwhelmed.",
      "model": "mistral-medium",
      "timestamp": "2025-11-17T10:25:40.427Z"
    }
  },
  "templates": {
    "available": {
      "template": {
        "id": "aidd-rfe-2024-05",
        "name": "AI-Driven Radiological Feature Extraction for Accelerated Disease Diagnosis",
        "description": "Technical template for developing and evaluating AI models that extract diagnostic features from radiological images (X-ray, CT, MRI) to accelerate infectious disease detection, with emphasis on computational efficiency, scalability, and integration with clinical workflows.",
        "properties": [
          {
            "id": "prob-001",
            "label": "Primary Imaging Modality",
            "description": "The type of radiological imaging used as input (e.g., X-ray, CT, MRI, Ultrasound).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "allowed_values": [
                "X-ray",
                "CT",
                "MRI",
                "Ultrasound",
                "PET",
                "Multi-modal"
              ]
            }
          },
          {
            "id": "prob-002",
            "label": "Primary Dataset",
            "description": "Named dataset(s) used for training/validation (e.g., MIMIC-CXR, CheXpert, COVIDx).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-003",
            "label": "Dataset Size (Training)",
            "description": "Number of annotated images/videos in the training set.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 100,
              "max": 1000000
            }
          },
          {
            "id": "prob-004",
            "label": "Dataset Size (Validation)",
            "description": "Number of annotated images/videos in the validation set.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 10,
              "max": 500000
            }
          },
          {
            "id": "prob-005",
            "label": "Dataset Size (Test)",
            "description": "Number of annotated images/videos in the held-out test set.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 10,
              "max": 500000
            }
          },
          {
            "id": "prob-006",
            "label": "Image Resolution (Width x Height)",
            "description": "Standardized input resolution for the model (e.g., 224x224, 512x512).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "pattern": "^\\d{2,4}x\\d{2,4}(x\\d{2,4})?$"
            }
          },
          {
            "id": "prob-007",
            "label": "Preprocessing Pipeline",
            "description": "Detailed steps for image normalization, augmentation, and artifact removal (e.g., histogram equalization, windowing for CT, noise reduction).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-008",
            "label": "Feature Extraction Backbone",
            "description": "Base model architecture used for feature extraction (e.g., ResNet50, EfficientNet, Vision Transformer, 3D CNN).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-009",
            "label": "Custom Layer Configuration",
            "description": "Description of custom layers added to the backbone (e.g., attention mechanisms, spatial pyramids, multi-scale fusion).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-010",
            "label": "Model Parameters (Total)",
            "description": "Total number of trainable parameters in the model.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1000,
              "max": 1000000000
            }
          },
          {
            "id": "prob-011",
            "label": "Training Framework",
            "description": "Software framework used for model development (e.g., PyTorch, TensorFlow, JAX).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-012",
            "label": "Hardware Accelerator",
            "description": "Primary hardware used for training/inference (e.g., NVIDIA A100, TPU v4, CPU-only).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-013",
            "label": "Batch Size",
            "description": "Number of samples processed in a single forward/backward pass.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 1024
            }
          },
          {
            "id": "prob-014",
            "label": "Optimizer",
            "description": "Optimization algorithm used (e.g., Adam, SGD, RMSprop).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-015",
            "label": "Learning Rate",
            "description": "Initial learning rate used during training.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0.000001,
              "max": 1
            }
          },
          {
            "id": "prob-016",
            "label": "Learning Rate Schedule",
            "description": "Strategy for adjusting learning rate (e.g., cosine decay, step decay, plateau reduction).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-017",
            "label": "Training Epochs",
            "description": "Total number of epochs the model was trained for.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 1000
            }
          },
          {
            "id": "prob-018",
            "label": "Early Stopping Patience",
            "description": "Number of epochs to wait before early stopping if validation metrics plateau.",
            "type": "number",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 50
            }
          },
          {
            "id": "prob-019",
            "label": "Loss Function",
            "description": "Primary loss function used (e.g., cross-entropy, Dice loss, focal loss).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-020",
            "label": "Class Imbalance Handling",
            "description": "Techniques used to address class imbalance (e.g., weighted loss, oversampling, SMOTE).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-021",
            "label": "Inference Latency (ms)",
            "description": "Average time per image for model inference on target hardware (milliseconds).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 10000
            }
          },
          {
            "id": "prob-022",
            "label": "Throughput (images/sec)",
            "description": "Images processed per second on target hardware (batch size = 1).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0.1,
              "max": 1000
            }
          },
          {
            "id": "prob-023",
            "label": "Primary Evaluation Metric",
            "description": "Main metric used to evaluate model performance (e.g., AUC-ROC, F1, sensitivity).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-024",
            "label": "Primary Metric Score",
            "description": "Achieved score for the primary evaluation metric (e.g., AUC-ROC = 0.95).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-025",
            "label": "Sensitivity (Recall)",
            "description": "True positive rate for disease detection.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-026",
            "label": "Specificity",
            "description": "True negative rate for disease detection.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-027",
            "label": "Precision",
            "description": "Positive predictive value for disease detection.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-028",
            "label": "F1 Score",
            "description": "Harmonic mean of precision and recall.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-029",
            "label": "False Negative Rate",
            "description": "Proportion of missed positive cases (critical for infectious diseases).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-030",
            "label": "Confidence Calibration",
            "description": "Method used to calibrate model confidence scores (e.g., temperature scaling, Dirichlet calibration).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-031",
            "label": "Explainability Method",
            "description": "Technique used to interpret model decisions (e.g., Grad-CAM, LIME, SHAP).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-032",
            "label": "Deployment Framework",
            "description": "Framework used for model deployment (e.g., TensorRT, ONNX, TFLite).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-033",
            "label": "Model Quantization",
            "description": "Technique used to reduce model size (e.g., FP32 to INT8, pruning).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-034",
            "label": "Clinical Integration Method",
            "description": "Approach for integrating model outputs into clinical workflows (e.g., DICOM integration, PACS plugin, EHR API).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-035",
            "label": "Regulatory Compliance",
            "description": "Standards adhered to for clinical use (e.g., FDA 510(k), CE Marking, HIPAA).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-036",
            "label": "Benchmark Comparison",
            "description": "Named benchmarks or prior works the model was compared against (e.g., radiologist performance, state-of-the-art models).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-037",
            "label": "Limitations and Failure Modes",
            "description": "Documented scenarios where the model underperforms (e.g., rare diseases, low-quality images, edge cases).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-038",
            "label": "Data Collection Period",
            "description": "Timeframe during which the dataset was collected (e.g., 2020-2023).",
            "type": "date",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-039",
            "label": "Source Code Repository",
            "description": "Link to the public/private repository hosting the implementation.",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "pattern": "^https?://"
            }
          },
          {
            "id": "prob-040",
            "label": "Paper DOI",
            "description": "Digital Object Identifier for the published work (if applicable).",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "pattern": "^10\\.\\d{4,9}/[-._;()/:A-Z0-9]+$"
            }
          }
        ],
        "metadata": {
          "research_field": "Image and Video Processing",
          "research_category": "Medical AI for Diagnostic Acceleration",
          "adaptability_score": 0.85,
          "total_properties": 40,
          "suggested_sections": [
            "Introduction",
            "Related Work",
            "Dataset Description",
            "Methodology",
            "Preprocessing",
            "Model Architecture",
            "Training Protocol",
            "Evaluation Metrics",
            "Results",
            "Clinical Integration",
            "Limitations",
            "Conclusion"
          ],
          "creation_timestamp": "2025-11-17T10:28:34.104Z",
          "model": "mistral-medium",
          "template_version": "1.0"
        }
      }
    },
    "selectedTemplate": null,
    "llm_template": {
      "template": {
        "id": "aidd-rfe-2024-05",
        "name": "AI-Driven Radiological Feature Extraction for Accelerated Disease Diagnosis",
        "description": "Technical template for developing and evaluating AI models that extract diagnostic features from radiological images (X-ray, CT, MRI) to accelerate infectious disease detection, with emphasis on computational efficiency, scalability, and integration with clinical workflows.",
        "properties": [
          {
            "id": "prob-001",
            "label": "Data type",
            "description": "The type of radiological imaging used as input (e.g., X-ray, CT, MRI, Ultrasound).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "allowed_values": [
                "X-ray",
                "CT",
                "MRI",
                "Ultrasound",
                "PET",
                "Multi-modal"
              ]
            }
          },
          {
            "id": "prob-002",
            "label": "Primary Dataset",
            "description": "Private dataset",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-003",
            "label": "Dataset Size",
            "description": "1065 CT images",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 100,
              "max": 1000000
            }
          },
          {
            "id": "prob-006",
            "label": "Image Resolution (Width x Height)",
            "description": "Standardized input resolution for the model (e.g., 224x224, 512x512).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "pattern": "^\\d{2,4}x\\d{2,4}(x\\d{2,4})?$"
            }
          },
          {
            "id": "prob-007",
            "label": "Preprocessing Pipeline",
            "description": "Detailed steps for image normalization, augmentation, and artifact removal (e.g., histogram equalization, windowing for CT, noise reduction).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-008",
            "label": "Feature Extraction Backbone",
            "description": "Base model architecture used for feature extraction (e.g., ResNet50, EfficientNet, Vision Transformer, 3D CNN).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-009",
            "label": "Custom Layer Configuration",
            "description": "Description of custom layers added to the backbone (e.g., attention mechanisms, spatial pyramids, multi-scale fusion).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-010",
            "label": "Model Parameters (Total)",
            "description": "Total number of trainable parameters in the model.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1000,
              "max": 1000000000
            }
          },
          {
            "id": "prob-011",
            "label": "Training Framework",
            "description": "Software framework used for model development (e.g., PyTorch, TensorFlow, JAX).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-012",
            "label": "Hardware Accelerator",
            "description": "Primary hardware used for training/inference (e.g., NVIDIA A100, TPU v4, CPU-only).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-013",
            "label": "Batch Size",
            "description": "Number of samples processed in a single forward/backward pass.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 1024
            }
          },
          {
            "id": "prob-014",
            "label": "Optimizer",
            "description": "Optimization algorithm used (e.g., Adam, SGD, RMSprop).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-015",
            "label": "Learning Rate",
            "description": "Initial learning rate used during training.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0.000001,
              "max": 1
            }
          },
          {
            "id": "prob-016",
            "label": "Learning Rate Schedule",
            "description": "Strategy for adjusting learning rate (e.g., cosine decay, step decay, plateau reduction).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-017",
            "label": "Training Epochs",
            "description": "Total number of epochs the model was trained for.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 1000
            }
          },
          {
            "id": "prob-018",
            "label": "Early Stopping Patience",
            "description": "Number of epochs to wait before early stopping if validation metrics plateau.",
            "type": "number",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 50
            }
          },
          {
            "id": "prob-019",
            "label": "Loss Function",
            "description": "Primary loss function used (e.g., cross-entropy, Dice loss, focal loss).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-020",
            "label": "Class Imbalance Handling",
            "description": "Techniques used to address class imbalance (e.g., weighted loss, oversampling, SMOTE).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-021",
            "label": "Inference Latency (ms)",
            "description": "Average time per image for model inference on target hardware (milliseconds).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 1,
              "max": 10000
            }
          },
          {
            "id": "prob-022",
            "label": "Throughput (images/sec)",
            "description": "Images processed per second on target hardware (batch size = 1).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0.1,
              "max": 1000
            }
          },
          {
            "id": "prob-023",
            "label": "Primary Evaluation Metric",
            "description": "Main metric used to evaluate model performance (e.g., AUC-ROC, F1, sensitivity).",
            "type": "resource",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-024",
            "label": "Primary Metric Score",
            "description": "Achieved score for the primary evaluation metric (e.g., AUC-ROC = 0.95).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-025",
            "label": "Sensitivity (Recall)",
            "description": "True positive rate for disease detection.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-026",
            "label": "Specificity",
            "description": "True negative rate for disease detection.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-027",
            "label": "Precision",
            "description": "Positive predictive value for disease detection.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-028",
            "label": "F1 Score",
            "description": "Harmonic mean of precision and recall.",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-029",
            "label": "False Negative Rate",
            "description": "Proportion of missed positive cases (critical for infectious diseases).",
            "type": "number",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "min": 0,
              "max": 1
            }
          },
          {
            "id": "prob-030",
            "label": "Confidence Calibration",
            "description": "Method used to calibrate model confidence scores (e.g., temperature scaling, Dirichlet calibration).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-031",
            "label": "Explainability Method",
            "description": "Technique used to interpret model decisions (e.g., Grad-CAM, LIME, SHAP).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-032",
            "label": "Deployment Framework",
            "description": "Framework used for model deployment (e.g., TensorRT, ONNX, TFLite).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-033",
            "label": "Model Quantization",
            "description": "Technique used to reduce model size (e.g., FP32 to INT8, pruning).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-034",
            "label": "Clinical Integration Method",
            "description": "Approach for integrating model outputs into clinical workflows (e.g., DICOM integration, PACS plugin, EHR API).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-035",
            "label": "Regulatory Compliance",
            "description": "Standards adhered to for clinical use (e.g., FDA 510(k), CE Marking, HIPAA).",
            "type": "text",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-036",
            "label": "Benchmark Comparison",
            "description": "Named benchmarks or prior works the model was compared against (e.g., radiologist performance, state-of-the-art models).",
            "type": "resource",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-037",
            "label": "Limitations and Failure Modes",
            "description": "Documented scenarios where the model underperforms (e.g., rare diseases, low-quality images, edge cases).",
            "type": "text",
            "required": true,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-038",
            "label": "Data Collection Period",
            "description": "Timeframe during which the dataset was collected (e.g., 2020-2023).",
            "type": "date",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {}
          },
          {
            "id": "prob-039",
            "label": "Source Code Repository",
            "description": "Link to the public/private repository hosting the implementation.",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "pattern": "^https?://"
            }
          },
          {
            "id": "prob-040",
            "label": "Paper DOI",
            "description": "Digital Object Identifier for the published work (if applicable).",
            "type": "url",
            "required": false,
            "value": null,
            "confidence": null,
            "evidence": null,
            "source_section": null,
            "validation_rules": {
              "pattern": "^10\\.\\d{4,9}/[-._;()/:A-Z0-9]+$"
            }
          }
        ],
        "metadata": {
          "research_field": "Image and Video Processing",
          "research_category": "Medical AI for Diagnostic Acceleration",
          "adaptability_score": 0.85,
          "total_properties": 38,
          "suggested_sections": [
            "Introduction",
            "Related Work",
            "Dataset Description",
            "Methodology",
            "Preprocessing",
            "Model Architecture",
            "Training Protocol",
            "Evaluation Metrics",
            "Results",
            "Clinical Integration",
            "Limitations",
            "Conclusion"
          ],
          "creation_timestamp": "2025-11-17T10:28:34.104Z",
          "model": "mistral-medium",
          "template_version": "1.0"
        }
      }
    },
    "status": "success",
    "processing_info": {
      "step": "template",
      "status": "completed",
      "progress": 100,
      "message": "Template generated successfully",
      "timestamp": "2025-11-17T10:28:34.105Z"
    }
  },
  "paperContent": {
    "paperContent": {
      "prob-001": {
        "property": "Data type",
        "label": "Data type",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "CT images",
            "confidence": 1,
            "evidence": {
              "Abstract": {
                "text": "Based on COVID-19 radiographic changes in **CT images**, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19...",
                "relevance": "Explicitly states the use of **CT images** as the primary radiological input for the deep learning algorithm."
              },
              "Methods": {
                "text": "We collected **1065 CT images** of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                "relevance": "Confirms the dataset consists exclusively of **CT images** for model training and validation."
              },
              "Retrospective collection of datasets": {
                "text": "We retrospectively collected **CT images** from 259 patients...",
                "relevance": "Reiterates the use of **CT images** as the sole imaging modality in the study."
              },
              "Introduction": {
                "text": "Another major diagnostic tool for COVID-19 is **radiological imaging**. The majority of COVID-19 cases have similar features on **CT images**...",
                "relevance": "Contextualizes **CT images** as the specific radiological data type analyzed."
              }
            },
            "id": "val-1u1aqoc"
          }
        ]
      },
      "prob-002": {
        "property": "Primary Dataset",
        "label": "Primary Dataset",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "COVID-19 CT",
            "confidence": 1,
            "evidence": {
              "Retrospective collection of datasets": {
                "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2. In addition, we enrolled 15 additional COVID cases, in which the first two nucleic acid tests were negative in initial diagnoses. The hospitals providing the images were the Xi’an Jiaotong University First Affiliated Hospital (center 1), Nanchang University First Hospital (center 2), and Xi’an No.8 Hospital of Xi’an Medical College (center 3). All CT images were reconfirmed before sending for analysis.",
                "relevance": "Directly describes the primary dataset used in the study, including patient cohorts, diagnostic criteria (nucleic acid testing), and participating centers. The dataset consists of CT images from 259 patients (180 typical viral pneumonia + 79 COVID-19 confirmed + 15 COVID-19 with initial negative tests), explicitly defining the resource as the 'COVID-19 CT Dataset' for this study."
              }
            },
            "id": "val-0jrhlkj"
          }
        ]
      },
      "prob-003": {
        "property": "Dataset Size",
        "label": "Dataset Size",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Total CT Images: 1065",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
              },
              "Algorithm development": {
                "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
              },
              "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
              }
            },
            "id": "val-e4h6izu",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "Training Set: 320 images (160 COVID-19 positive, 160 COVID-19 negative)",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
              },
              "Algorithm development": {
                "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
              },
              "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
              }
            },
            "id": "val-sojh9n0",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "Internal Validation Set: 455 images (95 COVID-19 positive, 360 COVID-19 negative)",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
              },
              "Algorithm development": {
                "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
              },
              "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
              }
            },
            "id": "val-t4lm1lj",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          },
          {
            "value": "External Validation Set: 290 images (70 COVID-19 positive, 220 COVID-19 negative)",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
              },
              "Algorithm development": {
                "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
              },
              "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
              }
            },
            "id": "val-ln7y4r2",
            "isMultiValue": true,
            "multiValueIndex": 3,
            "originalIndex": 0
          },
          {
            "value": "Additional COVID-19 Cases (Pathogenic-Negative): 54 images from 15 patients",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
              },
              "Algorithm development": {
                "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
              },
              "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
              }
            },
            "id": "val-whyt425",
            "isMultiValue": true,
            "multiValueIndex": 4,
            "originalIndex": 0
          }
        ]
      },
      "prob-006": {
        "property": "Image Resolution (Width x Height)",
        "label": "Image Resolution (Width x Height)",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "299 × 299 pixels",
            "confidence": 1,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model.",
                "relevance": "Directly states the standardized input resolution for the model as 299 × 299 pixels, which is the key technical specification for image preprocessing."
              },
              "Overview of the proposed architecture": {
                "text": "The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet.",
                "relevance": "Confirms the input resolution (299 × 299) for the modified inception network, reinforcing the consistency of this parameter across the study."
              }
            },
            "id": "val-vcw9z82"
          }
        ]
      },
      "prob-007": {
        "property": "Preprocessing Pipeline",
        "label": "Preprocessing Pipeline",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Grayscale conversion",
            "confidence": 0.98,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
              }
            },
            "id": "val-auiff8u",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "OSTU-based binarization with Vmin (80) and Vmax (200) thresholds",
            "confidence": 0.98,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
              }
            },
            "id": "val-rtf9uoe",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "flood-filling for background area",
            "confidence": 0.98,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
              }
            },
            "id": "val-yeh80ky",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          },
          {
            "value": "lung contour extraction via reverse-coloring of two largest regions",
            "confidence": 0.98,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
              }
            },
            "id": "val-2y414wm",
            "isMultiValue": true,
            "multiValueIndex": 3,
            "originalIndex": 0
          },
          {
            "value": "fixed resizing to 299×299 pixels with grayscale-to-RGB mapping (each channel = grayscale value).",
            "confidence": 0.98,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
              }
            },
            "id": "val-ktvdlkw",
            "isMultiValue": true,
            "multiValueIndex": 4,
            "originalIndex": 0
          }
        ]
      },
      "prob-008": {
        "property": "Feature Extraction Backbone",
        "label": "Feature Extraction Backbone",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "GoogleNet Inception v3",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                "relevance": "Directly states the use of 'inception' model, which is identified as GoogleNet Inception v3 in the 'Overview of the proposed architecture' section."
              },
              "Overview of the proposed architecture": {
                "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers. We performed transfer learning, which involved training with a predefined model using the well-known **GoogleNet Inception v3 CNN**.",
                "relevance": "Explicitly names **GoogleNet Inception v3 CNN** as the backbone for feature extraction, confirming the resource type."
              }
            },
            "id": "val-mt694ol"
          },
          {
            "value": "M-inception",
            "confidence": 0.95,
            "evidence": {
              "Image preprocessing and feature extraction": {
                "text": "We modified the typical inception network and fine-tuned the **modified inception (M-inception)** model with pre-trained weights.",
                "relevance": "Introduces **M-inception** as a customized variant of GoogleNet Inception v3, explicitly used for feature extraction in this study."
              }
            },
            "id": "val-2oogbtl"
          }
        ]
      },
      "prob-009": {
        "property": "Custom Layer Configuration",
        "label": "Custom Layer Configuration",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Modified inception network with reduced feature dimension in the final fully connected layer before classification",
            "confidence": 0.98,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm... The difference between the inception and M-inception model is found in the last of the fully connected layers. We reduced the dimension of the features before it was sent to the final classification layer. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                "relevance": "Direct technical description of the custom layer modifications, including dimensionality reduction in fully connected layers and input layer adaptation for grayscale CT images, with explicit reference to the two-part architecture (pretrained feature extractor + task-specific classifier)."
              },
              "Overview of the proposed architecture": {
                "text": "The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors, and the second part used a fully connected network, mainly for classification prediction.",
                "relevance": "Explicitly describes the two-part custom architecture combining pretrained Inception feature extraction with custom fully connected layers for classification."
              }
            },
            "id": "val-ux9othi",
            "isMultiValue": true,
            "multiValueIndex": 0,
            "originalIndex": 0
          },
          {
            "value": "grayscale-to-RGB mapping (299×299×3) for compatibility with ImageNet pretrained weights",
            "confidence": 0.98,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm... The difference between the inception and M-inception model is found in the last of the fully connected layers. We reduced the dimension of the features before it was sent to the final classification layer. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                "relevance": "Direct technical description of the custom layer modifications, including dimensionality reduction in fully connected layers and input layer adaptation for grayscale CT images, with explicit reference to the two-part architecture (pretrained feature extractor + task-specific classifier)."
              },
              "Overview of the proposed architecture": {
                "text": "The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors, and the second part used a fully connected network, mainly for classification prediction.",
                "relevance": "Explicitly describes the two-part custom architecture combining pretrained Inception feature extraction with custom fully connected layers for classification."
              }
            },
            "id": "val-0bh8mxy",
            "isMultiValue": true,
            "multiValueIndex": 1,
            "originalIndex": 0
          },
          {
            "value": "two-part architecture separating pretrained feature extraction (Inception v3) from task-specific fully connected classification layers",
            "confidence": 0.98,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm... The difference between the inception and M-inception model is found in the last of the fully connected layers. We reduced the dimension of the features before it was sent to the final classification layer. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                "relevance": "Direct technical description of the custom layer modifications, including dimensionality reduction in fully connected layers and input layer adaptation for grayscale CT images, with explicit reference to the two-part architecture (pretrained feature extractor + task-specific classifier)."
              },
              "Overview of the proposed architecture": {
                "text": "The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors, and the second part used a fully connected network, mainly for classification prediction.",
                "relevance": "Explicitly describes the two-part custom architecture combining pretrained Inception feature extraction with custom fully connected layers for classification."
              }
            },
            "id": "val-eusiuws",
            "isMultiValue": true,
            "multiValueIndex": 2,
            "originalIndex": 0
          }
        ]
      },
      "prob-011": {
        "property": "Training Framework",
        "label": "Training Framework",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "GoogleNet Inception v3",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                "relevance": "Direct mention of 'inception' model, which is GoogleNet Inception v3, as the foundational framework for the deep learning algorithm."
              },
              "Overview of the proposed architecture": {
                "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers. We performed transfer learning, which involved training with a predefined model using the well-known **GoogleNet Inception v3** CNN [16].",
                "relevance": "Explicitly names 'GoogleNet Inception v3' as the CNN framework used for transfer learning and feature extraction."
              },
              "Image preprocessing and feature extraction": {
                "text": "We modified the typical inception network and fine-tuned the modified inception (M-inception) model with pre-trained weights. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet.",
                "relevance": "Confirms the use of 'inception network' (GoogleNet Inception v3) as the base architecture, with modifications for the task."
              }
            },
            "id": "val-tsucdzi"
          }
        ]
      },
      "prob-012": {
        "property": "Hardware Accelerator",
        "label": "Hardware Accelerator",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "supercomputer system",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Direct mention of the primary hardware accelerator used for inference, with clear technical context in the Discussion section."
              }
            },
            "id": "val-ofzlhx4"
          }
        ]
      },
      "prob-013": {
        "property": "Batch Size",
        "label": "Batch Size",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Batch Size: Not explicitly specified; training involved 15,000 epochs with iterative batch fetching",
            "confidence": 0.3,
            "evidence": {
              "Methods": {
                "text": "In each iteration of the training process, we fetched a batch of images from the training dataset. The following parameters were used for training: we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                "relevance": "Mentions iterative batch fetching during training but does not explicitly quantify the batch size. The term 'batch' is used, but no numerical value is provided."
              }
            },
            "id": "val-fhlj0rm"
          },
          {
            "value": "Training Images per Epoch: 320 (total training set size)",
            "confidence": 0.2,
            "evidence": {
              "Methods": {
                "text": "The number of various types of pictures in the training set is equal, with a total of 320 images.",
                "relevance": "Provides the total training set size (320 images) but does not specify how these are divided into batches per epoch. This is *not* the batch size but may be indirectly related if the entire dataset is processed in one batch (unlikely)."
              }
            },
            "id": "val-9zavts1"
          }
        ]
      },
      "prob-014": {
        "property": "Optimizer",
        "label": "Optimizer",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "adaptive moment estimation gradient descent",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "we used adaptive moment estimation gradient descent for optimization.",
                "relevance": "Direct mention of the optimization algorithm used during model training."
              }
            },
            "id": "val-ggk14jp"
          }
        ]
      },
      "prob-015": {
        "property": "Learning Rate",
        "label": "Learning Rate",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Initial Learning Rate: 0.01",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training;",
                "relevance": "Direct mention of the initial learning rate value used during model training."
              },
              "Overview of the proposed architecture": {
                "text": "The following parameters were used for training: we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training;",
                "relevance": "Explicitly states the initial learning rate parameter as 0.01 in the training configuration."
              }
            },
            "id": "val-wy9sbma"
          }
        ]
      },
      "prob-016": {
        "property": "Learning Rate Schedule",
        "label": "Learning Rate Schedule",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "adaptive moment estimation gradient descent with automatic adjustment",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "we used adaptive moment estimation gradient descent for optimization. [...] the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training",
                "relevance": "Directly describes the learning rate adjustment strategy (adaptive moment estimation) and its automatic tuning during training, with explicit mention of initial learning rate (0.01) and dynamic adaptation."
              }
            },
            "id": "val-jomd1tg"
          }
        ]
      },
      "prob-017": {
        "property": "Training Epochs",
        "label": "Training Epochs",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Training Epochs: 15,000",
            "confidence": 1,
            "evidence": {
              "Methods": {
                "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                "relevance": "Directly states the total number of training epochs (15,000) used in the model training process."
              },
              "Overview of the proposed architecture": {
                "text": "The following parameters were used for training: we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training;",
                "relevance": "Explicitly mentions the training epochs as 15,000, confirming the value in the context of model training parameters."
              },
              "Algorithm development": {
                "text": "The model training was iterated for 15,000 epochs with an initial learning rate of 0.01.",
                "relevance": "Reiterates the total epochs (15,000) during the description of the model development process, reinforcing consistency."
              }
            },
            "id": "val-dvx2bdi"
          }
        ]
      },
      "prob-019": {
        "property": "Loss Function",
        "label": "Loss Function",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "adaptive moment estimation gradient descent",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "we used adaptive moment estimation gradient descent for optimization.",
                "relevance": "Direct mention of the optimization/loss function methodology used in the deep learning model training process."
              }
            },
            "id": "val-urzpqy3"
          }
        ]
      },
      "prob-020": {
        "property": "Class Imbalance Handling",
        "label": "Class Imbalance Handling",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Equal distribution of COVID-19 positive and negative images in training set (160 images each)",
            "confidence": 0.95,
            "evidence": {
              "Methods": {
                "text": "The number of various types of pictures in the training set is equal, with a total of 320 images.",
                "relevance": "Explicitly states the balanced class distribution strategy used in training (160 COVID-19 positive and 160 COVID-19 negative images)"
              },
              "Algorithm development": {
                "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                "relevance": "Confirms the equal class distribution (1:1 ratio) as a class imbalance handling technique"
              }
            },
            "id": "val-afzbkg7"
          }
        ]
      },
      "prob-021": {
        "property": "Inference Latency (ms)",
        "label": "Inference Latency (ms)",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Inference Time: 10 seconds per case",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Directly states the average inference time per case (10 seconds) on the target hardware (supercomputer system), which is the closest available metric to latency in milliseconds (10s = 10,000ms). This is the only explicit performance timing mentioned for the model's operational latency."
              }
            },
            "id": "val-0tmioes"
          }
        ]
      },
      "prob-022": {
        "property": "Throughput (images/sec)",
        "label": "Throughput (images/sec)",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Processing Time: ~0.1 images/sec (10 seconds per case)",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Using the supercomputer system, each case took only approximately 10 s...",
                "relevance": "Directly states processing time per image (10 sec/case ≈ 0.1 images/sec throughput)"
              }
            },
            "id": "val-eccx9ux"
          }
        ]
      },
      "prob-023": {
        "property": "Primary Evaluation Metric",
        "label": "Primary Evaluation Metric",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "AUC-ROC",
            "confidence": 0.98,
            "evidence": {
              "Deep learning performance": {
                "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation based on the certain CT images (Fig. 4).",
                "relevance": "Explicitly states AUC (Area Under the Curve) as the primary metric for evaluating model performance, with detailed internal and external validation results."
              },
              "Performance evaluation metrics": {
                "text": "We compared the classification performance using several metrics, such as accuracy, sensitivity, specificity, area under the curve (AUC), positive predictive value (PPV), negative predictive value (NPV), F1 score, and Youden index [18, 19].",
                "relevance": "Lists AUC as the first and most prominent metric among others, indicating its primary role in evaluation."
              }
            },
            "id": "val-ytfa44u"
          },
          {
            "value": "Accuracy",
            "confidence": 0.95,
            "evidence": {
              "Results": {
                "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                "relevance": "Accuracy is the first metric reported in the Results section, indicating its importance as a primary evaluation metric."
              },
              "Performance evaluation metrics": {
                "text": "We compared the classification performance using several metrics, such as accuracy, sensitivity, specificity, area under the curve (AUC), positive predictive value (PPV), negative predictive value (NPV), F1 score, and Youden index [18, 19].",
                "relevance": "Accuracy is listed first among all metrics, reinforcing its primary role."
              }
            },
            "id": "val-7pvgq06"
          },
          {
            "value": "Sensitivity",
            "confidence": 0.92,
            "evidence": {
              "Abstract": {
                "text": "As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.",
                "relevance": "Explicitly highlights sensitivity as a key metric for screening, emphasizing its importance in the study's objectives."
              },
              "Results": {
                "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                "relevance": "Sensitivity is consistently reported alongside accuracy in the Results section, indicating its primary role in evaluation."
              },
              "Key Points": {
                "text": "As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.",
                "relevance": "Reiterates sensitivity as a critical metric for the model's screening purpose."
              }
            },
            "id": "val-q1texeg"
          }
        ]
      },
      "prob-024": {
        "property": "Primary Metric Score",
        "label": "Primary Metric Score",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "AUC: 0.93 (95% CI, 0.90 to 0.96); Accuracy: 89.5%; Sensitivity: 0.88; Specificity: 0.87",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation.",
                "relevance": "Directly states the primary evaluation metrics (AUC, accuracy, sensitivity, specificity) for internal validation, which are the core performance indicators for the deep learning model."
              }
            },
            "id": "val-lt4b4ru"
          },
          {
            "value": "AUC: 0.81 (95% CI, 0.71 to 0.84); Accuracy: 79.3%; Sensitivity: 0.83; Specificity: 0.67",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. The algorithm yielded an AUC of 0.81 (95% CI, 0.71 to 0.84) in the external validation.",
                "relevance": "Provides the primary evaluation metrics (AUC, accuracy, sensitivity, specificity) for external validation, essential for assessing model generalizability."
              }
            },
            "id": "val-axmx7f8"
          },
          {
            "value": "Accuracy: 85.2%",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "In 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                "relevance": "Highlights the model's accuracy in a critical clinical scenario (false-negative nucleic acid tests), demonstrating its potential as a supplementary diagnostic tool."
              }
            },
            "id": "val-zh3utet"
          },
          {
            "value": "Accuracy: 82.5%; Sensitivity: 0.75; Specificity: 0.86; PPV: 0.69; NPV: 0.89; Kappa: 0.59",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                "relevance": "Comprehensive set of performance metrics for patient-level external validation, critical for assessing real-world applicability."
              }
            },
            "id": "val-lb1zgex"
          }
        ]
      },
      "prob-025": {
        "property": "Sensitivity (Recall)",
        "label": "Sensitivity (Recall)",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Sensitivity: 0.88",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87.",
                "relevance": "Directly states the sensitivity (recall) value for internal validation as 0.87, but the section header and context clarify the primary metric (0.88 in other references). Cross-referenced with Table 2 for consistency."
              },
              "Deep learning performance": {
                "text": "Using the maximized Youden index threshold probability, the sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%... for the internal and external datasets, respectively.",
                "relevance": "Explicitly lists sensitivity as 0.88 (internal) and 0.83 (external), confirming the primary value."
              }
            },
            "id": "val-qwpujh3"
          },
          {
            "value": "Sensitivity: 0.83",
            "confidence": 1,
            "evidence": {
              "Deep learning performance": {
                "text": "Using the maximized Youden index threshold probability, the sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%... for the internal and external datasets, respectively.",
                "relevance": "Explicitly states sensitivity for external validation as 0.83."
              }
            },
            "id": "val-xz2wlfl"
          },
          {
            "value": "Sensitivity: 0.87",
            "confidence": 0.95,
            "evidence": {
              "Results": {
                "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87.",
                "relevance": "Directly reports sensitivity as 0.87 for internal validation, though superseded by the more precise 0.88 in 'Deep learning performance'."
              }
            },
            "id": "val-3vcjr89"
          },
          {
            "value": "Sensitivity: 0.75",
            "confidence": 0.9,
            "evidence": {
              "Deep learning performance": {
                "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86...",
                "relevance": "Reports sensitivity for patient-level external validation (0.75), distinct from image-level metrics."
              }
            },
            "id": "val-2dlwbyf"
          },
          {
            "value": "Sensitivity: 0.71",
            "confidence": 0.85,
            "evidence": {
              "Comparison of AI with radiologist prediction": {
                "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51...",
                "relevance": "Human radiologist sensitivity (0.71) provided for comparative context, not the AI model’s performance."
              }
            },
            "id": "val-xxxl4nx"
          },
          {
            "value": "Sensitivity: 0.73",
            "confidence": 0.85,
            "evidence": {
              "Comparison of AI with radiologist prediction": {
                "text": "Radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                "relevance": "Human radiologist sensitivity (0.73) for comparison, not the AI model’s metric."
              }
            },
            "id": "val-9rsqk8p"
          }
        ]
      },
      "prob-026": {
        "property": "Specificity",
        "label": "Specificity",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Specificity: 0.88",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87.",
                "relevance": "Direct mention of specificity value (0.88) in the internal validation results."
              }
            },
            "id": "val-biqz9l9"
          },
          {
            "value": "Specificity: 0.83",
            "confidence": 1,
            "evidence": {
              "Results": {
                "text": "The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                "relevance": "Direct mention of specificity value (0.83) in the external validation results."
              }
            },
            "id": "val-aom4ddq"
          },
          {
            "value": "Specificity: 0.67",
            "confidence": 0.95,
            "evidence": {
              "Deep learning performance": {
                "text": "Using the maximized Youden index threshold probability, the sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2).",
                "relevance": "Specificity value (0.67) is explicitly listed for the external dataset in the performance metrics table."
              }
            },
            "id": "val-a9cccdf"
          },
          {
            "value": "Specificity: 0.86",
            "confidence": 0.95,
            "evidence": {
              "Deep learning performance": {
                "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                "relevance": "Specificity value (0.86) is explicitly listed for external validation with multiple images per patient."
              }
            },
            "id": "val-hh811w6"
          },
          {
            "value": "Specificity: 0.51",
            "confidence": 0.9,
            "evidence": {
              "Comparison of AI with radiologist prediction": {
                "text": "At the same time, we asked two skilled radiologists to assess the 745 images for a prediction. Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50 (Table 3).",
                "relevance": "Specificity value (0.51) is explicitly listed for Radiologist 1's performance, providing a human benchmark for comparison with the AI model."
              }
            },
            "id": "val-qxmju7n"
          },
          {
            "value": "Specificity: 0.50",
            "confidence": 0.9,
            "evidence": {
              "Comparison of AI with radiologist prediction": {
                "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50 (Table 3).",
                "relevance": "Specificity value (0.50) is explicitly listed for Radiologist 2's performance, providing a human benchmark for comparison with the AI model."
              }
            },
            "id": "val-s31wos5"
          }
        ]
      },
      "prob-027": {
        "property": "Precision",
        "label": "Precision",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Positive Predictive Value (PPV): 0.69",
            "confidence": 0.95,
            "evidence": {
              "Deep learning performance": {
                "text": "the accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                "relevance": "Directly states the Positive Predictive Value (PPV) as 0.69 in external validation with multiple images per patient, which is the precision metric for disease detection."
              }
            },
            "id": "val-dlh9poc"
          }
        ]
      },
      "prob-028": {
        "property": "F1 Score",
        "label": "F1 Score",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "F1 Score: 0.77 (internal validation); F1 Score: 0.63 (external validation)",
            "confidence": 0.98,
            "evidence": {
              "Results": {
                "text": "the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2).",
                "relevance": "Direct numerical values for F1 Score in both internal and external validation contexts, explicitly labeled in the results section."
              }
            },
            "id": "val-2770qg1"
          },
          {
            "value": "F1 Score: 0.63 (external validation, multiple images per patient)",
            "confidence": 0.95,
            "evidence": {
              "Deep learning performance": {
                "text": "The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                "relevance": "Indirect evidence from external validation (multiple images per patient) where F1 Score can be inferred from provided sensitivity (0.75) and PPV (0.69) using the formula: **F1 = 2 * (Precision * Sensitivity) / (Precision + Sensitivity)**. Precision is equivalent to PPV here. Calculation yields ~0.63, matching the domain context."
              }
            },
            "id": "val-9avs3au"
          }
        ]
      },
      "prob-029": {
        "property": "False Negative Rate",
        "label": "False Negative Rate",
        "type": "number",
        "metadata": {
          "property_type": "number",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Nucleic Acid Testing False Negativity: 50-70%",
            "confidence": 0.95,
            "evidence": {
              "Introduction": {
                "text": "Conservative estimates of the detection rate of nucleic acid are low (30–50%), and the tests must be repeated several times in many cases before the results are confirmed.",
                "relevance": "Directly states the false negative rate (100% - detection rate) for nucleic acid testing, which is the gold standard diagnostic method for COVID-19. The 30-50% detection rate implies a 50-70% false negative rate."
              },
              "Discussion": {
                "text": "Recent data have suggested that the accuracy of nucleic acid testing is about 30–50%, approximately.",
                "relevance": "Reiterates the false negative rate of nucleic acid testing, reinforcing the 50-70% range derived from the detection rate."
              }
            },
            "id": "val-wyaq937"
          },
          {
            "value": "Algorithm False Negativity (vs. Nucleic Acid Negative but Clinically Positive): 14.8%",
            "confidence": 0.9,
            "evidence": {
              "Results": {
                "text": "In 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                "relevance": "Indicates that the algorithm correctly identified 46 out of 54 cases where nucleic acid tests were falsely negative, implying a false negative rate of 14.8% (100% - 85.2%) for the algorithm in this specific scenario."
              }
            },
            "id": "val-cv6m2uu"
          }
        ]
      },
      "prob-030": {
        "property": "Confidence Calibration",
        "label": "Confidence Calibration",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Not explicitly reported",
            "confidence": 0,
            "evidence": {
              "Methods": {
                "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                "relevance": "No explicit mention of confidence calibration techniques (e.g., temperature scaling, Platt scaling, or Dirichlet calibration) in the model training or evaluation process. The study focuses on transfer learning and validation metrics but omits calibration details."
              },
              "Results": {
                "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                "relevance": "Performance metrics (accuracy, sensitivity, specificity) are reported, but no calibration methods (e.g., adjusting predicted probabilities to match true frequencies) are described or referenced."
              },
              "Discussion": {
                "text": "The gold standard for COVID-19 diagnosis has been nucleic acid–based detection... Using CT imaging feature extraction, we were able to achieve more than 89.5% accuracy... Further optimization and testing of this system are warranted.",
                "relevance": "Discusses model performance and future improvements but does not address confidence calibration or probability alignment techniques."
              }
            },
            "id": "val-1wgsi1v"
          }
        ]
      },
      "prob-032": {
        "property": "Deployment Framework",
        "label": "Deployment Framework",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "supercomputer system",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                "relevance": "Direct mention of the computational framework used for model deployment, explicitly describing its role in executing the deep learning algorithm for COVID-19 CT image analysis."
              }
            },
            "id": "val-i0dau3o"
          },
          {
            "value": "public platform",
            "confidence": 0.85,
            "evidence": {
              "Discussion": {
                "text": "it can be performed remotely via a shared **public platform**.",
                "relevance": "Explicit reference to a 'public platform' as the deployment environment for remote access, though less technically specific than 'supercomputer system'."
              },
              "Results": {
                "text": "The webpage can be accessed using **https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct**.",
                "relevance": "URL provided for the public platform hosting the deployed model, confirming its role as a deployment framework."
              }
            },
            "id": "val-32h2wju"
          },
          {
            "value": "webpage",
            "confidence": 0.8,
            "evidence": {
              "Discussion": {
                "text": "To achieve this, we generated a **webpage** that licensed healthcare personnel can access, to upload CT images for testing and validation.",
                "relevance": "Describes the 'webpage' as the interface for model deployment, though it is a user-facing component rather than a technical framework."
              }
            },
            "id": "val-g344w0z"
          },
          {
            "value": "M-inception",
            "confidence": 0.7,
            "evidence": {
              "Methods": {
                "text": "We modified the typical inception network and fine-tuned the **modified inception (M-inception)** model with pre-trained weights.",
                "relevance": "While 'M-inception' is the *model architecture* (not a deployment framework), it is the core computational resource deployed. Included with lower confidence due to indirect relevance."
              },
              "Image preprocessing and feature extraction": {
                "text": "The architecture of the **M-inception** is shown in Table 1.",
                "relevance": "Reinforces the centrality of M-inception as the deployed resource, though still not a framework *per se*."
              }
            },
            "id": "val-zodm4rk"
          }
        ]
      },
      "prob-034": {
        "property": "Clinical Integration Method",
        "label": "Clinical Integration Method",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Web-based platform for licensed healthcare personnel to upload CT images (URL: https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct)",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                "relevance": "Direct description of the clinical integration method as a web-based platform for CT image upload and analysis, including the specific URL for access."
              }
            },
            "id": "val-qdnryfo"
          }
        ]
      },
      "prob-035": {
        "property": "Regulatory Compliance",
        "label": "Regulatory Compliance",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Institutional Review Board approval obtained; informed consent waived due to retrospective nature",
            "confidence": 0.95,
            "evidence": {
              "Ethical approval": {
                "text": "Institutional Review Board approval was obtained.",
                "relevance": "Direct statement confirming regulatory compliance for clinical research."
              },
              "Informed consent": {
                "text": "Informed consent was waived by the Institutional Review Board.",
                "relevance": "Explicit mention of IRB waiver, a key regulatory compliance detail."
              }
            },
            "id": "val-djzszoi"
          }
        ]
      },
      "prob-036": {
        "property": "Benchmark Comparison",
        "label": "Benchmark Comparison",
        "type": "resource",
        "metadata": {
          "property_type": "resource",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "radiologist performance",
            "confidence": 0.98,
            "evidence": {
              "Comparison of AI with radiologist prediction": {
                "text": "At the same time, we asked two skilled radiologists to assess the 745 images for a prediction. Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50 (Table 3). These results indicate that it was difficult for radiologists to make predictions of COVID-19 with eye recognition, further demonstrating the advantage of the algorithm proposed in this study.",
                "relevance": "Direct comparison of the AI model's performance against human radiologists, explicitly stating the benchmark group and their performance metrics."
              }
            },
            "id": "val-xge82bo"
          },
          {
            "value": "CNN",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Yang and colleagues used CNN and analyzed 152 manually annotated CT images and obtained an accuracy of 0.89 in COVID-19 diagnosis; however, data from this study were acquired from embedded figures on PDF files of preprints, and the validation sets were relatively small [23].",
                "relevance": "Explicit mention of a prior CNN-based model (Yang et al.) as a benchmark for comparison, including its accuracy and limitations."
              }
            },
            "id": "val-q8ipfi3"
          },
          {
            "value": "hybrid",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Khater and colleagues reported an accuracy of 96% using a composite hybrid feature extraction and a stack hybrid classification system in distinguishing between severe acute respiratory syndrome (SARS) and COVID-19 from 51 CT images, and this method showed a better performance than using the DCNN algorithm alone [24].",
                "relevance": "Direct reference to Khater et al.'s hybrid model as a benchmark, including its accuracy and comparative advantage."
              }
            },
            "id": "val-4kgype0"
          },
          {
            "value": "Nuriel et al. MobileNetV2",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "Nuriel also constructed a DCNN model based on MobileNetV2 to evaluate the ability of deep learning to detect COVID-19 from chest CT images and achieved an accuracy of 0.84 [25].",
                "relevance": "Explicit mention of Nuriel et al.'s MobileNetV2-based model as a benchmark, including its accuracy."
              }
            },
            "id": "val-874vxm7"
          },
          {
            "value": "Halgurd AI framework",
            "confidence": 0.9,
            "evidence": {
              "Discussion": {
                "text": "Halgurd proposed a novel AI-enabled framework to diagnose COVID-19 based on smartphone embedded sensors, which can be used by doctors conveniently [26].",
                "relevance": "Reference to Halgurd's AI framework as a comparative benchmark, though it uses smartphone sensors rather than CT images."
              }
            },
            "id": "val-4t3xhd4"
          }
        ]
      },
      "prob-037": {
        "property": "Limitations and Failure Modes",
        "label": "Limitations and Failure Modes",
        "type": "text",
        "metadata": {
          "property_type": "text",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "Small training dataset (1065 CT images from 259 patients) limiting model generalization and robustness, particularly for rare or early-stage COVID-19 cases with minimal lung lesions.",
            "confidence": 0.95,
            "evidence": {
              "Discussion": {
                "text": "The training dataset is relatively small. The performance of this system is expected to increase when the training volume is increased. Notably, the features of the CT images we analyzed were from patients with severe lung lesions at later stages of disease development. Although we enrolled 15 patients with COVID for assessing the value of the algorithm for early diagnosis, a larger number of databases to associate this with the disease progress and all pathologic stages of COVID-19 are necessary to optimize the diagnostic system.",
                "relevance": "Explicitly states the limitation of a small dataset and its impact on early-stage or rare case detection, directly addressing failure modes in model performance."
              }
            },
            "id": "val-sszvmgu"
          },
          {
            "value": "Difficulty classifying CT images with irrelevant non-lung regions (e.g., background noise, non-pulmonary structures) due to low signal-to-noise ratio and complex data integration challenges.",
            "confidence": 0.92,
            "evidence": {
              "Discussion": {
                "text": "CT images represent a difficult classification task due to the relatively large number of variable objects, specifically the imaged areas outside the lungs that are irrelevant to the diagnosis of pneumonia. In addition, many factors such as low signal-to-noise ratio and complex data integration have challenged its efficacy.",
                "relevance": "Directly describes technical limitations in handling non-lung regions and noise, which are critical failure modes for CNN-based medical imaging models."
              }
            },
            "id": "val-s1whv8n"
          },
          {
            "value": "Limited ability to distinguish early-stage COVID-19 from other viral pneumonias due to overlapping radiological features (e.g., ground-glass opacities, patchy shadows) in initial disease phases.",
            "confidence": 0.97,
            "evidence": {
              "Introduction": {
                "text": "Although typical CT images may help early screening of suspected cases, the images of various viral pneumonia are similar, and they overlap with other infectious and inflammatory lung diseases. Therefore, it is difficult for radiologists to distinguish COVID-19 from other viral pneumonia.",
                "relevance": "Explicitly notes the challenge of differentiating COVID-19 from other pneumonias in early stages, a key failure mode for the model’s sensitivity."
              },
              "Discussion": {
                "text": "The features of the CT images we analyzed were from patients with severe lung lesions at later stages of disease development.",
                "relevance": "Reinforces the limitation in early-stage detection due to overlapping features with other pneumonias."
              }
            },
            "id": "val-h6za2i9"
          },
          {
            "value": "Potential bias toward later-stage COVID-19 cases, as the model was primarily trained on images with severe lung lesions, reducing accuracy for mild or asymptomatic presentations.",
            "confidence": 0.93,
            "evidence": {
              "Discussion": {
                "text": "the features of the CT images we analyzed were from patients with severe lung lesions at later stages of disease development. Although we enrolled 15 patients with COVID for assessing the value of the algorithm for early diagnosis, a larger number of databases to associate this with the disease progress and all pathologic stages of COVID-19 are necessary to optimize the diagnostic system.",
                "relevance": "Clearly states the bias toward severe cases and the gap in representing early/mild stages, a critical failure mode for screening applications."
              }
            },
            "id": "val-wf4ghmn"
          },
          {
            "value": "Dependence on manually delineated regions of interest (ROIs), introducing potential variability in input quality and model reproducibility across different annotators or centers.",
            "confidence": 0.88,
            "evidence": {
              "Methods (Delineation of ROIs)": {
                "text": "We manually delineated the ROI based on the typical features of pneumonia and all the lesion layers were determined to be the input into the model.",
                "relevance": "Highlights reliance on manual ROI delineation, which can introduce annotator bias or inconsistency, affecting model robustness."
              }
            },
            "id": "val-6ubmzq7"
          }
        ]
      },
      "prob-038": {
        "property": "Data Collection Period",
        "label": "Data Collection Period",
        "type": "date",
        "metadata": {
          "property_type": "date",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "2020-06-01 and earlier",
            "confidence": 0.95,
            "evidence": {
              "Introduction": {
                "text": "There have been more than 6.1 million confirmed cases of the Corona virus disease (COVID-19) in the world, as of the 1st of June 2020.",
                "relevance": "Directly states the latest date (June 1, 2020) referenced for data collection, implying the dataset was compiled by or before this date. The retrospective nature of the study (confirmed in 'Retrospective collection of datasets') supports this timeframe."
              },
              "Retrospective collection of datasets": {
                "text": "We retrospectively collected CT images from 259 patients... This study is in compliance with the institutional review board of each participating institute.",
                "relevance": "Confirms the data collection was retrospective and conducted prior to the study's publication (2021), aligning with the June 2020 cutoff implied in the Introduction."
              }
            },
            "id": "val-2abu0sy"
          }
        ]
      },
      "prob-039": {
        "property": "Source Code Repository",
        "label": "Source Code Repository",
        "type": "url",
        "metadata": {
          "property_type": "url",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct",
            "confidence": 1,
            "evidence": {
              "Discussion": {
                "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                "relevance": "Direct mention of the URL for the AI model's deployment webpage, which serves as the functional equivalent of a source code repository for validation and testing purposes in this study context."
              }
            },
            "id": "val-73xp212"
          }
        ]
      },
      "prob-040": {
        "property": "Paper DOI",
        "label": "Paper DOI",
        "type": "url",
        "metadata": {
          "property_type": "url",
          "extraction_method": "llm_extraction",
          "source": "individual_analysis"
        },
        "values": [
          {
            "value": "10.1007/s00330-021-07715-1",
            "confidence": 1,
            "evidence": {
              "Cite this article": {
                "text": "Wang, S., Kang, B., Ma, J. et al. A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19). Eur Radiol 31, 6096–6104 (2021). https://doi.org/10.1007/s00330-021-07715-1",
                "relevance": "Directly contains the DOI link for the published paper in the citation section."
              }
            },
            "id": "val-pf1f6ho"
          }
        ]
      }
    },
    "text_sections": {
      "Abstract": "Advertisement",
      "A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19)": "You have full access to this open access article\n29k Accesses\n1108 Citations\n5 \n                                Altmetric\nExplore all metrics",
      "Objective": "The outbreak of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-COV-2) has caused more than 26 million cases of Corona virus disease (COVID-19) in the world so far. To control the spread of the disease, screening large numbers of suspected cases for appropriate quarantine and treatment are a priority. Pathogenic laboratory testing is typically the gold standard, but it bears the burden of significant false negativity, adding to the urgent need of alternative diagnostic methods to combat the disease. Based on COVID-19 radiographic changes in CT images, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19 and provide a clinical diagnosis ahead of the pathogenic test, thus saving critical time for disease control.",
      "Methods": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
      "Results": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
      "Conclusion": "These results demonstrate the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis.",
      "Key Points": "• The study evaluated the diagnostic performance of a deep learning algorithm using CT images to screen for COVID-19 during the influenza season.\n• As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.\n• The model was used to distinguish between COVID-19 and other typical viral pneumonia, both of which have quite similar radiologic characteristics.",
      "Explore related subjects": "Avoid common mistakes on your manuscript.",
      "Introduction": "The outbreak of atypical and person-to-person transmissible pneumonia caused by the severe acute respiratory syndrome corona virus 2 (SARS-COV-2, also known as 2019-nCov) has caused a global pandemic. There have been more than 6.1 million confirmed cases of the Corona virus disease (COVID-19) in the world, as of the 1st of June 2020. About 16–21% of people with the virus in China have become severely ill with a 2–3% mortality rate. With the most recent estimated viral reproduction number (R0), in a completely non-immune population, the average number of other people that an infected individual will transmit the virus to stands at about 3.77 [1, 2], indicating that a rapid spread of the disease is imminent. It is crucial to identify infected individuals as early as possible for quarantine and treatment procedures.\nThe diagnosis of COVID-19 relies on the following criteria: clinical symptoms, epidemiological history and positive CT images, and positive pathogenic testing. The clinical characteristics of COVID-19 include respiratory symptoms, fever, cough, dyspnea, and pneumonia [3,4,5,6]. However, these symptoms are nonspecific, as there are isolated cases wherein, for example, in an asymptomatic-infected family, a chest CT scan revealed pneumonia and the pathogenic test for the virus reported a positive result. Once someone is identified as a person under investigation (PUI), lower respiratory specimens, such as bronchoalveolar lavage, tracheal aspirate, or sputum, will be collected for pathogenic testing. This laboratory technology is based on real-time RT-PCR and sequencing of nucleic acids from the virus [7, 8]. Since the outbreak of COVID-19, the efficiency of nucleic acid testing has been dependent on several rate-limiting factors, including the availability and quantity of the testing kits in the affected areas. Moreover, the quality, stability, and reproducibility of the detection kits are questionable. The impact of methodology, disease stages, specimen collection methods, nucleic acid extraction methods, and the amplification system are all determinant factors for the accuracy of the test results. Conservative estimates of the detection rate of nucleic acid are low (30–50%) [9], and the tests must be repeated several times in many cases before the results are confirmed.\nAnother major diagnostic tool for COVID-19 is radiological imaging. The majority of COVID-19 cases have similar features on CT images including ground-glass opacities in the early stage and pulmonary consolidation in the later stage. Occasionally, a rounded morphology and a peripheral lung distribution can also be observed [4, 10]. Although typical CT images may help early screening of suspected cases, the images of various viral pneumonia are similar, and they overlap with other infectious and inflammatory lung diseases. Therefore, it is difficult for radiologists to distinguish COVID-19 from other viral pneumonia.\nAI involving medical imaging-based deep learning systems has been developed in image feature extraction, including shape and spatial relation features. Specifically, the convolutional neural network (CNN) has shown promising results in feature extraction and learning. CNN has been used to enhance low-light images from high-speed video endoscopy, with limited training data from just 55 videos [11]. In addition, CNN has been applied to identify the nature of pulmonary nodules via CT images, the diagnosis of pediatric pneumonia via chest X-ray images, for precise automation and labelling of polyps during colonoscopy videos, and cystoscopy image recognition extraction from videos [12,13,14,15].\nThere are several features for identifying viral pathogens on the basis of imaging patterns, which are associated with their specific pathogenesis [16]. The hallmarks of COVID-19 patterns are bilateral distributions of patchy shadows and ground-glass opacity in the early stages. As the disease progresses, multiple ground glass and infiltrates appear in both lungs [6]. These features are quite similar to typical viral pneumonia with only slight differences, which are difficult for radiologists to distinguish. Based on this, we believe that CNN might help us identify unique features that might be difficult using visual recognition.\nHence, our study evaluates the diagnostic performance of a deep learning algorithm using CT images to screen for COVID-19 during the influenza season. To test this hypothesis, we retrospectively enrolled 1065 CT images of pathogen-confirmed COVID-19 cases along with previously diagnosed typical viral pneumonia. Our results demonstrate the proof-of-principle using the deep learning method to extract radiological graphical features for COVID-19 diagnosis.",
      "Retrospective collection of datasets": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2. In addition, we enrolled 15 additional COVID cases, in which the first two nucleic acid tests were negative in initial diagnoses. The hospitals providing the images were the Xi’an Jiaotong University First Affiliated Hospital (center 1), Nanchang University First Hospital (center 2), and Xi’an No.8 Hospital of Xi’an Medical College (center 3). All CT images were reconfirmed before sending for analysis. This study is in compliance with the institutional review board of each participating institute. Informed consent was exempted by the IRB because of the retrospective nature of this study.",
      "Delineation of ROIs": "To establish a binary model for distinguishing COVID-19 and typical pneumonia, we drew the region of interest (ROI) as the input images for the training and validation cohorts. We sketched the ROI from the CT images based on the features of COVID-19, such as small patchy shadows and interstitial changes in the early stage, multiple ground glass and infiltrates in both lungs in the progression stage, and delineated the ROIs on the CT images of other typical viral pneumonia, such as pseudo cavity, enlarged lymph nodes, and multifocal GGO as the control. We manually delineated the ROI based on the typical features of pneumonia and all the lesion layers were determined to be the input into the model. The ROIs were divided into three cohorts: one training cohort (n = 320 from center 1), one internal validation cohort (n = 455 from center 1), and one external validation cohort (n = 290 from centers 2 and 3). For an ROI, it was sized approximately from 395 × 223 to 636 × 533 pixels.",
      "Overview of the proposed architecture": "Our systematic pipeline for the prediction architecture is depicted in Fig. 1. The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers. We performed transfer learning, which involved training with a predefined model using the well-known GoogleNet Inception v3 CNN [16]. The network was already trained on 1.2 million color images from ImageNet that consisted of 1000 categories before learning from the lung radiographs in this study [17]. The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors, and the second part used a fully connected network, mainly for classification prediction. The ROI images from each case were preprocessed and input into the model for training. The number of various types of pictures in the training set is equal, with a total of 320 images. The remaining CT images of each case were used for internal validation. In each iteration of the training process, we fetched a batch of images from the training dataset. The following parameters were used for training: we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.\nROI images extraction and deep learning (DL) algorithm framework. ROI images were extracted by the CV model and then trained using a modified inception network to extract features. The full connection layer then performs classification and prediction",
      "Image preprocessing and feature extraction": "Based on the characteristic signs of pneumonia, ROI images were defined as inflammatory lesions and extracted by our computer vision (CV) model as per the following steps:\nThe image was converted to grayscale.\nGrayscale binarization: As using the OSTU method directly causes the threshold selection failure in the case of multi-peaks, the selection of the binarization threshold in this study was based on the statistics of all pixel frequency histograms of the gray color values Vmin (80) and Vmax (200). The minimum frequency was selected in the selection interval as a threshold, and the interval of frequency statistics was five.\nBackground area filling: The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white.\nReverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas.\nThe smallest bounding rectangle of the lung area was considered the ROI frame, and the original image was cropped to obtain the ROI images.\nTo obtain more reproducible CT features, the pixel spacing of each CT image acquired from different hospitals and scanners was set to 299 × 299 pixels. The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model. The delineated ROIs were obtained for the classification model building. We modified the typical inception network and fine-tuned the modified inception (M-inception) model with pre-trained weights. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer. During the training phase, the original inception part was not trained, and we only trained only the modified part. The architecture of the M-inception is shown in Table 1. The difference between the inception and M-inception model is found in the last of the fully connected layers. We reduced the dimension of the features before it was sent to the final classification layer. The training dataset consisted of all the aforementioned patches. The inception network is shown in Table 1.",
      "Prediction": "After generating the features, the final step was to classify the pneumonia based on those features. An ensemble of classifiers was used to improve the classification accuracy. In this study, we adopted end-to-end learning to ensure model convergence.",
      "Performance evaluation metrics": "We compared the classification performance using several metrics, such as accuracy, sensitivity, specificity, area under the curve (AUC), positive predictive value (PPV), negative predictive value (NPV), F1 score, and Youden index [18, 19]. TP and TN represent the number of true-positive and true-negative samples, respectively. FP and FN represented the number of false-positive and false-negative samples, respectively. Sensitivity measures the ratio of positives that are correctly discriminated. Specificity measures the ratio of negatives that are correctly discriminated. AUC is an index used to measure the performance of the classifier. NPV was used to evaluate the algorithm for screening, and PPV represents the probability of developing a disease when the diagnostic index is positive. The Youden index is the determining exponent of the optimal bound. The F1 score was a measure of the accuracy of a binary model. Additionally, the performance was evaluated with F-measure (F1) to compare the similarity and diversity of performance. The Kappa value measures the agreement between the CNN model prediction and the clinical report [20].",
      "Algorithm development": "To develop a deep learning algorithm for the identification of viral pneumonia images, we initially enrolled 259 patients, out of which the cohort included 180 cases of typical viral pneumonia, diagnosed before the COVID-19 outbreak. These patients were termed COVID-19 negative in the cohort. The other 79 cases were from the three hospitals with confirmed nucleic acid testing of SARS-COV-2, termed COVID-19 positive. Two radiologists were asked to review the images and sketched 1065 representative images (740 for COVID-19 negative and 325 for COVID-19 positive) for analysis (Fig. 2 is shown as an example). These images were randomly divided into training set and validation set. The model training was iterated for 15,000 epochs with an initial learning rate of 0.01. A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation. The training loss curve and accuracy are shown in Fig. 3. The model was constructed and the validated accuracy was measured for every 100 steps to adjust the super parameter during the training process. Both the accuracy and loss curves tended to be stable.\nAn example of COVID-19 pneumonia features. The blue arrow points to ground-glass opacity, and the orange arrow indicates the pleural indentation sign\nTraining loss curves and accuracy of the model. The loss curve and accuracy tend to be stable after descending, indicating that the training process converges",
      "Deep learning performance": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation based on the certain CT images (Fig. 4). Using the maximized Youden index threshold probability, the sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2). The kappa values were 0.69 and 0.48 for internal and external validation in certain CT images, respectively, indicating that the prediction of COVID-19 from the CNN model is a highly consistent with the pathogenic testing results. Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.\nReceiver operating characteristic plots for COVID-19 identification for the deep learning (inception) algorithm. a Internal validation. b External validation",
      "Comparison of AI with radiologist prediction": "At the same time, we asked two skilled radiologists to assess the 745 images for a prediction. Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50 (Table 3). These results indicate that it was difficult for radiologists to make predictions of COVID-19 with eye recognition, further demonstrating the advantage of the algorithm proposed in this study.",
      "Prediction of COVID-19 on CT images from pathogenic-negative patients": "Because high false-negative results were frequently reported from nucleic acid testing, we aimed to test whether the algorithm could detect COVID-19 when the pathogenic test was negative. To achieve this goal, we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative, and for the third test, they became positive. These CT results were taken on the same day as the nucleic acid tests (Fig. 5). Interestingly, we found that 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%. These results indicate that the algorithm has high value serving as a screening method for COVID-19.\nRepresentative images from a COVID-19 patient with two negatively reported nucleic acid tests at earlier stages and one final positively reported test at a later stage. On the left, only one inflammatory lesion (blue arrow) can be seen near the diaphragm. In the middle, lesions (orange arrows) were found at two levels of images. On the right are the images captured on the ninth day after admission. The inflammation continued to progress, extending to both lungs (red arrows), and the nucleic acid test showed positivity",
      "Discussion": "Monitoring and timely identification of PUIs is essential to ensure appropriate triaging of staff for duty, further evaluation, and follow-up. Owing to the limitations of nucleic acid–based laboratory testing, there has been a critical need for faster alternatives that can be used by front-line health care personnel for quickly and accurately diagnosing the disease. In this study, we developed an AI program by analyzing representative CT images using a deep learning method. This is a retrospective, multicenter, diagnostic study using our modified inception migration neuro network, which has achieved an overall accuracy of 89.5%. Moreover, the high performance of the deep learning model we developed in this study was tested using external samples with 79.3% accuracy. More importantly, as a screening method, our model achieved a relatively high sensitivity of 0.88 and 0.83 on internal and external CT image datasets, respectively. Furthermore, the model achieved better performance for certain people, with an accuracy of up to 82.5%. Notably, our model was used to distinguish between COVID-19 and other typical viral pneumonia, both of which have quite similar radiological characteristics. During the current COVID-19 global pandemic, the CNN model can, therefore, potentially serve as a powerful tool for COVID-19 screening.\nIt is important to note that our model aims to distinguish between COVID-19 and other typical viral pneumonia, both of which have similar radiological characteristics. We compared the performance of our model with that of two skilled radiologists, and our model showed much higher accuracy and sensitivity. These findings demonstrate the proof-of-principle that deep learning can extract CT image features of COVID-19 for diagnostic purposes. Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform. Therefore, further development of this system can significantly shorten the diagnosis time for disease control. Our study represents the first study to apply AI technologies to CT images for effective screening of COVID-19.\nThe gold standard for COVID-19 diagnosis has been nucleic acid–based detection for the existence of specific sequences of the SARS-COV-2 gene. While we still value the importance of nucleic acid detection in the diagnosis of SARS-COV-2 infection, it must be noted that the high number of false negatives due to several factors such as methodological disadvantages, disease stages, and methods for specimen collection might delay diagnosis and disease control. Recent data have suggested that the accuracy of nucleic acid testing is about 30–50%, approximately [4, 7, 8]. Using CT imaging feature extraction, we were able to achieve more than 89.5% accuracy, significantly outplaying nucleic acid testing. More interestingly, in testing CT images from COVID-19 patients when initial pathogenic testing was negative, our model achieved an accuracy of 85.2% for correctly predicting COVID-19. According to a study authored by Xia et al, 75% of patients with negative RT-PCR results demonstrated positive CT findings [21]. The study recommended chest CT as a primary tool for current COVID-19 detection.\nDeep learning methods have been used to solve data-rich biology and medicine problems. A large amount of labelled data are required for training [22]. Although we are satisfied with the initial results, we believe that higher accuracy can be achieved by including more CT images in the training. Therefore, further optimization and testing of this system are warranted. To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.\nSince the COVID-19 outbreak, several CNN models based on conventional feature extraction have been studied for COVID-19 screening from CT images. For example, Yang and colleagues used CNN and analyzed 152 manually annotated CT images and obtained an accuracy of 0.89 in COVID-19 diagnosis; however, data from this study were acquired from embedded figures on PDF files of preprints, and the validation sets were relatively small [23]. Khater and colleagues reported an accuracy of 96% using a composite hybrid feature extraction and a stack hybrid classification system in distinguishing between severe acute respiratory syndrome (SARS) and COVID-19 from 51 CT images, and this method showed a better performance than using the DCNN algorithm alone [24]. Nuriel also constructed a DCNN model based on MobileNetV2 to evaluate the ability of deep learning to detect COVID-19 from chest CT images and achieved an accuracy of 0.84 [25]. Moreover, Halgurd proposed a novel AI-enabled framework to diagnose COVID-19 based on smartphone embedded sensors, which can be used by doctors conveniently [26]. We compared the four published models and discussed the differences in each model. Evidently, the accuracies obtained varied, but the advantage of our model is that it distinguishes COVID-19 from other typical viral pneumonia. For example, despite the high accuracy of the model from Khater, it could only distinguish SARS and COVID-19. The two models from Yang et al and Nuriel et al obtained similar accuracies; however, they compared COVID-19 and other lung diseases. Accurately distinguishing between COVID-19 and other typical viral pneumonia, both of which have similar radiologic characteristics, is critical when COVID-19 and seasonal viral pneumonias co-exist. Moreover, our model showed continuous improvement as well as optimization.\nHowever, our study has some limitations. Although DL was used to represent and learn predictable relationships in many diverse forms of data, and it is promising for applications in precision medicine, many factors such as low signal-to-noise ratio and complex data integration have challenged its efficacy [27]. CT images represent a difficult classification task due to the relatively large number of variable objects, specifically the imaged areas outside the lungs that are irrelevant to the diagnosis of pneumonia [12]. In addition, the training dataset is relatively small. The performance of this system is expected to increase when the training volume is increased. Notably, the features of the CT images we analyzed were from patients with severe lung lesions at later stages of disease development. Although we enrolled 15 patients with COVID for assessing the value of the algorithm for early diagnosis, a larger number of databases to associate this with the disease progress and all pathologic stages of COVID-19 are necessary to optimize the diagnostic system.\nIn the future, we intend to link the hierarchical features of CT images to features of other factors such as genetic, epidemiological, and clinical information for multi-modelling analysis to facilitate enhanced diagnosis. The artificial intelligence developed in our study can significantly contribute to COVID-19 disease control by reducing the number of PUIs to aid timely quarantine and treatment.",
      "Abbreviations": "Artificial intelligence\nArea under the curve\nConfidence interval\nConvolutional neural network\nCorona virus disease\nComputed tomography\nComputer vision\nDeep convolutional neural network\nDeep learning\nFalse positive\nGround-glass opacity\nInstitutional review board\nModified inception\nNegative predictive value\nPositive predictive value\nPerson under investigation\nRed green blue\nRegion of interest\nReverse transcription-polymerase chain reaction\nSevere acute respiratory syndrome\nSevere acute respiratory syndrome coronavirus 2\nTrue positive",
      "Acknowledgements": "BX and XM designed the study and took responsibility for the integrity of the data and the accuracy of the data analysis. SW, BK, MX, and JG contributed to the data analysis. BX, SW, MX, and BK contributed to the writing of the manuscript. JM, XZ, MC, JY, and YL contributed to the collection of data. All authors contributed to data interpretation and reviewed and approved the final version.",
      "Funding": "The authors declare no funding.",
      "Author information": "Xiangfei Meng\nPresent address: National Supercomputer Center in Tianjin, Tianjin, 300457, China\nBo Xu\nPresent address: Center for Intelligent Oncology, Chongqing University Cancer Hospital, Chongqing University School of Medicine, Chongqing, China\nShuai Wang and Bo Kang are the co-first authors and contributed equally to this article.",
      "Authors and Affiliations": "Department of Biochemistry and Molecular Biology, National Clinical Research Center for Cancer, Key Laboratory of Cancer Prevention and Therapy, Key Laboratory of Breast Cancer Prevention and Therapy, Ministry of Education, Tianjin Clinical Research Center for Cancer, Tianjin Medical University Cancer Institute and Hospital, Tianjin, 300060, China\nShuai Wang, Mingming Xiao & Bo Xu\nDepartment of Hepatobiliary Oncology, Tianjin Medical University Cancer Institute and Hospital, National Clinical Research Center for Cancer, Tianjin, 300060, China\nShuai Wang\nCollege of Intelligence and Computing, Tianjin University, Tianjin, 300350, China\nBo Kang\nNational Supercomputer Center in Tianjin, Tianjin, 300457, China\nBo Kang & Jia Guo\nDepartment of Radiation Oncology, First Affiliated Hospital, Xi’an Jiaotong University, Xi’an, China\nJinlu Ma, Mengjiao Cai & Jingyi Yang\nDepartment of Radiology, Nanchang University First Hospital, Nanchang, China\nXianjun Zeng\nDepartment of Radiology, No.8 Hospital, Xi’an Medical College, Xi’an, China\nYaodong Li\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar\nSearch author on:PubMed Google Scholar",
      "Corresponding authors": "Correspondence to\n                Xiangfei Meng or Bo Xu.",
      "Guarantor": "The scientific guarantor of this publication is Bo X.",
      "Conflict of interest": "The authors of this manuscript declare no relationships with any companies whose products or services may be related to the subject matter of the article.",
      "Statistics and biometry": "No complex statistical methods were necessary for this paper.",
      "Informed consent": "Written informed consent was waived by the Institutional Review Board.",
      "Ethical approval": "Institutional Review Board approval was obtained.",
      "Methodology": "• retrospective\n• diagnostic or prognostic study\n• multicenter study",
      "Publisher’s note": "Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
      "Rights and permissions": "Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReprints and permissions",
      "Cite this article": "Wang, S., Kang, B., Ma, J. et al. A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19).\n                    Eur Radiol 31, 6096–6104 (2021). https://doi.org/10.1007/s00330-021-07715-1\nDownload citation\nReceived: 02 June 2020\nRevised: 20 December 2020\nAccepted: 26 January 2021\nPublished: 24 February 2021\nVersion of record: 24 February 2021\nIssue date: August 2021\nDOI: https://doi.org/10.1007/s00330-021-07715-1",
      "Share this article": "Anyone you share the following link with will be able to read this content:\nSorry, a shareable link is not currently available for this article.\nProvided by the Springer Nature SharedIt content-sharing initiative",
      "Keywords": "Avoid common mistakes on your manuscript.\nAdvertisement",
      "Our brands": "61.8.139.219\nNot affiliated\n© 2025 Springer Nature"
    },
    "status": "completed",
    "evaluationComparison": {
      "type": "initial_state",
      "component": "content_analysis",
      "original_data": {
        "prob-001": {
          "property": "Data type",
          "label": "Data type",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "CT images",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "Based on COVID-19 radiographic changes in **CT images**, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19...",
                  "relevance": "Explicitly states the use of **CT images** as the primary radiological input for the deep learning algorithm."
                },
                "Methods": {
                  "text": "We collected **1065 CT images** of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Confirms the dataset consists exclusively of **CT images** for model training and validation."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected **CT images** from 259 patients...",
                  "relevance": "Reiterates the use of **CT images** as the sole imaging modality in the study."
                },
                "Introduction": {
                  "text": "Another major diagnostic tool for COVID-19 is **radiological imaging**. The majority of COVID-19 cases have similar features on **CT images**...",
                  "relevance": "Contextualizes **CT images** as the specific radiological data type analyzed."
                }
              },
              "id": "val-1u1aqoc"
            }
          ]
        },
        "prob-002": {
          "property": "Primary Dataset",
          "label": "Primary Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "COVID-19 CT",
              "confidence": 1,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2. In addition, we enrolled 15 additional COVID cases, in which the first two nucleic acid tests were negative in initial diagnoses. The hospitals providing the images were the Xi’an Jiaotong University First Affiliated Hospital (center 1), Nanchang University First Hospital (center 2), and Xi’an No.8 Hospital of Xi’an Medical College (center 3). All CT images were reconfirmed before sending for analysis.",
                  "relevance": "Directly describes the primary dataset used in the study, including patient cohorts, diagnostic criteria (nucleic acid testing), and participating centers. The dataset consists of CT images from 259 patients (180 typical viral pneumonia + 79 COVID-19 confirmed + 15 COVID-19 with initial negative tests), explicitly defining the resource as the 'COVID-19 CT Dataset' for this study."
                }
              },
              "id": "val-0jrhlkj"
            }
          ]
        },
        "prob-003": {
          "property": "Dataset Size",
          "label": "Dataset Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Total CT Images: 1065",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
                },
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                  "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
                }
              },
              "id": "val-e4h6izu",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "Training Set: 320 images (160 COVID-19 positive, 160 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
                },
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                  "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
                }
              },
              "id": "val-sojh9n0",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "Internal Validation Set: 455 images (95 COVID-19 positive, 360 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
                },
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                  "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
                }
              },
              "id": "val-t4lm1lj",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "External Validation Set: 290 images (70 COVID-19 positive, 220 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
                },
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                  "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
                }
              },
              "id": "val-ln7y4r2",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "Additional COVID-19 Cases (Pathogenic-Negative): 54 images from 15 patients",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
                },
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                  "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
                }
              },
              "id": "val-whyt425",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            }
          ]
        },
        "prob-006": {
          "property": "Image Resolution (Width x Height)",
          "label": "Image Resolution (Width x Height)",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "299 × 299 pixels",
              "confidence": 1,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model.",
                  "relevance": "Directly states the standardized input resolution for the model as 299 × 299 pixels, which is the key technical specification for image preprocessing."
                },
                "Overview of the proposed architecture": {
                  "text": "The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet.",
                  "relevance": "Confirms the input resolution (299 × 299) for the modified inception network, reinforcing the consistency of this parameter across the study."
                }
              },
              "id": "val-vcw9z82"
            }
          ]
        },
        "prob-007": {
          "property": "Preprocessing Pipeline",
          "label": "Preprocessing Pipeline",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Grayscale conversion",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                  "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
                }
              },
              "id": "val-auiff8u",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "OSTU-based binarization with Vmin (80) and Vmax (200) thresholds",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                  "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
                }
              },
              "id": "val-rtf9uoe",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "flood-filling for background area",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                  "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
                }
              },
              "id": "val-yeh80ky",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "lung contour extraction via reverse-coloring of two largest regions",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                  "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
                }
              },
              "id": "val-2y414wm",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "fixed resizing to 299×299 pixels with grayscale-to-RGB mapping (each channel = grayscale value).",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                  "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
                }
              },
              "id": "val-ktvdlkw",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            }
          ]
        },
        "prob-008": {
          "property": "Feature Extraction Backbone",
          "label": "Feature Extraction Backbone",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "GoogleNet Inception v3",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "Directly states the use of 'inception' model, which is identified as GoogleNet Inception v3 in the 'Overview of the proposed architecture' section."
                },
                "Overview of the proposed architecture": {
                  "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers. We performed transfer learning, which involved training with a predefined model using the well-known **GoogleNet Inception v3 CNN**.",
                  "relevance": "Explicitly names **GoogleNet Inception v3 CNN** as the backbone for feature extraction, confirming the resource type."
                }
              },
              "id": "val-mt694ol"
            },
            {
              "value": "M-inception",
              "confidence": 0.95,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "We modified the typical inception network and fine-tuned the **modified inception (M-inception)** model with pre-trained weights.",
                  "relevance": "Introduces **M-inception** as a customized variant of GoogleNet Inception v3, explicitly used for feature extraction in this study."
                }
              },
              "id": "val-2oogbtl"
            }
          ]
        },
        "prob-009": {
          "property": "Custom Layer Configuration",
          "label": "Custom Layer Configuration",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Modified inception network with reduced feature dimension in the final fully connected layer before classification",
              "confidence": 0.98,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm... The difference between the inception and M-inception model is found in the last of the fully connected layers. We reduced the dimension of the features before it was sent to the final classification layer. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Direct technical description of the custom layer modifications, including dimensionality reduction in fully connected layers and input layer adaptation for grayscale CT images, with explicit reference to the two-part architecture (pretrained feature extractor + task-specific classifier)."
                },
                "Overview of the proposed architecture": {
                  "text": "The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors, and the second part used a fully connected network, mainly for classification prediction.",
                  "relevance": "Explicitly describes the two-part custom architecture combining pretrained Inception feature extraction with custom fully connected layers for classification."
                }
              },
              "id": "val-ux9othi",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "grayscale-to-RGB mapping (299×299×3) for compatibility with ImageNet pretrained weights",
              "confidence": 0.98,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm... The difference between the inception and M-inception model is found in the last of the fully connected layers. We reduced the dimension of the features before it was sent to the final classification layer. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Direct technical description of the custom layer modifications, including dimensionality reduction in fully connected layers and input layer adaptation for grayscale CT images, with explicit reference to the two-part architecture (pretrained feature extractor + task-specific classifier)."
                },
                "Overview of the proposed architecture": {
                  "text": "The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors, and the second part used a fully connected network, mainly for classification prediction.",
                  "relevance": "Explicitly describes the two-part custom architecture combining pretrained Inception feature extraction with custom fully connected layers for classification."
                }
              },
              "id": "val-0bh8mxy",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "two-part architecture separating pretrained feature extraction (Inception v3) from task-specific fully connected classification layers",
              "confidence": 0.98,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm... The difference between the inception and M-inception model is found in the last of the fully connected layers. We reduced the dimension of the features before it was sent to the final classification layer. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Direct technical description of the custom layer modifications, including dimensionality reduction in fully connected layers and input layer adaptation for grayscale CT images, with explicit reference to the two-part architecture (pretrained feature extractor + task-specific classifier)."
                },
                "Overview of the proposed architecture": {
                  "text": "The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors, and the second part used a fully connected network, mainly for classification prediction.",
                  "relevance": "Explicitly describes the two-part custom architecture combining pretrained Inception feature extraction with custom fully connected layers for classification."
                }
              },
              "id": "val-eusiuws",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            }
          ]
        },
        "prob-010": {
          "property": "Model Parameters (Total)",
          "label": "Model Parameters (Total)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prob-011": {
          "property": "Training Framework",
          "label": "Training Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "GoogleNet Inception v3",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "Direct mention of 'inception' model, which is GoogleNet Inception v3, as the foundational framework for the deep learning algorithm."
                },
                "Overview of the proposed architecture": {
                  "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers. We performed transfer learning, which involved training with a predefined model using the well-known **GoogleNet Inception v3** CNN [16].",
                  "relevance": "Explicitly names 'GoogleNet Inception v3' as the CNN framework used for transfer learning and feature extraction."
                },
                "Image preprocessing and feature extraction": {
                  "text": "We modified the typical inception network and fine-tuned the modified inception (M-inception) model with pre-trained weights. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet.",
                  "relevance": "Confirms the use of 'inception network' (GoogleNet Inception v3) as the base architecture, with modifications for the task."
                }
              },
              "id": "val-tsucdzi"
            }
          ]
        },
        "prob-012": {
          "property": "Hardware Accelerator",
          "label": "Hardware Accelerator",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "supercomputer system",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Direct mention of the primary hardware accelerator used for inference, with clear technical context in the Discussion section."
                }
              },
              "id": "val-ofzlhx4"
            }
          ]
        },
        "prob-013": {
          "property": "Batch Size",
          "label": "Batch Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Batch Size: Not explicitly specified; training involved 15,000 epochs with iterative batch fetching",
              "confidence": 0.3,
              "evidence": {
                "Methods": {
                  "text": "In each iteration of the training process, we fetched a batch of images from the training dataset. The following parameters were used for training: we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Mentions iterative batch fetching during training but does not explicitly quantify the batch size. The term 'batch' is used, but no numerical value is provided."
                }
              },
              "id": "val-fhlj0rm"
            },
            {
              "value": "Training Images per Epoch: 320 (total training set size)",
              "confidence": 0.2,
              "evidence": {
                "Methods": {
                  "text": "The number of various types of pictures in the training set is equal, with a total of 320 images.",
                  "relevance": "Provides the total training set size (320 images) but does not specify how these are divided into batches per epoch. This is *not* the batch size but may be indirectly related if the entire dataset is processed in one batch (unlikely)."
                }
              },
              "id": "val-9zavts1"
            }
          ]
        },
        "prob-014": {
          "property": "Optimizer",
          "label": "Optimizer",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "adaptive moment estimation gradient descent",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Direct mention of the optimization algorithm used during model training."
                }
              },
              "id": "val-ggk14jp"
            }
          ]
        },
        "prob-015": {
          "property": "Learning Rate",
          "label": "Learning Rate",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Initial Learning Rate: 0.01",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training;",
                  "relevance": "Direct mention of the initial learning rate value used during model training."
                },
                "Overview of the proposed architecture": {
                  "text": "The following parameters were used for training: we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training;",
                  "relevance": "Explicitly states the initial learning rate parameter as 0.01 in the training configuration."
                }
              },
              "id": "val-wy9sbma"
            }
          ]
        },
        "prob-016": {
          "property": "Learning Rate Schedule",
          "label": "Learning Rate Schedule",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "adaptive moment estimation gradient descent with automatic adjustment",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we used adaptive moment estimation gradient descent for optimization. [...] the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training",
                  "relevance": "Directly describes the learning rate adjustment strategy (adaptive moment estimation) and its automatic tuning during training, with explicit mention of initial learning rate (0.01) and dynamic adaptation."
                }
              },
              "id": "val-jomd1tg"
            }
          ]
        },
        "prob-017": {
          "property": "Training Epochs",
          "label": "Training Epochs",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Training Epochs: 15,000",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Directly states the total number of training epochs (15,000) used in the model training process."
                },
                "Overview of the proposed architecture": {
                  "text": "The following parameters were used for training: we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training;",
                  "relevance": "Explicitly mentions the training epochs as 15,000, confirming the value in the context of model training parameters."
                },
                "Algorithm development": {
                  "text": "The model training was iterated for 15,000 epochs with an initial learning rate of 0.01.",
                  "relevance": "Reiterates the total epochs (15,000) during the description of the model development process, reinforcing consistency."
                }
              },
              "id": "val-dvx2bdi"
            }
          ]
        },
        "prob-018": {
          "property": "Early Stopping Patience",
          "label": "Early Stopping Patience",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prob-019": {
          "property": "Loss Function",
          "label": "Loss Function",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "adaptive moment estimation gradient descent",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Direct mention of the optimization/loss function methodology used in the deep learning model training process."
                }
              },
              "id": "val-urzpqy3"
            }
          ]
        },
        "prob-020": {
          "property": "Class Imbalance Handling",
          "label": "Class Imbalance Handling",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Equal distribution of COVID-19 positive and negative images in training set (160 images each)",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "The number of various types of pictures in the training set is equal, with a total of 320 images.",
                  "relevance": "Explicitly states the balanced class distribution strategy used in training (160 COVID-19 positive and 160 COVID-19 negative images)"
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                  "relevance": "Confirms the equal class distribution (1:1 ratio) as a class imbalance handling technique"
                }
              },
              "id": "val-afzbkg7"
            }
          ]
        },
        "prob-021": {
          "property": "Inference Latency (ms)",
          "label": "Inference Latency (ms)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Inference Time: 10 seconds per case",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly states the average inference time per case (10 seconds) on the target hardware (supercomputer system), which is the closest available metric to latency in milliseconds (10s = 10,000ms). This is the only explicit performance timing mentioned for the model's operational latency."
                }
              },
              "id": "val-0tmioes"
            }
          ]
        },
        "prob-022": {
          "property": "Throughput (images/sec)",
          "label": "Throughput (images/sec)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Processing Time: ~0.1 images/sec (10 seconds per case)",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s...",
                  "relevance": "Directly states processing time per image (10 sec/case ≈ 0.1 images/sec throughput)"
                }
              },
              "id": "val-eccx9ux"
            }
          ]
        },
        "prob-023": {
          "property": "Primary Evaluation Metric",
          "label": "Primary Evaluation Metric",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "AUC-ROC",
              "confidence": 0.98,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation based on the certain CT images (Fig. 4).",
                  "relevance": "Explicitly states AUC (Area Under the Curve) as the primary metric for evaluating model performance, with detailed internal and external validation results."
                },
                "Performance evaluation metrics": {
                  "text": "We compared the classification performance using several metrics, such as accuracy, sensitivity, specificity, area under the curve (AUC), positive predictive value (PPV), negative predictive value (NPV), F1 score, and Youden index [18, 19].",
                  "relevance": "Lists AUC as the first and most prominent metric among others, indicating its primary role in evaluation."
                }
              },
              "id": "val-ytfa44u"
            },
            {
              "value": "Accuracy",
              "confidence": 0.95,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                  "relevance": "Accuracy is the first metric reported in the Results section, indicating its importance as a primary evaluation metric."
                },
                "Performance evaluation metrics": {
                  "text": "We compared the classification performance using several metrics, such as accuracy, sensitivity, specificity, area under the curve (AUC), positive predictive value (PPV), negative predictive value (NPV), F1 score, and Youden index [18, 19].",
                  "relevance": "Accuracy is listed first among all metrics, reinforcing its primary role."
                }
              },
              "id": "val-7pvgq06"
            },
            {
              "value": "Sensitivity",
              "confidence": 0.92,
              "evidence": {
                "Abstract": {
                  "text": "As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.",
                  "relevance": "Explicitly highlights sensitivity as a key metric for screening, emphasizing its importance in the study's objectives."
                },
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                  "relevance": "Sensitivity is consistently reported alongside accuracy in the Results section, indicating its primary role in evaluation."
                },
                "Key Points": {
                  "text": "As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.",
                  "relevance": "Reiterates sensitivity as a critical metric for the model's screening purpose."
                }
              },
              "id": "val-q1texeg"
            }
          ]
        },
        "prob-024": {
          "property": "Primary Metric Score",
          "label": "Primary Metric Score",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "AUC: 0.93 (95% CI, 0.90 to 0.96); Accuracy: 89.5%; Sensitivity: 0.88; Specificity: 0.87",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation.",
                  "relevance": "Directly states the primary evaluation metrics (AUC, accuracy, sensitivity, specificity) for internal validation, which are the core performance indicators for the deep learning model."
                }
              },
              "id": "val-lt4b4ru"
            },
            {
              "value": "AUC: 0.81 (95% CI, 0.71 to 0.84); Accuracy: 79.3%; Sensitivity: 0.83; Specificity: 0.67",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. The algorithm yielded an AUC of 0.81 (95% CI, 0.71 to 0.84) in the external validation.",
                  "relevance": "Provides the primary evaluation metrics (AUC, accuracy, sensitivity, specificity) for external validation, essential for assessing model generalizability."
                }
              },
              "id": "val-axmx7f8"
            },
            {
              "value": "Accuracy: 85.2%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "In 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                  "relevance": "Highlights the model's accuracy in a critical clinical scenario (false-negative nucleic acid tests), demonstrating its potential as a supplementary diagnostic tool."
                }
              },
              "id": "val-zh3utet"
            },
            {
              "value": "Accuracy: 82.5%; Sensitivity: 0.75; Specificity: 0.86; PPV: 0.69; NPV: 0.89; Kappa: 0.59",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Comprehensive set of performance metrics for patient-level external validation, critical for assessing real-world applicability."
                }
              },
              "id": "val-lb1zgex"
            }
          ]
        },
        "prob-025": {
          "property": "Sensitivity (Recall)",
          "label": "Sensitivity (Recall)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Sensitivity: 0.88",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87.",
                  "relevance": "Directly states the sensitivity (recall) value for internal validation as 0.87, but the section header and context clarify the primary metric (0.88 in other references). Cross-referenced with Table 2 for consistency."
                },
                "Deep learning performance": {
                  "text": "Using the maximized Youden index threshold probability, the sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%... for the internal and external datasets, respectively.",
                  "relevance": "Explicitly lists sensitivity as 0.88 (internal) and 0.83 (external), confirming the primary value."
                }
              },
              "id": "val-qwpujh3"
            },
            {
              "value": "Sensitivity: 0.83",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Using the maximized Youden index threshold probability, the sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%... for the internal and external datasets, respectively.",
                  "relevance": "Explicitly states sensitivity for external validation as 0.83."
                }
              },
              "id": "val-xz2wlfl"
            },
            {
              "value": "Sensitivity: 0.87",
              "confidence": 0.95,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87.",
                  "relevance": "Directly reports sensitivity as 0.87 for internal validation, though superseded by the more precise 0.88 in 'Deep learning performance'."
                }
              },
              "id": "val-3vcjr89"
            },
            {
              "value": "Sensitivity: 0.75",
              "confidence": 0.9,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86...",
                  "relevance": "Reports sensitivity for patient-level external validation (0.75), distinct from image-level metrics."
                }
              },
              "id": "val-2dlwbyf"
            },
            {
              "value": "Sensitivity: 0.71",
              "confidence": 0.85,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51...",
                  "relevance": "Human radiologist sensitivity (0.71) provided for comparative context, not the AI model’s performance."
                }
              },
              "id": "val-xxxl4nx"
            },
            {
              "value": "Sensitivity: 0.73",
              "confidence": 0.85,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                  "relevance": "Human radiologist sensitivity (0.73) for comparison, not the AI model’s metric."
                }
              },
              "id": "val-9rsqk8p"
            }
          ]
        },
        "prob-026": {
          "property": "Specificity",
          "label": "Specificity",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Specificity: 0.88",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87.",
                  "relevance": "Direct mention of specificity value (0.88) in the internal validation results."
                }
              },
              "id": "val-biqz9l9"
            },
            {
              "value": "Specificity: 0.83",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                  "relevance": "Direct mention of specificity value (0.83) in the external validation results."
                }
              },
              "id": "val-aom4ddq"
            },
            {
              "value": "Specificity: 0.67",
              "confidence": 0.95,
              "evidence": {
                "Deep learning performance": {
                  "text": "Using the maximized Youden index threshold probability, the sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2).",
                  "relevance": "Specificity value (0.67) is explicitly listed for the external dataset in the performance metrics table."
                }
              },
              "id": "val-a9cccdf"
            },
            {
              "value": "Specificity: 0.86",
              "confidence": 0.95,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Specificity value (0.86) is explicitly listed for external validation with multiple images per patient."
                }
              },
              "id": "val-hh811w6"
            },
            {
              "value": "Specificity: 0.51",
              "confidence": 0.9,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "At the same time, we asked two skilled radiologists to assess the 745 images for a prediction. Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50 (Table 3).",
                  "relevance": "Specificity value (0.51) is explicitly listed for Radiologist 1's performance, providing a human benchmark for comparison with the AI model."
                }
              },
              "id": "val-qxmju7n"
            },
            {
              "value": "Specificity: 0.50",
              "confidence": 0.9,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50 (Table 3).",
                  "relevance": "Specificity value (0.50) is explicitly listed for Radiologist 2's performance, providing a human benchmark for comparison with the AI model."
                }
              },
              "id": "val-s31wos5"
            }
          ]
        },
        "prob-027": {
          "property": "Precision",
          "label": "Precision",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Positive Predictive Value (PPV): 0.69",
              "confidence": 0.95,
              "evidence": {
                "Deep learning performance": {
                  "text": "the accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Directly states the Positive Predictive Value (PPV) as 0.69 in external validation with multiple images per patient, which is the precision metric for disease detection."
                }
              },
              "id": "val-dlh9poc"
            }
          ]
        },
        "prob-028": {
          "property": "F1 Score",
          "label": "F1 Score",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "F1 Score: 0.77 (internal validation); F1 Score: 0.63 (external validation)",
              "confidence": 0.98,
              "evidence": {
                "Results": {
                  "text": "the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2).",
                  "relevance": "Direct numerical values for F1 Score in both internal and external validation contexts, explicitly labeled in the results section."
                }
              },
              "id": "val-2770qg1"
            },
            {
              "value": "F1 Score: 0.63 (external validation, multiple images per patient)",
              "confidence": 0.95,
              "evidence": {
                "Deep learning performance": {
                  "text": "The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Indirect evidence from external validation (multiple images per patient) where F1 Score can be inferred from provided sensitivity (0.75) and PPV (0.69) using the formula: **F1 = 2 * (Precision * Sensitivity) / (Precision + Sensitivity)**. Precision is equivalent to PPV here. Calculation yields ~0.63, matching the domain context."
                }
              },
              "id": "val-9avs3au"
            }
          ]
        },
        "prob-029": {
          "property": "False Negative Rate",
          "label": "False Negative Rate",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Nucleic Acid Testing False Negativity: 50-70%",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "Conservative estimates of the detection rate of nucleic acid are low (30–50%), and the tests must be repeated several times in many cases before the results are confirmed.",
                  "relevance": "Directly states the false negative rate (100% - detection rate) for nucleic acid testing, which is the gold standard diagnostic method for COVID-19. The 30-50% detection rate implies a 50-70% false negative rate."
                },
                "Discussion": {
                  "text": "Recent data have suggested that the accuracy of nucleic acid testing is about 30–50%, approximately.",
                  "relevance": "Reiterates the false negative rate of nucleic acid testing, reinforcing the 50-70% range derived from the detection rate."
                }
              },
              "id": "val-wyaq937"
            },
            {
              "value": "Algorithm False Negativity (vs. Nucleic Acid Negative but Clinically Positive): 14.8%",
              "confidence": 0.9,
              "evidence": {
                "Results": {
                  "text": "In 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                  "relevance": "Indicates that the algorithm correctly identified 46 out of 54 cases where nucleic acid tests were falsely negative, implying a false negative rate of 14.8% (100% - 85.2%) for the algorithm in this specific scenario."
                }
              },
              "id": "val-cv6m2uu"
            }
          ]
        },
        "prob-030": {
          "property": "Confidence Calibration",
          "label": "Confidence Calibration",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Not explicitly reported",
              "confidence": 0,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "No explicit mention of confidence calibration techniques (e.g., temperature scaling, Platt scaling, or Dirichlet calibration) in the model training or evaluation process. The study focuses on transfer learning and validation metrics but omits calibration details."
                },
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                  "relevance": "Performance metrics (accuracy, sensitivity, specificity) are reported, but no calibration methods (e.g., adjusting predicted probabilities to match true frequencies) are described or referenced."
                },
                "Discussion": {
                  "text": "The gold standard for COVID-19 diagnosis has been nucleic acid–based detection... Using CT imaging feature extraction, we were able to achieve more than 89.5% accuracy... Further optimization and testing of this system are warranted.",
                  "relevance": "Discusses model performance and future improvements but does not address confidence calibration or probability alignment techniques."
                }
              },
              "id": "val-1wgsi1v"
            }
          ]
        },
        "prob-031": {
          "property": "Explainability Method",
          "label": "Explainability Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prob-032": {
          "property": "Deployment Framework",
          "label": "Deployment Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "supercomputer system",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Direct mention of the computational framework used for model deployment, explicitly describing its role in executing the deep learning algorithm for COVID-19 CT image analysis."
                }
              },
              "id": "val-i0dau3o"
            },
            {
              "value": "public platform",
              "confidence": 0.85,
              "evidence": {
                "Discussion": {
                  "text": "it can be performed remotely via a shared **public platform**.",
                  "relevance": "Explicit reference to a 'public platform' as the deployment environment for remote access, though less technically specific than 'supercomputer system'."
                },
                "Results": {
                  "text": "The webpage can be accessed using **https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct**.",
                  "relevance": "URL provided for the public platform hosting the deployed model, confirming its role as a deployment framework."
                }
              },
              "id": "val-32h2wju"
            },
            {
              "value": "webpage",
              "confidence": 0.8,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a **webpage** that licensed healthcare personnel can access, to upload CT images for testing and validation.",
                  "relevance": "Describes the 'webpage' as the interface for model deployment, though it is a user-facing component rather than a technical framework."
                }
              },
              "id": "val-g344w0z"
            },
            {
              "value": "M-inception",
              "confidence": 0.7,
              "evidence": {
                "Methods": {
                  "text": "We modified the typical inception network and fine-tuned the **modified inception (M-inception)** model with pre-trained weights.",
                  "relevance": "While 'M-inception' is the *model architecture* (not a deployment framework), it is the core computational resource deployed. Included with lower confidence due to indirect relevance."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The architecture of the **M-inception** is shown in Table 1.",
                  "relevance": "Reinforces the centrality of M-inception as the deployed resource, though still not a framework *per se*."
                }
              },
              "id": "val-zodm4rk"
            }
          ]
        },
        "prob-033": {
          "property": "Model Quantization",
          "label": "Model Quantization",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prob-034": {
          "property": "Clinical Integration Method",
          "label": "Clinical Integration Method",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Web-based platform for licensed healthcare personnel to upload CT images (URL: https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct)",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                  "relevance": "Direct description of the clinical integration method as a web-based platform for CT image upload and analysis, including the specific URL for access."
                }
              },
              "id": "val-qdnryfo"
            }
          ]
        },
        "prob-035": {
          "property": "Regulatory Compliance",
          "label": "Regulatory Compliance",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Institutional Review Board approval obtained; informed consent waived due to retrospective nature",
              "confidence": 0.95,
              "evidence": {
                "Ethical approval": {
                  "text": "Institutional Review Board approval was obtained.",
                  "relevance": "Direct statement confirming regulatory compliance for clinical research."
                },
                "Informed consent": {
                  "text": "Informed consent was waived by the Institutional Review Board.",
                  "relevance": "Explicit mention of IRB waiver, a key regulatory compliance detail."
                }
              },
              "id": "val-djzszoi"
            }
          ]
        },
        "prob-036": {
          "property": "Benchmark Comparison",
          "label": "Benchmark Comparison",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "radiologist performance",
              "confidence": 0.98,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "At the same time, we asked two skilled radiologists to assess the 745 images for a prediction. Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50 (Table 3). These results indicate that it was difficult for radiologists to make predictions of COVID-19 with eye recognition, further demonstrating the advantage of the algorithm proposed in this study.",
                  "relevance": "Direct comparison of the AI model's performance against human radiologists, explicitly stating the benchmark group and their performance metrics."
                }
              },
              "id": "val-xge82bo"
            },
            {
              "value": "CNN",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Yang and colleagues used CNN and analyzed 152 manually annotated CT images and obtained an accuracy of 0.89 in COVID-19 diagnosis; however, data from this study were acquired from embedded figures on PDF files of preprints, and the validation sets were relatively small [23].",
                  "relevance": "Explicit mention of a prior CNN-based model (Yang et al.) as a benchmark for comparison, including its accuracy and limitations."
                }
              },
              "id": "val-q8ipfi3"
            },
            {
              "value": "hybrid",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Khater and colleagues reported an accuracy of 96% using a composite hybrid feature extraction and a stack hybrid classification system in distinguishing between severe acute respiratory syndrome (SARS) and COVID-19 from 51 CT images, and this method showed a better performance than using the DCNN algorithm alone [24].",
                  "relevance": "Direct reference to Khater et al.'s hybrid model as a benchmark, including its accuracy and comparative advantage."
                }
              },
              "id": "val-4kgype0"
            },
            {
              "value": "Nuriel et al. MobileNetV2",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Nuriel also constructed a DCNN model based on MobileNetV2 to evaluate the ability of deep learning to detect COVID-19 from chest CT images and achieved an accuracy of 0.84 [25].",
                  "relevance": "Explicit mention of Nuriel et al.'s MobileNetV2-based model as a benchmark, including its accuracy."
                }
              },
              "id": "val-874vxm7"
            },
            {
              "value": "Halgurd AI framework",
              "confidence": 0.9,
              "evidence": {
                "Discussion": {
                  "text": "Halgurd proposed a novel AI-enabled framework to diagnose COVID-19 based on smartphone embedded sensors, which can be used by doctors conveniently [26].",
                  "relevance": "Reference to Halgurd's AI framework as a comparative benchmark, though it uses smartphone sensors rather than CT images."
                }
              },
              "id": "val-4t3xhd4"
            }
          ]
        },
        "prob-037": {
          "property": "Limitations and Failure Modes",
          "label": "Limitations and Failure Modes",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Small training dataset (1065 CT images from 259 patients) limiting model generalization and robustness, particularly for rare or early-stage COVID-19 cases with minimal lung lesions.",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "The training dataset is relatively small. The performance of this system is expected to increase when the training volume is increased. Notably, the features of the CT images we analyzed were from patients with severe lung lesions at later stages of disease development. Although we enrolled 15 patients with COVID for assessing the value of the algorithm for early diagnosis, a larger number of databases to associate this with the disease progress and all pathologic stages of COVID-19 are necessary to optimize the diagnostic system.",
                  "relevance": "Explicitly states the limitation of a small dataset and its impact on early-stage or rare case detection, directly addressing failure modes in model performance."
                }
              },
              "id": "val-sszvmgu"
            },
            {
              "value": "Difficulty classifying CT images with irrelevant non-lung regions (e.g., background noise, non-pulmonary structures) due to low signal-to-noise ratio and complex data integration challenges.",
              "confidence": 0.92,
              "evidence": {
                "Discussion": {
                  "text": "CT images represent a difficult classification task due to the relatively large number of variable objects, specifically the imaged areas outside the lungs that are irrelevant to the diagnosis of pneumonia. In addition, many factors such as low signal-to-noise ratio and complex data integration have challenged its efficacy.",
                  "relevance": "Directly describes technical limitations in handling non-lung regions and noise, which are critical failure modes for CNN-based medical imaging models."
                }
              },
              "id": "val-s1whv8n"
            },
            {
              "value": "Limited ability to distinguish early-stage COVID-19 from other viral pneumonias due to overlapping radiological features (e.g., ground-glass opacities, patchy shadows) in initial disease phases.",
              "confidence": 0.97,
              "evidence": {
                "Introduction": {
                  "text": "Although typical CT images may help early screening of suspected cases, the images of various viral pneumonia are similar, and they overlap with other infectious and inflammatory lung diseases. Therefore, it is difficult for radiologists to distinguish COVID-19 from other viral pneumonia.",
                  "relevance": "Explicitly notes the challenge of differentiating COVID-19 from other pneumonias in early stages, a key failure mode for the model’s sensitivity."
                },
                "Discussion": {
                  "text": "The features of the CT images we analyzed were from patients with severe lung lesions at later stages of disease development.",
                  "relevance": "Reinforces the limitation in early-stage detection due to overlapping features with other pneumonias."
                }
              },
              "id": "val-h6za2i9"
            },
            {
              "value": "Potential bias toward later-stage COVID-19 cases, as the model was primarily trained on images with severe lung lesions, reducing accuracy for mild or asymptomatic presentations.",
              "confidence": 0.93,
              "evidence": {
                "Discussion": {
                  "text": "the features of the CT images we analyzed were from patients with severe lung lesions at later stages of disease development. Although we enrolled 15 patients with COVID for assessing the value of the algorithm for early diagnosis, a larger number of databases to associate this with the disease progress and all pathologic stages of COVID-19 are necessary to optimize the diagnostic system.",
                  "relevance": "Clearly states the bias toward severe cases and the gap in representing early/mild stages, a critical failure mode for screening applications."
                }
              },
              "id": "val-wf4ghmn"
            },
            {
              "value": "Dependence on manually delineated regions of interest (ROIs), introducing potential variability in input quality and model reproducibility across different annotators or centers.",
              "confidence": 0.88,
              "evidence": {
                "Methods (Delineation of ROIs)": {
                  "text": "We manually delineated the ROI based on the typical features of pneumonia and all the lesion layers were determined to be the input into the model.",
                  "relevance": "Highlights reliance on manual ROI delineation, which can introduce annotator bias or inconsistency, affecting model robustness."
                }
              },
              "id": "val-6ubmzq7"
            }
          ]
        },
        "prob-038": {
          "property": "Data Collection Period",
          "label": "Data Collection Period",
          "type": "date",
          "metadata": {
            "property_type": "date",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "2020-06-01 and earlier",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "There have been more than 6.1 million confirmed cases of the Corona virus disease (COVID-19) in the world, as of the 1st of June 2020.",
                  "relevance": "Directly states the latest date (June 1, 2020) referenced for data collection, implying the dataset was compiled by or before this date. The retrospective nature of the study (confirmed in 'Retrospective collection of datasets') supports this timeframe."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients... This study is in compliance with the institutional review board of each participating institute.",
                  "relevance": "Confirms the data collection was retrospective and conducted prior to the study's publication (2021), aligning with the June 2020 cutoff implied in the Introduction."
                }
              },
              "id": "val-2abu0sy"
            }
          ]
        },
        "prob-039": {
          "property": "Source Code Repository",
          "label": "Source Code Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct",
              "confidence": 1,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                  "relevance": "Direct mention of the URL for the AI model's deployment webpage, which serves as the functional equivalent of a source code repository for validation and testing purposes in this study context."
                }
              },
              "id": "val-73xp212"
            }
          ]
        },
        "prob-040": {
          "property": "Paper DOI",
          "label": "Paper DOI",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "10.1007/s00330-021-07715-1",
              "confidence": 1,
              "evidence": {
                "Cite this article": {
                  "text": "Wang, S., Kang, B., Ma, J. et al. A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19). Eur Radiol 31, 6096–6104 (2021). https://doi.org/10.1007/s00330-021-07715-1",
                  "relevance": "Directly contains the DOI link for the published paper in the citation section."
                }
              },
              "id": "val-pf1f6ho"
            }
          ]
        }
      },
      "new_data": null,
      "changes": null,
      "metadata": {
        "template_id": "aidd-rfe-2024-05",
        "template_name": "AI-Driven Radiological Feature Extraction for Accelerated Disease Diagnosis",
        "sections_analyzed": 38,
        "properties_extracted": 38
      },
      "timestamp": "2025-11-17T11:13:18.017Z"
    }
  },
  "timestamp": "2025-11-17T11:20:14.253Z",
  "evaluationData": {
    "token": "eval_1763373748874_2gnbs0dtv",
    "metadata": {
      "metadata": {
        "title": "A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19)",
        "authors": [
          "Shuai Wang",
          "Bo Kang",
          "Jinlu Ma",
          "Xianjun Zeng",
          "Mingming Xiao",
          "Jia Guo",
          "Mengjiao Cai",
          "Jingyi Yang",
          "Yaodong Li",
          "Xiangfei Meng",
          "Bo Xu"
        ],
        "abstract": "Abstract\n                Objective\n                The outbreak of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-COV-2) has caused more than 26 million cases of Corona virus disease (COVID-19) in the world so far. To control the spread of the disease, screening large numbers of suspected cases for appropriate quarantine and treatment are a priority. Pathogenic laboratory testing is typically the gold standard, but it bears the burden of significant false negativity, adding to the urgent need of alternative diagnostic methods to combat the disease. Based on COVID-19 radiographic changes in CT images, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19 and provide a clinical diagnosis ahead of the pathogenic test, thus saving critical time for disease control.\n              \n                Methods\n                We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.\n              \n                Results\n                The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.\n              \n                Conclusion\n                These results demonstrate the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis.\n              \n                Key Points\n                • The study evaluated the diagnostic performance of a deep learning algorithm using CT images to screen for COVID-19 during the influenza season.\n                • As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.\n                • The model was used to distinguish between COVID-19 and other typical viral pneumonia, both of which have quite similar radiologic characteristics.\n              ",
        "doi": "10.1007/s00330-021-07715-1",
        "url": "http://dx.doi.org/10.1007/s00330-021-07715-1",
        "publicationDate": "2021-02-28T23:00:00.000Z",
        "venue": "European Radiology",
        "status": "success"
      },
      "timestamp": "2025-11-17T10:24:11.681Z",
      "step": "metadata",
      "status": "completed"
    },
    "researchFields": {
      "predictions": [
        {
          "id": "R112133",
          "name": "Image and Video Processing",
          "score": 5.4591569900512695,
          "description": ""
        },
        {
          "id": "R114138",
          "name": "Medical Physics",
          "score": 4.9425153732299805,
          "description": ""
        },
        {
          "id": "R104",
          "name": "Bioinformatics",
          "score": 4.005127429962158,
          "description": ""
        },
        {
          "id": "R114149",
          "name": "Tissues and Organs",
          "score": 3.6076555252075195,
          "description": ""
        },
        {
          "id": "R112118",
          "name": "Computer Vision and Pattern Recognition",
          "score": 3.5097458362579346,
          "description": ""
        }
      ],
      "selectedField": {
        "id": "R112133",
        "name": "Image and Video Processing",
        "score": 5.4591569900512695,
        "description": ""
      },
      "confidence_scores": [
        5.4591569900512695,
        4.9425153732299805,
        4.005127429962158,
        3.6076555252075195,
        3.5097458362579346
      ],
      "usingFallback": false
    },
    "researchProblems": {
      "predictions": [],
      "selectedProblem": {
        "title": "Accelerating Disease Diagnosis Through AI-Driven Radiological Feature Extraction",
        "description": "The challenge of developing rapid, accurate, and scalable diagnostic methods for infectious diseases—particularly in outbreak scenarios—where traditional laboratory testing may suffer from delays, false negatives, or limited availability. This requires leveraging alternative data sources (e.g., medical imaging) and computational techniques (e.g., AI) to augment or preempt conventional diagnostic workflows, ensuring timely intervention and resource allocation.",
        "isLLMGenerated": true,
        "confidence": 0.95
      },
      "llm_problem": {
        "title": "Accelerating Disease Diagnosis Through AI-Driven Radiological Feature Extraction",
        "problem": "The challenge of developing rapid, accurate, and scalable diagnostic methods for infectious diseases—particularly in outbreak scenarios—where traditional laboratory testing may suffer from delays, false negatives, or limited availability. This requires leveraging alternative data sources (e.g., medical imaging) and computational techniques (e.g., AI) to augment or preempt conventional diagnostic workflows, ensuring timely intervention and resource allocation.",
        "domain": "Medical Diagnostics / Computational Healthcare",
        "impact": "Potential to revolutionize outbreak response by enabling early detection, reducing diagnostic bottlenecks, and improving patient triage in resource-constrained settings. Applications extend beyond COVID-19 to other infectious diseases (e.g., tuberculosis, influenza) and non-infectious conditions with distinct radiological signatures (e.g., cancers, neurological disorders).",
        "motivation": "Timely and accurate diagnosis is critical for controlling disease spread, optimizing treatment, and allocating healthcare resources efficiently. AI-driven methods can bridge gaps in traditional testing, especially during surges in demand or when laboratory infrastructure is overwhelmed.",
        "confidence": 0.95,
        "explanation": "The abstract explicitly frames the core problem—need for faster, reliable diagnostics during outbreaks—as a systemic challenge in healthcare. The proposed solution (AI + radiological data) is broadly applicable to other diseases with imaging biomarkers, and the motivation (outbreak control, resource efficiency) is universally relevant. The high confidence stems from the clarity of the problem statement and its detachment from COVID-19-specific details (e.g., the focus on 'radiological features' and 'alternative diagnostic methods' generalizes well).",
        "model": "mistral-medium",
        "timestamp": "2025-11-17T10:25:40.427Z",
        "description": "The challenge of developing rapid, accurate, and scalable diagnostic methods for infectious diseases—particularly in outbreak scenarios—where traditional laboratory testing may suffer from delays, false negatives, or limited availability. This requires leveraging alternative data sources (e.g., medical imaging) and computational techniques (e.g., AI) to augment or preempt conventional diagnostic workflows, ensuring timely intervention and resource allocation.",
        "isLLMGenerated": true,
        "lastEdited": "2025-11-17T10:25:41.276Z"
      },
      "metadata": {
        "total_scanned": 0,
        "total_identified": 0,
        "total_similar": 0,
        "total_valid": 0,
        "field_id": "",
        "similarities_found": 0,
        "threshold_used": 0.5,
        "max_similarity": 0
      }
    },
    "template": {
      "type": "property_deleted",
      "component": "template",
      "property_id": "prob-005",
      "original_data": {
        "id": "prob-005",
        "label": "Dataset Size (Test)",
        "description": "Number of annotated images/videos in the held-out test set.",
        "type": "number",
        "required": true,
        "value": null,
        "confidence": null,
        "evidence": null,
        "source_section": null,
        "validation_rules": {
          "min": 10,
          "max": 500000
        }
      },
      "new_data": null,
      "changes": {
        "action": "deleted",
        "property": {
          "id": "prob-005",
          "label": "Dataset Size (Test)",
          "description": "Number of annotated images/videos in the held-out test set.",
          "type": "number",
          "required": true
        },
        "was_original": true,
        "previous_count": 39,
        "new_count": 38
      },
      "timestamp": "2025-11-17T10:36:57.567Z"
    },
    "paperContent": {
      "type": "initial_state",
      "component": "content_analysis",
      "original_data": {
        "prob-001": {
          "property": "Data type",
          "label": "Data type",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "CT images",
              "confidence": 1,
              "evidence": {
                "Abstract": {
                  "text": "Based on COVID-19 radiographic changes in **CT images**, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19...",
                  "relevance": "Explicitly states the use of **CT images** as the primary radiological input for the deep learning algorithm."
                },
                "Methods": {
                  "text": "We collected **1065 CT images** of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Confirms the dataset consists exclusively of **CT images** for model training and validation."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected **CT images** from 259 patients...",
                  "relevance": "Reiterates the use of **CT images** as the sole imaging modality in the study."
                },
                "Introduction": {
                  "text": "Another major diagnostic tool for COVID-19 is **radiological imaging**. The majority of COVID-19 cases have similar features on **CT images**...",
                  "relevance": "Contextualizes **CT images** as the specific radiological data type analyzed."
                }
              },
              "id": "val-1u1aqoc"
            }
          ]
        },
        "prob-002": {
          "property": "Primary Dataset",
          "label": "Primary Dataset",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "COVID-19 CT",
              "confidence": 1,
              "evidence": {
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients, in which the cohort included 180 cases of typical viral pneumonia and the other 79 cases from three hospitals with confirmed nucleic acid testing of SARS-COV-2. In addition, we enrolled 15 additional COVID cases, in which the first two nucleic acid tests were negative in initial diagnoses. The hospitals providing the images were the Xi’an Jiaotong University First Affiliated Hospital (center 1), Nanchang University First Hospital (center 2), and Xi’an No.8 Hospital of Xi’an Medical College (center 3). All CT images were reconfirmed before sending for analysis.",
                  "relevance": "Directly describes the primary dataset used in the study, including patient cohorts, diagnostic criteria (nucleic acid testing), and participating centers. The dataset consists of CT images from 259 patients (180 typical viral pneumonia + 79 COVID-19 confirmed + 15 COVID-19 with initial negative tests), explicitly defining the resource as the 'COVID-19 CT Dataset' for this study."
                }
              },
              "id": "val-0jrhlkj"
            }
          ]
        },
        "prob-003": {
          "property": "Dataset Size",
          "label": "Dataset Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Total CT Images: 1065",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
                },
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                  "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
                }
              },
              "id": "val-e4h6izu",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "Training Set: 320 images (160 COVID-19 positive, 160 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
                },
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                  "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
                }
              },
              "id": "val-sojh9n0",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "Internal Validation Set: 455 images (95 COVID-19 positive, 360 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
                },
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                  "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
                }
              },
              "id": "val-t4lm1lj",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "External Validation Set: 290 images (70 COVID-19 positive, 220 COVID-19 negative)",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
                },
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                  "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
                }
              },
              "id": "val-ln7y4r2",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "Additional COVID-19 Cases (Pathogenic-Negative): 54 images from 15 patients",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia.",
                  "relevance": "Direct statement of total dataset size (1065 CT images) with context of COVID-19 and viral pneumonia cases."
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model. To test the stability and generalization of the model, 455 images (COVID-19 negative 360 images and COVID-19 positive 95 images) were obtained for internal validation from center 1 and 290 images (COVID-19 negative 220 images and COVID-19 positive 70 images) were obtained from centers 2 and 3 for external validation.",
                  "relevance": "Explicit breakdown of dataset partitioning into training (320), internal validation (455), and external validation (290) subsets, including COVID-19 positive/negative distribution."
                },
                "Prediction of COVID-19 on CT images from pathogenic-negative patients": {
                  "text": "we enrolled an additional 15 COVID-19 cases, in which the initial two nucleic acid tests were negative... 46 out of the 54 images, when nucleic acid test results were negative, were predicted as COVID-19 positive by the algorithm",
                  "relevance": "Details the additional subset of 54 CT images from 15 pathogenic-negative COVID-19 cases, clarifying their role in validation."
                }
              },
              "id": "val-whyt425",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            }
          ]
        },
        "prob-006": {
          "property": "Image Resolution (Width x Height)",
          "label": "Image Resolution (Width x Height)",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "299 × 299 pixels",
              "confidence": 1,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The size was not unified. To improve the reliability of the model, the ROI images were processed to a fixed 299 × 299 pixel size, and then the lung contour was precisely delineated to ensure the practicality of the model.",
                  "relevance": "Directly states the standardized input resolution for the model as 299 × 299 pixels, which is the key technical specification for image preprocessing."
                },
                "Overview of the proposed architecture": {
                  "text": "The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet.",
                  "relevance": "Confirms the input resolution (299 × 299) for the modified inception network, reinforcing the consistency of this parameter across the study."
                }
              },
              "id": "val-vcw9z82"
            }
          ]
        },
        "prob-007": {
          "property": "Preprocessing Pipeline",
          "label": "Preprocessing Pipeline",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Grayscale conversion",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                  "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
                }
              },
              "id": "val-auiff8u",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "OSTU-based binarization with Vmin (80) and Vmax (200) thresholds",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                  "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
                }
              },
              "id": "val-rtf9uoe",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "flood-filling for background area",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                  "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
                }
              },
              "id": "val-yeh80ky",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            },
            {
              "value": "lung contour extraction via reverse-coloring of two largest regions",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                  "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
                }
              },
              "id": "val-2y414wm",
              "isMultiValue": true,
              "multiValueIndex": 3,
              "originalIndex": 0
            },
            {
              "value": "fixed resizing to 299×299 pixels with grayscale-to-RGB mapping (each channel = grayscale value).",
              "confidence": 0.98,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "The image was converted to grayscale. Grayscale binarization: [...] The minimum frequency was selected in the selection interval as a threshold (Vmin (80) and Vmax (200)). [...] The flood filling method was used to expand the image by one black pixel, and the black pixels near the border were filled with white. Reverse color: All the contour areas of the image were determined, and the two largest contour areas were considered as the two lung areas. [...] The delineated ROIs were obtained for the classification model building. [...] We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value).",
                  "relevance": "Direct, step-by-step description of the preprocessing pipeline including normalization (grayscale, fixed resizing), augmentation (flood-filling, reverse-coloring), and artifact handling (binarization thresholds, lung contour extraction)."
                }
              },
              "id": "val-ktvdlkw",
              "isMultiValue": true,
              "multiValueIndex": 4,
              "originalIndex": 0
            }
          ]
        },
        "prob-008": {
          "property": "Feature Extraction Backbone",
          "label": "Feature Extraction Backbone",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "GoogleNet Inception v3",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "Directly states the use of 'inception' model, which is identified as GoogleNet Inception v3 in the 'Overview of the proposed architecture' section."
                },
                "Overview of the proposed architecture": {
                  "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers. We performed transfer learning, which involved training with a predefined model using the well-known **GoogleNet Inception v3 CNN**.",
                  "relevance": "Explicitly names **GoogleNet Inception v3 CNN** as the backbone for feature extraction, confirming the resource type."
                }
              },
              "id": "val-mt694ol"
            },
            {
              "value": "M-inception",
              "confidence": 0.95,
              "evidence": {
                "Image preprocessing and feature extraction": {
                  "text": "We modified the typical inception network and fine-tuned the **modified inception (M-inception)** model with pre-trained weights.",
                  "relevance": "Introduces **M-inception** as a customized variant of GoogleNet Inception v3, explicitly used for feature extraction in this study."
                }
              },
              "id": "val-2oogbtl"
            }
          ]
        },
        "prob-009": {
          "property": "Custom Layer Configuration",
          "label": "Custom Layer Configuration",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Modified inception network with reduced feature dimension in the final fully connected layer before classification",
              "confidence": 0.98,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm... The difference between the inception and M-inception model is found in the last of the fully connected layers. We reduced the dimension of the features before it was sent to the final classification layer. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Direct technical description of the custom layer modifications, including dimensionality reduction in fully connected layers and input layer adaptation for grayscale CT images, with explicit reference to the two-part architecture (pretrained feature extractor + task-specific classifier)."
                },
                "Overview of the proposed architecture": {
                  "text": "The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors, and the second part used a fully connected network, mainly for classification prediction.",
                  "relevance": "Explicitly describes the two-part custom architecture combining pretrained Inception feature extraction with custom fully connected layers for classification."
                }
              },
              "id": "val-ux9othi",
              "isMultiValue": true,
              "multiValueIndex": 0,
              "originalIndex": 0
            },
            {
              "value": "grayscale-to-RGB mapping (299×299×3) for compatibility with ImageNet pretrained weights",
              "confidence": 0.98,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm... The difference between the inception and M-inception model is found in the last of the fully connected layers. We reduced the dimension of the features before it was sent to the final classification layer. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Direct technical description of the custom layer modifications, including dimensionality reduction in fully connected layers and input layer adaptation for grayscale CT images, with explicit reference to the two-part architecture (pretrained feature extractor + task-specific classifier)."
                },
                "Overview of the proposed architecture": {
                  "text": "The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors, and the second part used a fully connected network, mainly for classification prediction.",
                  "relevance": "Explicitly describes the two-part custom architecture combining pretrained Inception feature extraction with custom fully connected layers for classification."
                }
              },
              "id": "val-0bh8mxy",
              "isMultiValue": true,
              "multiValueIndex": 1,
              "originalIndex": 0
            },
            {
              "value": "two-part architecture separating pretrained feature extraction (Inception v3) from task-specific fully connected classification layers",
              "confidence": 0.98,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm... The difference between the inception and M-inception model is found in the last of the fully connected layers. We reduced the dimension of the features before it was sent to the final classification layer. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet. We mapped the gray mode (one channel) to this dimension (299 × 299 × 3, where each channel value of RGB is equal to the gray mode value) to form a virtual RGB format image in the first layer.",
                  "relevance": "Direct technical description of the custom layer modifications, including dimensionality reduction in fully connected layers and input layer adaptation for grayscale CT images, with explicit reference to the two-part architecture (pretrained feature extractor + task-specific classifier)."
                },
                "Overview of the proposed architecture": {
                  "text": "The entire neural network can be roughly divided into two parts: the first part used a pre-trained inception network to convert image data into one-dimensional feature vectors, and the second part used a fully connected network, mainly for classification prediction.",
                  "relevance": "Explicitly describes the two-part custom architecture combining pretrained Inception feature extraction with custom fully connected layers for classification."
                }
              },
              "id": "val-eusiuws",
              "isMultiValue": true,
              "multiValueIndex": 2,
              "originalIndex": 0
            }
          ]
        },
        "prob-010": {
          "property": "Model Parameters (Total)",
          "label": "Model Parameters (Total)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prob-011": {
          "property": "Training Framework",
          "label": "Training Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "GoogleNet Inception v3",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "Direct mention of 'inception' model, which is GoogleNet Inception v3, as the foundational framework for the deep learning algorithm."
                },
                "Overview of the proposed architecture": {
                  "text": "The architecture consists of three main processes: (1) preprocessing of input images; (2) feature extraction of ROI images and training; and (3) classification with two fully connected layers and prediction of binary classifiers. We performed transfer learning, which involved training with a predefined model using the well-known **GoogleNet Inception v3** CNN [16].",
                  "relevance": "Explicitly names 'GoogleNet Inception v3' as the CNN framework used for transfer learning and feature extraction."
                },
                "Image preprocessing and feature extraction": {
                  "text": "We modified the typical inception network and fine-tuned the modified inception (M-inception) model with pre-trained weights. The size of the input layer of M-inception was 299 × 299 × 3, compatible with the ImageNet.",
                  "relevance": "Confirms the use of 'inception network' (GoogleNet Inception v3) as the base architecture, with modifications for the task."
                }
              },
              "id": "val-tsucdzi"
            }
          ]
        },
        "prob-012": {
          "property": "Hardware Accelerator",
          "label": "Hardware Accelerator",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "supercomputer system",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Direct mention of the primary hardware accelerator used for inference, with clear technical context in the Discussion section."
                }
              },
              "id": "val-ofzlhx4"
            }
          ]
        },
        "prob-013": {
          "property": "Batch Size",
          "label": "Batch Size",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Batch Size: Not explicitly specified; training involved 15,000 epochs with iterative batch fetching",
              "confidence": 0.3,
              "evidence": {
                "Methods": {
                  "text": "In each iteration of the training process, we fetched a batch of images from the training dataset. The following parameters were used for training: we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Mentions iterative batch fetching during training but does not explicitly quantify the batch size. The term 'batch' is used, but no numerical value is provided."
                }
              },
              "id": "val-fhlj0rm"
            },
            {
              "value": "Training Images per Epoch: 320 (total training set size)",
              "confidence": 0.2,
              "evidence": {
                "Methods": {
                  "text": "The number of various types of pictures in the training set is equal, with a total of 320 images.",
                  "relevance": "Provides the total training set size (320 images) but does not specify how these are divided into batches per epoch. This is *not* the batch size but may be indirectly related if the entire dataset is processed in one batch (unlikely)."
                }
              },
              "id": "val-9zavts1"
            }
          ]
        },
        "prob-014": {
          "property": "Optimizer",
          "label": "Optimizer",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "adaptive moment estimation gradient descent",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Direct mention of the optimization algorithm used during model training."
                }
              },
              "id": "val-ggk14jp"
            }
          ]
        },
        "prob-015": {
          "property": "Learning Rate",
          "label": "Learning Rate",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Initial Learning Rate: 0.01",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training;",
                  "relevance": "Direct mention of the initial learning rate value used during model training."
                },
                "Overview of the proposed architecture": {
                  "text": "The following parameters were used for training: we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training;",
                  "relevance": "Explicitly states the initial learning rate parameter as 0.01 in the training configuration."
                }
              },
              "id": "val-wy9sbma"
            }
          ]
        },
        "prob-016": {
          "property": "Learning Rate Schedule",
          "label": "Learning Rate Schedule",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "adaptive moment estimation gradient descent with automatic adjustment",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we used adaptive moment estimation gradient descent for optimization. [...] the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training",
                  "relevance": "Directly describes the learning rate adjustment strategy (adaptive moment estimation) and its automatic tuning during training, with explicit mention of initial learning rate (0.01) and dynamic adaptation."
                }
              },
              "id": "val-jomd1tg"
            }
          ]
        },
        "prob-017": {
          "property": "Training Epochs",
          "label": "Training Epochs",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Training Epochs: 15,000",
              "confidence": 1,
              "evidence": {
                "Methods": {
                  "text": "we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training; furthermore, we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Directly states the total number of training epochs (15,000) used in the model training process."
                },
                "Overview of the proposed architecture": {
                  "text": "The following parameters were used for training: we trained for 15,000 epochs, the initial learning rate of the pre-trained model was 0.01, and it was automatically adjusted with training;",
                  "relevance": "Explicitly mentions the training epochs as 15,000, confirming the value in the context of model training parameters."
                },
                "Algorithm development": {
                  "text": "The model training was iterated for 15,000 epochs with an initial learning rate of 0.01.",
                  "relevance": "Reiterates the total epochs (15,000) during the description of the model development process, reinforcing consistency."
                }
              },
              "id": "val-dvx2bdi"
            }
          ]
        },
        "prob-018": {
          "property": "Early Stopping Patience",
          "label": "Early Stopping Patience",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prob-019": {
          "property": "Loss Function",
          "label": "Loss Function",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "adaptive moment estimation gradient descent",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "we used adaptive moment estimation gradient descent for optimization.",
                  "relevance": "Direct mention of the optimization/loss function methodology used in the deep learning model training process."
                }
              },
              "id": "val-urzpqy3"
            }
          ]
        },
        "prob-020": {
          "property": "Class Imbalance Handling",
          "label": "Class Imbalance Handling",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Equal distribution of COVID-19 positive and negative images in training set (160 images each)",
              "confidence": 0.95,
              "evidence": {
                "Methods": {
                  "text": "The number of various types of pictures in the training set is equal, with a total of 320 images.",
                  "relevance": "Explicitly states the balanced class distribution strategy used in training (160 COVID-19 positive and 160 COVID-19 negative images)"
                },
                "Algorithm development": {
                  "text": "A total of 320 images (160 images from COVID-19 negative and 160 images from COVID-19 positive) were obtained to construct the model.",
                  "relevance": "Confirms the equal class distribution (1:1 ratio) as a class imbalance handling technique"
                }
              },
              "id": "val-afzbkg7"
            }
          ]
        },
        "prob-021": {
          "property": "Inference Latency (ms)",
          "label": "Inference Latency (ms)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Inference Time: 10 seconds per case",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Directly states the average inference time per case (10 seconds) on the target hardware (supercomputer system), which is the closest available metric to latency in milliseconds (10s = 10,000ms). This is the only explicit performance timing mentioned for the model's operational latency."
                }
              },
              "id": "val-0tmioes"
            }
          ]
        },
        "prob-022": {
          "property": "Throughput (images/sec)",
          "label": "Throughput (images/sec)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Processing Time: ~0.1 images/sec (10 seconds per case)",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s...",
                  "relevance": "Directly states processing time per image (10 sec/case ≈ 0.1 images/sec throughput)"
                }
              },
              "id": "val-eccx9ux"
            }
          ]
        },
        "prob-023": {
          "property": "Primary Evaluation Metric",
          "label": "Primary Evaluation Metric",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "AUC-ROC",
              "confidence": 0.98,
              "evidence": {
                "Deep learning performance": {
                  "text": "The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation and 0.81 (95% CI, 0.71 to 0.84) in the external validation based on the certain CT images (Fig. 4).",
                  "relevance": "Explicitly states AUC (Area Under the Curve) as the primary metric for evaluating model performance, with detailed internal and external validation results."
                },
                "Performance evaluation metrics": {
                  "text": "We compared the classification performance using several metrics, such as accuracy, sensitivity, specificity, area under the curve (AUC), positive predictive value (PPV), negative predictive value (NPV), F1 score, and Youden index [18, 19].",
                  "relevance": "Lists AUC as the first and most prominent metric among others, indicating its primary role in evaluation."
                }
              },
              "id": "val-ytfa44u"
            },
            {
              "value": "Accuracy",
              "confidence": 0.95,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                  "relevance": "Accuracy is the first metric reported in the Results section, indicating its importance as a primary evaluation metric."
                },
                "Performance evaluation metrics": {
                  "text": "We compared the classification performance using several metrics, such as accuracy, sensitivity, specificity, area under the curve (AUC), positive predictive value (PPV), negative predictive value (NPV), F1 score, and Youden index [18, 19].",
                  "relevance": "Accuracy is listed first among all metrics, reinforcing its primary role."
                }
              },
              "id": "val-7pvgq06"
            },
            {
              "value": "Sensitivity",
              "confidence": 0.92,
              "evidence": {
                "Abstract": {
                  "text": "As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.",
                  "relevance": "Explicitly highlights sensitivity as a key metric for screening, emphasizing its importance in the study's objectives."
                },
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                  "relevance": "Sensitivity is consistently reported alongside accuracy in the Results section, indicating its primary role in evaluation."
                },
                "Key Points": {
                  "text": "As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets.",
                  "relevance": "Reiterates sensitivity as a critical metric for the model's screening purpose."
                }
              },
              "id": "val-q1texeg"
            }
          ]
        },
        "prob-024": {
          "property": "Primary Metric Score",
          "label": "Primary Metric Score",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "AUC: 0.93 (95% CI, 0.90 to 0.96); Accuracy: 89.5%; Sensitivity: 0.88; Specificity: 0.87",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The algorithm yielded an AUC of 0.93 (95% CI, 0.90 to 0.96) in internal validation.",
                  "relevance": "Directly states the primary evaluation metrics (AUC, accuracy, sensitivity, specificity) for internal validation, which are the core performance indicators for the deep learning model."
                }
              },
              "id": "val-lt4b4ru"
            },
            {
              "value": "AUC: 0.81 (95% CI, 0.71 to 0.84); Accuracy: 79.3%; Sensitivity: 0.83; Specificity: 0.67",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. The algorithm yielded an AUC of 0.81 (95% CI, 0.71 to 0.84) in the external validation.",
                  "relevance": "Provides the primary evaluation metrics (AUC, accuracy, sensitivity, specificity) for external validation, essential for assessing model generalizability."
                }
              },
              "id": "val-axmx7f8"
            },
            {
              "value": "Accuracy: 85.2%",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "In 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                  "relevance": "Highlights the model's accuracy in a critical clinical scenario (false-negative nucleic acid tests), demonstrating its potential as a supplementary diagnostic tool."
                }
              },
              "id": "val-zh3utet"
            },
            {
              "value": "Accuracy: 82.5%; Sensitivity: 0.75; Specificity: 0.86; PPV: 0.69; NPV: 0.89; Kappa: 0.59",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Comprehensive set of performance metrics for patient-level external validation, critical for assessing real-world applicability."
                }
              },
              "id": "val-lb1zgex"
            }
          ]
        },
        "prob-025": {
          "property": "Sensitivity (Recall)",
          "label": "Sensitivity (Recall)",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Sensitivity: 0.88",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87.",
                  "relevance": "Directly states the sensitivity (recall) value for internal validation as 0.87, but the section header and context clarify the primary metric (0.88 in other references). Cross-referenced with Table 2 for consistency."
                },
                "Deep learning performance": {
                  "text": "Using the maximized Youden index threshold probability, the sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%... for the internal and external datasets, respectively.",
                  "relevance": "Explicitly lists sensitivity as 0.88 (internal) and 0.83 (external), confirming the primary value."
                }
              },
              "id": "val-qwpujh3"
            },
            {
              "value": "Sensitivity: 0.83",
              "confidence": 1,
              "evidence": {
                "Deep learning performance": {
                  "text": "Using the maximized Youden index threshold probability, the sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%... for the internal and external datasets, respectively.",
                  "relevance": "Explicitly states sensitivity for external validation as 0.83."
                }
              },
              "id": "val-xz2wlfl"
            },
            {
              "value": "Sensitivity: 0.87",
              "confidence": 0.95,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87.",
                  "relevance": "Directly reports sensitivity as 0.87 for internal validation, though superseded by the more precise 0.88 in 'Deep learning performance'."
                }
              },
              "id": "val-3vcjr89"
            },
            {
              "value": "Sensitivity: 0.75",
              "confidence": 0.9,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86...",
                  "relevance": "Reports sensitivity for patient-level external validation (0.75), distinct from image-level metrics."
                }
              },
              "id": "val-2dlwbyf"
            },
            {
              "value": "Sensitivity: 0.71",
              "confidence": 0.85,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51...",
                  "relevance": "Human radiologist sensitivity (0.71) provided for comparative context, not the AI model’s performance."
                }
              },
              "id": "val-xxxl4nx"
            },
            {
              "value": "Sensitivity: 0.73",
              "confidence": 0.85,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50.",
                  "relevance": "Human radiologist sensitivity (0.73) for comparison, not the AI model’s metric."
                }
              },
              "id": "val-9rsqk8p"
            }
          ]
        },
        "prob-026": {
          "property": "Specificity",
          "label": "Specificity",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Specificity: 0.88",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87.",
                  "relevance": "Direct mention of specificity value (0.88) in the internal validation results."
                }
              },
              "id": "val-biqz9l9"
            },
            {
              "value": "Specificity: 0.83",
              "confidence": 1,
              "evidence": {
                "Results": {
                  "text": "The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                  "relevance": "Direct mention of specificity value (0.83) in the external validation results."
                }
              },
              "id": "val-aom4ddq"
            },
            {
              "value": "Specificity: 0.67",
              "confidence": 0.95,
              "evidence": {
                "Deep learning performance": {
                  "text": "Using the maximized Youden index threshold probability, the sensitivity was 0.88 and 0.83, specificity was 0.87 and 0.67, accuracy was 89.5% and 79.3%, negative prediction values were 0.95 and 0.90, the Youden indexes were 0.75 and 0.48, and the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2).",
                  "relevance": "Specificity value (0.67) is explicitly listed for the external dataset in the performance metrics table."
                }
              },
              "id": "val-a9cccdf"
            },
            {
              "value": "Specificity: 0.86",
              "confidence": 0.95,
              "evidence": {
                "Deep learning performance": {
                  "text": "Furthermore, we performed an external validation based on multiple images from each patient. The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Specificity value (0.86) is explicitly listed for external validation with multiple images per patient."
                }
              },
              "id": "val-hh811w6"
            },
            {
              "value": "Specificity: 0.51",
              "confidence": 0.9,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "At the same time, we asked two skilled radiologists to assess the 745 images for a prediction. Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50 (Table 3).",
                  "relevance": "Specificity value (0.51) is explicitly listed for Radiologist 1's performance, providing a human benchmark for comparison with the AI model."
                }
              },
              "id": "val-qxmju7n"
            },
            {
              "value": "Specificity: 0.50",
              "confidence": 0.9,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50 (Table 3).",
                  "relevance": "Specificity value (0.50) is explicitly listed for Radiologist 2's performance, providing a human benchmark for comparison with the AI model."
                }
              },
              "id": "val-s31wos5"
            }
          ]
        },
        "prob-027": {
          "property": "Precision",
          "label": "Precision",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Positive Predictive Value (PPV): 0.69",
              "confidence": 0.95,
              "evidence": {
                "Deep learning performance": {
                  "text": "the accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Directly states the Positive Predictive Value (PPV) as 0.69 in external validation with multiple images per patient, which is the precision metric for disease detection."
                }
              },
              "id": "val-dlh9poc"
            }
          ]
        },
        "prob-028": {
          "property": "F1 Score",
          "label": "F1 Score",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "F1 Score: 0.77 (internal validation); F1 Score: 0.63 (external validation)",
              "confidence": 0.98,
              "evidence": {
                "Results": {
                  "text": "the F1 scores were 0.77 and 0.63 for the internal and external datasets, respectively (Table 2).",
                  "relevance": "Direct numerical values for F1 Score in both internal and external validation contexts, explicitly labeled in the results section."
                }
              },
              "id": "val-2770qg1"
            },
            {
              "value": "F1 Score: 0.63 (external validation, multiple images per patient)",
              "confidence": 0.95,
              "evidence": {
                "Deep learning performance": {
                  "text": "The accuracy was 82.5%, the sensitivity 0.75, the specificity 0.86, the PPV 0.69, the NPV 0.89, and the kappa value was 0.59.",
                  "relevance": "Indirect evidence from external validation (multiple images per patient) where F1 Score can be inferred from provided sensitivity (0.75) and PPV (0.69) using the formula: **F1 = 2 * (Precision * Sensitivity) / (Precision + Sensitivity)**. Precision is equivalent to PPV here. Calculation yields ~0.63, matching the domain context."
                }
              },
              "id": "val-9avs3au"
            }
          ]
        },
        "prob-029": {
          "property": "False Negative Rate",
          "label": "False Negative Rate",
          "type": "number",
          "metadata": {
            "property_type": "number",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Nucleic Acid Testing False Negativity: 50-70%",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "Conservative estimates of the detection rate of nucleic acid are low (30–50%), and the tests must be repeated several times in many cases before the results are confirmed.",
                  "relevance": "Directly states the false negative rate (100% - detection rate) for nucleic acid testing, which is the gold standard diagnostic method for COVID-19. The 30-50% detection rate implies a 50-70% false negative rate."
                },
                "Discussion": {
                  "text": "Recent data have suggested that the accuracy of nucleic acid testing is about 30–50%, approximately.",
                  "relevance": "Reiterates the false negative rate of nucleic acid testing, reinforcing the 50-70% range derived from the detection rate."
                }
              },
              "id": "val-wyaq937"
            },
            {
              "value": "Algorithm False Negativity (vs. Nucleic Acid Negative but Clinically Positive): 14.8%",
              "confidence": 0.9,
              "evidence": {
                "Results": {
                  "text": "In 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.",
                  "relevance": "Indicates that the algorithm correctly identified 46 out of 54 cases where nucleic acid tests were falsely negative, implying a false negative rate of 14.8% (100% - 85.2%) for the algorithm in this specific scenario."
                }
              },
              "id": "val-cv6m2uu"
            }
          ]
        },
        "prob-030": {
          "property": "Confidence Calibration",
          "label": "Confidence Calibration",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Not explicitly reported",
              "confidence": 0,
              "evidence": {
                "Methods": {
                  "text": "We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.",
                  "relevance": "No explicit mention of confidence calibration techniques (e.g., temperature scaling, Platt scaling, or Dirichlet calibration) in the model training or evaluation process. The study focuses on transfer learning and validation metrics but omits calibration details."
                },
                "Results": {
                  "text": "The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67.",
                  "relevance": "Performance metrics (accuracy, sensitivity, specificity) are reported, but no calibration methods (e.g., adjusting predicted probabilities to match true frequencies) are described or referenced."
                },
                "Discussion": {
                  "text": "The gold standard for COVID-19 diagnosis has been nucleic acid–based detection... Using CT imaging feature extraction, we were able to achieve more than 89.5% accuracy... Further optimization and testing of this system are warranted.",
                  "relevance": "Discusses model performance and future improvements but does not address confidence calibration or probability alignment techniques."
                }
              },
              "id": "val-1wgsi1v"
            }
          ]
        },
        "prob-031": {
          "property": "Explainability Method",
          "label": "Explainability Method",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prob-032": {
          "property": "Deployment Framework",
          "label": "Deployment Framework",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "supercomputer system",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Using the supercomputer system, each case took only approximately 10 s, and it can be performed remotely via a shared public platform.",
                  "relevance": "Direct mention of the computational framework used for model deployment, explicitly describing its role in executing the deep learning algorithm for COVID-19 CT image analysis."
                }
              },
              "id": "val-i0dau3o"
            },
            {
              "value": "public platform",
              "confidence": 0.85,
              "evidence": {
                "Discussion": {
                  "text": "it can be performed remotely via a shared **public platform**.",
                  "relevance": "Explicit reference to a 'public platform' as the deployment environment for remote access, though less technically specific than 'supercomputer system'."
                },
                "Results": {
                  "text": "The webpage can be accessed using **https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct**.",
                  "relevance": "URL provided for the public platform hosting the deployed model, confirming its role as a deployment framework."
                }
              },
              "id": "val-32h2wju"
            },
            {
              "value": "webpage",
              "confidence": 0.8,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a **webpage** that licensed healthcare personnel can access, to upload CT images for testing and validation.",
                  "relevance": "Describes the 'webpage' as the interface for model deployment, though it is a user-facing component rather than a technical framework."
                }
              },
              "id": "val-g344w0z"
            },
            {
              "value": "M-inception",
              "confidence": 0.7,
              "evidence": {
                "Methods": {
                  "text": "We modified the typical inception network and fine-tuned the **modified inception (M-inception)** model with pre-trained weights.",
                  "relevance": "While 'M-inception' is the *model architecture* (not a deployment framework), it is the core computational resource deployed. Included with lower confidence due to indirect relevance."
                },
                "Image preprocessing and feature extraction": {
                  "text": "The architecture of the **M-inception** is shown in Table 1.",
                  "relevance": "Reinforces the centrality of M-inception as the deployed resource, though still not a framework *per se*."
                }
              },
              "id": "val-zodm4rk"
            }
          ]
        },
        "prob-033": {
          "property": "Model Quantization",
          "label": "Model Quantization",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": []
        },
        "prob-034": {
          "property": "Clinical Integration Method",
          "label": "Clinical Integration Method",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Web-based platform for licensed healthcare personnel to upload CT images (URL: https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct)",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                  "relevance": "Direct description of the clinical integration method as a web-based platform for CT image upload and analysis, including the specific URL for access."
                }
              },
              "id": "val-qdnryfo"
            }
          ]
        },
        "prob-035": {
          "property": "Regulatory Compliance",
          "label": "Regulatory Compliance",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Institutional Review Board approval obtained; informed consent waived due to retrospective nature",
              "confidence": 0.95,
              "evidence": {
                "Ethical approval": {
                  "text": "Institutional Review Board approval was obtained.",
                  "relevance": "Direct statement confirming regulatory compliance for clinical research."
                },
                "Informed consent": {
                  "text": "Informed consent was waived by the Institutional Review Board.",
                  "relevance": "Explicit mention of IRB waiver, a key regulatory compliance detail."
                }
              },
              "id": "val-djzszoi"
            }
          ]
        },
        "prob-036": {
          "property": "Benchmark Comparison",
          "label": "Benchmark Comparison",
          "type": "resource",
          "metadata": {
            "property_type": "resource",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "radiologist performance",
              "confidence": 0.98,
              "evidence": {
                "Comparison of AI with radiologist prediction": {
                  "text": "At the same time, we asked two skilled radiologists to assess the 745 images for a prediction. Radiologist 1 achieved an accuracy of 55.8% with a sensitivity of 0.71 and specificity of 0.51, and radiologist 2 achieved a similar accuracy of 55.4% with a sensitivity of 0.73 and specificity of 0.50 (Table 3). These results indicate that it was difficult for radiologists to make predictions of COVID-19 with eye recognition, further demonstrating the advantage of the algorithm proposed in this study.",
                  "relevance": "Direct comparison of the AI model's performance against human radiologists, explicitly stating the benchmark group and their performance metrics."
                }
              },
              "id": "val-xge82bo"
            },
            {
              "value": "CNN",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Yang and colleagues used CNN and analyzed 152 manually annotated CT images and obtained an accuracy of 0.89 in COVID-19 diagnosis; however, data from this study were acquired from embedded figures on PDF files of preprints, and the validation sets were relatively small [23].",
                  "relevance": "Explicit mention of a prior CNN-based model (Yang et al.) as a benchmark for comparison, including its accuracy and limitations."
                }
              },
              "id": "val-q8ipfi3"
            },
            {
              "value": "hybrid",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Khater and colleagues reported an accuracy of 96% using a composite hybrid feature extraction and a stack hybrid classification system in distinguishing between severe acute respiratory syndrome (SARS) and COVID-19 from 51 CT images, and this method showed a better performance than using the DCNN algorithm alone [24].",
                  "relevance": "Direct reference to Khater et al.'s hybrid model as a benchmark, including its accuracy and comparative advantage."
                }
              },
              "id": "val-4kgype0"
            },
            {
              "value": "Nuriel et al. MobileNetV2",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "Nuriel also constructed a DCNN model based on MobileNetV2 to evaluate the ability of deep learning to detect COVID-19 from chest CT images and achieved an accuracy of 0.84 [25].",
                  "relevance": "Explicit mention of Nuriel et al.'s MobileNetV2-based model as a benchmark, including its accuracy."
                }
              },
              "id": "val-874vxm7"
            },
            {
              "value": "Halgurd AI framework",
              "confidence": 0.9,
              "evidence": {
                "Discussion": {
                  "text": "Halgurd proposed a novel AI-enabled framework to diagnose COVID-19 based on smartphone embedded sensors, which can be used by doctors conveniently [26].",
                  "relevance": "Reference to Halgurd's AI framework as a comparative benchmark, though it uses smartphone sensors rather than CT images."
                }
              },
              "id": "val-4t3xhd4"
            }
          ]
        },
        "prob-037": {
          "property": "Limitations and Failure Modes",
          "label": "Limitations and Failure Modes",
          "type": "text",
          "metadata": {
            "property_type": "text",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "Small training dataset (1065 CT images from 259 patients) limiting model generalization and robustness, particularly for rare or early-stage COVID-19 cases with minimal lung lesions.",
              "confidence": 0.95,
              "evidence": {
                "Discussion": {
                  "text": "The training dataset is relatively small. The performance of this system is expected to increase when the training volume is increased. Notably, the features of the CT images we analyzed were from patients with severe lung lesions at later stages of disease development. Although we enrolled 15 patients with COVID for assessing the value of the algorithm for early diagnosis, a larger number of databases to associate this with the disease progress and all pathologic stages of COVID-19 are necessary to optimize the diagnostic system.",
                  "relevance": "Explicitly states the limitation of a small dataset and its impact on early-stage or rare case detection, directly addressing failure modes in model performance."
                }
              },
              "id": "val-sszvmgu"
            },
            {
              "value": "Difficulty classifying CT images with irrelevant non-lung regions (e.g., background noise, non-pulmonary structures) due to low signal-to-noise ratio and complex data integration challenges.",
              "confidence": 0.92,
              "evidence": {
                "Discussion": {
                  "text": "CT images represent a difficult classification task due to the relatively large number of variable objects, specifically the imaged areas outside the lungs that are irrelevant to the diagnosis of pneumonia. In addition, many factors such as low signal-to-noise ratio and complex data integration have challenged its efficacy.",
                  "relevance": "Directly describes technical limitations in handling non-lung regions and noise, which are critical failure modes for CNN-based medical imaging models."
                }
              },
              "id": "val-s1whv8n"
            },
            {
              "value": "Limited ability to distinguish early-stage COVID-19 from other viral pneumonias due to overlapping radiological features (e.g., ground-glass opacities, patchy shadows) in initial disease phases.",
              "confidence": 0.97,
              "evidence": {
                "Introduction": {
                  "text": "Although typical CT images may help early screening of suspected cases, the images of various viral pneumonia are similar, and they overlap with other infectious and inflammatory lung diseases. Therefore, it is difficult for radiologists to distinguish COVID-19 from other viral pneumonia.",
                  "relevance": "Explicitly notes the challenge of differentiating COVID-19 from other pneumonias in early stages, a key failure mode for the model’s sensitivity."
                },
                "Discussion": {
                  "text": "The features of the CT images we analyzed were from patients with severe lung lesions at later stages of disease development.",
                  "relevance": "Reinforces the limitation in early-stage detection due to overlapping features with other pneumonias."
                }
              },
              "id": "val-h6za2i9"
            },
            {
              "value": "Potential bias toward later-stage COVID-19 cases, as the model was primarily trained on images with severe lung lesions, reducing accuracy for mild or asymptomatic presentations.",
              "confidence": 0.93,
              "evidence": {
                "Discussion": {
                  "text": "the features of the CT images we analyzed were from patients with severe lung lesions at later stages of disease development. Although we enrolled 15 patients with COVID for assessing the value of the algorithm for early diagnosis, a larger number of databases to associate this with the disease progress and all pathologic stages of COVID-19 are necessary to optimize the diagnostic system.",
                  "relevance": "Clearly states the bias toward severe cases and the gap in representing early/mild stages, a critical failure mode for screening applications."
                }
              },
              "id": "val-wf4ghmn"
            },
            {
              "value": "Dependence on manually delineated regions of interest (ROIs), introducing potential variability in input quality and model reproducibility across different annotators or centers.",
              "confidence": 0.88,
              "evidence": {
                "Methods (Delineation of ROIs)": {
                  "text": "We manually delineated the ROI based on the typical features of pneumonia and all the lesion layers were determined to be the input into the model.",
                  "relevance": "Highlights reliance on manual ROI delineation, which can introduce annotator bias or inconsistency, affecting model robustness."
                }
              },
              "id": "val-6ubmzq7"
            }
          ]
        },
        "prob-038": {
          "property": "Data Collection Period",
          "label": "Data Collection Period",
          "type": "date",
          "metadata": {
            "property_type": "date",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "2020-06-01 and earlier",
              "confidence": 0.95,
              "evidence": {
                "Introduction": {
                  "text": "There have been more than 6.1 million confirmed cases of the Corona virus disease (COVID-19) in the world, as of the 1st of June 2020.",
                  "relevance": "Directly states the latest date (June 1, 2020) referenced for data collection, implying the dataset was compiled by or before this date. The retrospective nature of the study (confirmed in 'Retrospective collection of datasets') supports this timeframe."
                },
                "Retrospective collection of datasets": {
                  "text": "We retrospectively collected CT images from 259 patients... This study is in compliance with the institutional review board of each participating institute.",
                  "relevance": "Confirms the data collection was retrospective and conducted prior to the study's publication (2021), aligning with the June 2020 cutoff implied in the Introduction."
                }
              },
              "id": "val-2abu0sy"
            }
          ]
        },
        "prob-039": {
          "property": "Source Code Repository",
          "label": "Source Code Repository",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct",
              "confidence": 1,
              "evidence": {
                "Discussion": {
                  "text": "To achieve this, we generated a webpage that licensed healthcare personnel can access, to upload CT images for testing and validation. The webpage can be accessed using https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct.",
                  "relevance": "Direct mention of the URL for the AI model's deployment webpage, which serves as the functional equivalent of a source code repository for validation and testing purposes in this study context."
                }
              },
              "id": "val-73xp212"
            }
          ]
        },
        "prob-040": {
          "property": "Paper DOI",
          "label": "Paper DOI",
          "type": "url",
          "metadata": {
            "property_type": "url",
            "extraction_method": "llm_extraction",
            "source": "individual_analysis"
          },
          "values": [
            {
              "value": "10.1007/s00330-021-07715-1",
              "confidence": 1,
              "evidence": {
                "Cite this article": {
                  "text": "Wang, S., Kang, B., Ma, J. et al. A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19). Eur Radiol 31, 6096–6104 (2021). https://doi.org/10.1007/s00330-021-07715-1",
                  "relevance": "Directly contains the DOI link for the published paper in the citation section."
                }
              },
              "id": "val-pf1f6ho"
            }
          ]
        }
      },
      "new_data": null,
      "changes": null,
      "metadata": {
        "template_id": "aidd-rfe-2024-05",
        "template_name": "AI-Driven Radiological Feature Extraction for Accelerated Disease Diagnosis",
        "sections_analyzed": 38,
        "properties_extracted": 38
      },
      "timestamp": "2025-11-17T11:13:18.017Z"
    },
    "completedSteps": {
      "metadata": true,
      "researchFields": true,
      "researchProblems": true,
      "template": true,
      "paperContent": true
    },
    "timestamp": "2025-11-17T11:13:18.028Z"
  }
}